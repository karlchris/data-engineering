# About the Author

![Karl photo](assets/img/photo_small.jpeg)

_Halo_, I am Karl Christian :wave:

Expert in data engineering, problem solving and building a platform.

On a mission to democratize access to data, provide analytics as a platform, and building data driven framework as a mindset.

## Experiences

### [Careem](https://www.careem.com/) - Senior Data Warehouse Engineer

_May 2022 - Current_

Location: Dubai, UAE

- Work with engineering and business team to understand data requirements and build Data mart & pipelines, increased the reliability to 99.5% for having the batch data under SLA (tools: Airflow, Spark SQL, Hive, AWS EMR)
- Performing automated Data Quality checks within the pipeline, decreased data bug from around 60% (reported manually by stakeholders, usually it's unknown) to less than 10% (tools: Open Metadata, Trino)
- Produce and maintain strong documentations in Github.
- Deployed and maintained analytics platform - Redash using Docker and AWS EC2.

### [Traveloka](https://www.traveloka.com/en-id) - Senior Data Engineer

_April 2021 - April 2022_

Location: Jakarta, Indonesia

- Build and optimize ETL over wide organizations (CI/CD, dbt, GCP), handled more than 1k dbt models, almost 10 pipelines in daily, weekly, or monthly.
- Build Back End service for ML using GraphQL endpoint and Python, deployed in K8 and GCP.
- Build Data Ingestor from APIs using Airflow, Docker and K8.
- Build Integrated pipeline between GCP and AWS, build pipeline for Recommendation AI

### [Traveloka](https://www.traveloka.com/en-id) - Data Warehouse Engineer

_August 2019 - Mar 2021_

Location: Jakarta, Indonesia

- Build Data Warehouse and analytics data marts
- Building Data Pipeline, including batch for transactional and summary data using Cloud Composer
- Batch Data ingestion (Scraping/Crawling), e.g API, Web, Sheets, etc
- CI/CD with Cloudbuild, integrating with Github
- Containerization with docker and orchestration with kubernetes
- Deploying ML model with Cloud Composer

### [Tokopedia](https://www.tokopedia.com/) - Business Intelligence

_September 2018 - July 2019_

Location: Jakarta, Indonesia

- Build monitoring dashboard for Enterprise performance using Tableau and GCP to support Strategy team decision making.
- Implementing Batch processing to preprocess raw data into refined data warehouse using Airflow and BigQuery.

## Education

Bachelor of Science (Engineering major), [ITB](https://www.itb.ac.id/), 2012 - 2016

_First Class Honour, Cum Laude_

## Projects

- **Careem - Build Grocery Datamart and integrations to external**

  > Build Grocery data mart based on stakeholder requirements and business use cases, optimized the pipeline and delivery to have high reliability and scalable.

  > Created the design pattern on how to implement jobs in DAG, so it's testable and scalable, which increase reliability.

  > Implementing CI/CD to check the SQL and job relationship, and other necessary checks to ensure that the deployed jobs won't have any typos.

  > Implemented data quality checks on all the tables to reduce data issues.

  > Skills: Airflow, SQL, Spark, Hive, Trino, Open Metadata, REST API, AWS, Docker, Kubernetes

- **Traveloka - Build GraphQL Service Endpoint to support Product**

  > Build an endpoint service to generate customer profiling, the endpoint can be queried through GraphQL.

  > The backend is using FastAPI and deployed through Kubernetes, Docker and GCP.
  > Helping whole Financial Services BE team to simplify their process, they will only get the result, all the computations are happening within data platform service.

  > Skills: GraphQL, FastAPI, Kubernetes, Docker, Python, GCP

- **Traveloka - Data Modeling in Financial Services**

  > This project will involve dimensional modeling, we decide it's hybrid between Star and Snowflake schema.

  > Besides data modeling, project members will need to apply data engineering knowledge, understand the infrastructure behind the data warehouse and data pipelines.

  > Skills: DBT, dimensional modeling, GCP

- **Traveloka - External Data Ingestion to Data Warehouse**

  > Ingesting data from third party apps, such as Facebook, App Annie, Braze, Allocadia, etc into the BigQuery data warehouse.

  > Resulting create framework, boiler plate to do another ingestion quicker, testable and reliable

  > Skills: ETL, Airflow, Docker, Kubernetes GCP, Python, Shell

- **Tokopedia - Build OKR Company Dashboard**

  > Build dashboard to monitor the ongoing achievements from multiple Business Units are still on track with company target

  > Skills: Tableau, Airflow, GCP

- [Educative - Deploying a Web Application over Kubernetes](https://www.educative.io/verify-certificate/y8E3zVt2ZL7Bq0VxgSyzvm0y2QQgcm)
- [Educative - Setting up a Streaming Data Pipeline with Kafka](https://www.educative.io/verify-certificate/g5g3ywCwE9xpA617JFKAkp1KL22xSk)

## Certifications

- [Educative - Docker for Developers](https://www.educative.io/verify-certificate/r0w3pLtnWZ5LgVKopIQ5mqOQp446U6)
- [Educative - Learn Object-Oriented Programming in Python](https://www.educative.io/verify-certificate/j2l3BzfZn5G0MVK7rFxzJkBxy552FA)
- [Educative - Mastering Unit Testing with Pytest](https://www.educative.io/verify-certificate/wnDQEXnKW6AFPwvpj9R0RjcQLAwmQ2994UG)
- [Linkedin Learning - Analyzing Big Data with Hive](https://www.linkedin.com/learning/certificates/2f65729dd55b0e2e9dce9232c9e9327a2f53f50a60464d081296372b735d214d?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base_certifications_details%3BydX6llRCQJeNV2NHM0hVAA%3D%3D)
