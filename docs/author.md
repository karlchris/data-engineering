# About the Author :bust_in_silhouette:

![Karl photo](assets/img/photo_small.jpeg)

_Halo_, I am [Karl Christian](https://www.linkedin.com/in/karllchris/) :wave:

Expert in data engineering, problem solving and building a platform. :computer:

On a mission to democratize access to data, provide analytics as a platform, and building data driven framework as a mindset.

## Experiences :briefcase:

### [Careem](https://www.careem.com/) - Senior Data Warehouse Engineer

_May 2022 - Current_

Location: Dubai, UAE

- Work with engineering and business team to understand data requirements and build Data mart & pipelines, increased the reliability to 99.5% for having the batch data under SLA (tools: Airflow, Spark SQL, Hive, AWS EMR)
- Performing automated Data Quality checks within the pipeline, decreased data bug from around 60% (reported manually by stakeholders, usually it's unknown) to less than 10% (tools: Open Metadata, Trino)
- Produce and maintain strong documentations in Github.
- Deployed and maintained analytics platform - Redash using Docker and AWS EC2.

### [Traveloka](https://www.traveloka.com/en-id) - Senior Data Engineer

_April 2021 - April 2022_

Location: Jakarta, Indonesia

- Build and optimize ETL over wide organizations (CI/CD, dbt, GCP), handled more than 1k dbt models, almost 10 pipelines in daily, weekly, or monthly.
- Build Back End service for ML using GraphQL endpoint and Python, deployed in K8 and GCP.
- Build Data Ingestor from APIs using Airflow, Docker and K8.
- Build Integrated pipeline between GCP and AWS, build pipeline for Recommendation AI

### [Traveloka](https://www.traveloka.com/en-id) - Data Warehouse Engineer

_August 2019 - Mar 2021_

Location: Jakarta, Indonesia

- Build Data Warehouse and analytics data marts
- Building Data Pipeline, including batch for transactional and summary data using Cloud Composer
- Batch Data ingestion (Scraping/Crawling), e.g API, Web, Sheets, etc
- CI/CD with Cloudbuild, integrating with Github
- Containerization with docker and orchestration with kubernetes
- Deploying ML model with Cloud Composer

### [Tokopedia](https://www.tokopedia.com/) - Business Intelligence

_September 2018 - July 2019_

Location: Jakarta, Indonesia

- Build monitoring dashboard for Enterprise performance using Tableau and GCP to support Strategy team decision making.
- Implementing Batch processing to preprocess raw data into refined data warehouse using Airflow and BigQuery.

## Education :school:

Bachelor of Science (Engineering major), [ITB](https://www.itb.ac.id/), 2012 - 2016

_First Class Honour, Cum Laude_

## Projects :rocket:

### Build Grocery Datamart and integrations to external

Company: Careem

- Build Grocery data mart based on stakeholder requirements and business use cases, optimized the pipeline and delivery to have high reliability and scalable.
- Created the design pattern on how to implement jobs in DAG, so it's testable and scalable, which increase reliability.
- Implementing CI/CD to check the SQL and job relationship, and other necessary checks to ensure that the deployed jobs won't have any typos.
- Implemented data quality checks on all the tables to reduce data issues.
- Integrated pipeline with Azure, sending the generated files to Azure Blob Storage.

> Skills: Airflow, SQL, Spark, Hive, Trino, Open Metadata, REST API, AWS, Azure, Docker, Kubernetes

### Build GraphQL Service Endpoint to support Product

Company: Traveloka

- Build an endpoint service to generate customer profiling, the endpoint can be queried through GraphQL.
- The backend is using FastAPI and deployed through Kubernetes, Docker and GCP.
- Helping whole Financial Services BE team to simplify their process, they will only get the result, all the computations are happening within data platform service.

> Skills: GraphQL, FastAPI, Kubernetes, Docker, Python, GCP

### Data Modeling in Financial Services

Company: Traveloka

- This project will involve dimensional modeling, we decide it's hybrid between Star and Snowflake schema.
- Besides data modeling, project members will need to apply data engineering knowledge, understand the infrastructure behind the data warehouse and data pipelines.

> Skills: DBT, dimensional modeling, GCP

### Data Modeling and Orchestration for Customer Experience (CX) team

Company: Traveloka

- Build and migrate CX SQL models from native BQ into native DBT SQL, and build the DBT pipeline from scratch.

> Skills: DBT, dimensional modeling, GCP, Airflow

### External Data Ingestion to Data Warehouse

Company: Traveloka

- Ingesting data from third party apps, such as Facebook, App Annie, Braze, Allocadia, etc into the BigQuery data warehouse.
- Resulting create framework, boiler plate to do another ingestion quicker, testable and reliable

> Skills: ETL, Airflow, Docker, Kubernetes GCP, Python, Shell

### Build OKR Company Dashboard

Company: Tokopedia

- Build dashboard to monitor the ongoing achievements from multiple Business Units are still on track with company target

> Skills: Tableau, Airflow, GCP

## Certifications :bookmark:

- [Educative - Docker for Developers](https://www.educative.io/verify-certificate/r0w3pLtnWZ5LgVKopIQ5mqOQp446U6)
- [Educative - Deploying a Web Application over Kubernetes](https://www.educative.io/verify-certificate/y8E3zVt2ZL7Bq0VxgSyzvm0y2QQgcm)
- [Educative - Setting up a Streaming Data Pipeline with Kafka](https://www.educative.io/verify-certificate/g5g3ywCwE9xpA617JFKAkp1KL22xSk)
- [Educative - Learn Object-Oriented Programming in Python](https://www.educative.io/verify-certificate/j2l3BzfZn5G0MVK7rFxzJkBxy552FA)
- [Educative - Mastering Unit Testing with Pytest](https://www.educative.io/verify-certificate/wnDQEXnKW6AFPwvpj9R0RjcQLAwmQ2994UG)
- [Linkedin Learning - Analyzing Big Data with Hive](https://www.linkedin.com/learning/certificates/2f65729dd55b0e2e9dce9232c9e9327a2f53f50a60464d081296372b735d214d?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base_certifications_details%3BydX6llRCQJeNV2NHM0hVAA%3D%3D)

## Volunteering :muscle:

### Instructor - Data, Business & Analytics

_February 2022 - May 2022_

Part of Ruangguru x Kampus Merdeka

Teaching about:

- Data Analytics
- Product Analytics
- Analytics framework
- Defining OKR & KPI

> Technical skills: SQL, Python, Tableau.

### Mentor at [Google Bangkit](https://grow.google/intl/id_id/bangkit/?tab=machine-learning)

_January 2020 - June 2020_

Google Bangkit is a program to educate machine learning to all enthusiast in Indonesia, so we can apply ML better in every aspects of Indonesia together.

Google Bangkit is coordinated by Google itself, and with the help of 4 unicorns in Indonesia (Traveloka, Gojek, Tokopedia, and Bukalapak), they help by becoming the volunteers as mentor.

This is very pleasant experiences even for me, understanding variety of data savvy in Indonesia people and nurture them to become machine learning enthusiast.
