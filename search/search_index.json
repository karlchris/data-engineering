{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Data Engineering Works","text":"<p>Hello everyone </p> <p>SELAMAT DATANG !!! (1)</p> <ol> <li>Selamat datang is \"welcome\" in Indonesia  language. </li> </ol> <p>I am Karl Christian, an experienced data guy.</p> <p>I will share my knowledge repository for almost 7+ years of my working experience around Data and especially Data Engineering. You will find out about what skill set we should have as Data Engineer.</p> <p>I have experiences as data engineer, data warehousing and about analytics engineering. </p>"},{"location":"#overview","title":"Overview","text":"<p>Tip</p> <p>First, I bring up some exciting projects about data engineering, check it out here PROJECTS.</p> <p>I assure it's easy to follow even for beginners.</p> <p>If you want to deep dive into the topics and get basic/advance understandings, I suggest you go through the learning tabs, such as below:</p> <ul> <li>Data Engineering</li> <li>Data Modeling</li> <li>Data Architecture</li> <li>Data Orchestration</li> <li>Data Ingestion</li> <li>Data Processing</li> <li>Data Quality</li> <li>Learning Python</li> <li>Many more</li> </ul> <p>Stay tune to this notes, I will keep updating it to the most recent Data Engineering updates.</p> <p>Quote</p> <p>My perspective is that data architecture is like an ever-evolving river. It\u2019s like the Mississippi River, the Mississippi River from one day to the next is never the same. It\u2019s always changing. The same goes to data architecture. What\u2019s happened is Data Warehousing applies to structured transaction-based data. That\u2019s really the heart of data warehousing, but there\u2019s other data in the corporation that\u2019s viable and important data as well.</p> <p>Bill Inmon, \"Father of Data Warehouse\"</p> <p>Find my profile in About the author.</p>"},{"location":"author/","title":"About the Author","text":"<p>Halo, I am Karl Christian </p> <p>I have worked on some expertise, such as data engineering, data warehouse, problem solving and building a platform. </p> <p>I always have a growth mindset and purpose to democratize access to data, provide analytics as a platform, and building data driven framework.</p> <p>Some of technical skills that I'm capable of: SQL, Python, Airflow, Docker, Kubernetes, AWS, GCP, dimensional modeling, DBT, Spark, Hive, Shell, many more.</p> <p>Check my LinkedIn for getting to know.</p>"},{"location":"author/#projects","title":"Projects","text":""},{"location":"author/#build-grocery-datamart-and-integrations-to-external","title":"Build Grocery Datamart and integrations to external","text":"<p>Company: Careem</p> <ul> <li>Build Grocery data mart based on stakeholder requirements and business use cases, optimized the pipeline and delivery to have high reliability and scalable.</li> <li>Created the design pattern on how to implement jobs in DAG, so it's testable and scalable, which increase reliability.</li> <li>Implementing CI/CD to check the SQL and job relationship, and other necessary checks to ensure that the deployed jobs won't have any typos.</li> <li>Implemented data quality checks on all the tables to reduce data issues.</li> <li>Integrated pipeline with Azure, sending the generated files to Azure Blob Storage.</li> </ul> <p>Skills: Airflow, SQL, Spark, Hive, Trino, Open Metadata, REST API, AWS, Azure, Docker, Kubernetes</p>"},{"location":"author/#build-graphql-service-endpoint-to-support-product","title":"Build GraphQL Service Endpoint to support Product","text":"<p>Company: Traveloka</p> <ul> <li>Build an endpoint service to generate customer profiling, the endpoint can be queried through GraphQL.</li> <li>The backend is using FastAPI and deployed through Kubernetes, Docker and GCP.</li> <li>Helping whole Financial Services BE team to simplify their process, they will only get the result, all the computations are happening within data platform service.</li> </ul> <p>Skills: GraphQL, FastAPI, Kubernetes, Docker, Python, GCP</p>"},{"location":"author/#data-modeling-in-financial-services","title":"Data Modeling in Financial Services","text":"<p>Company: Traveloka</p> <ul> <li>This project will involve dimensional modeling, we decide it's hybrid between Star and Snowflake schema.</li> <li>Besides data modeling, project members will need to apply data engineering knowledge, understand the infrastructure behind the data warehouse and data pipelines.</li> </ul> <p>Skills: DBT, dimensional modeling, GCP</p>"},{"location":"author/#data-modeling-and-orchestration-for-customer-experience-team","title":"Data Modeling and Orchestration for Customer Experience team","text":"<p>Company: Traveloka</p> <ul> <li>Build and migrate Customer Experience SQL models from native BQ into native DBT SQL, and build the DBT pipeline from scratch.</li> </ul> <p>Skills: DBT, dimensional modeling, GCP, Airflow</p>"},{"location":"author/#external-data-ingestion-to-data-warehouse","title":"External Data Ingestion to Data Warehouse","text":"<p>Company: Traveloka</p> <ul> <li>Ingesting data from third party API, such as Facebook, App Annie, Braze, Allocadia, etc into the BigQuery data warehouse.</li> <li>Resulting create framework, boiler plate to do another ingestion quicker, testable and reliable</li> </ul> <p>Skills: ETL, Airflow, Docker, Kubernetes GCP, Python, Shell</p>"},{"location":"author/#build-okr-company-dashboard","title":"Build OKR Company Dashboard","text":"<p>Company: Tokopedia</p> <ul> <li>Build dashboard to monitor the ongoing achievements from multiple Business Units are still on track with company target.</li> </ul> <p>Skills: Tableau, Airflow, GCP</p>"},{"location":"data-architecture/","title":"Data Architecture","text":"<p>We will look at two widely-used cloud data architecture frameworks:</p> <ul> <li>Lambda architecture</li> <li>Kappa architecture</li> </ul> <p>The list can still grow, but it encourages us to do a trade-off analysis before choosing the right cloud data architecture.</p> <p>Cloud data architecture, by design, handles the ingestion, transformation, and analysis of data that is too large or complex for traditional data architectures.</p> <p>Then, we will take a look into 2 popular data storage architecture:</p> <ul> <li>Data Lake</li> <li>Data Mesh</li> </ul>"},{"location":"data-architecture/#patterns-of-good-cloud-data-architecture","title":"Patterns of Good Cloud Data Architecture","text":"<p>Let's learn about 5 principles for cloud-native data architecture that are useful for designing and operating reliable, cost-effective and efficient systems in the cloud.</p> <p>Cloud offers incredible capabilities, but without deliberate design decisions, the architecture can become fragile, expensive, and challenging to maintain. Most cloud environments have not just one application but several technologies that need to be integrated.</p> <p>The overarching goal of cloud architecture is to connect the dots to provide customers with a valuable online platform.</p>"},{"location":"data-architecture/#5-cloud-native-architecture-principles","title":"5 cloud-native architecture principles","text":"<p>This is essential for creating a good design:</p> <ul> <li> <p>Reliable: The system should continue to work correctly regardless of system faults or human errors.</p> </li> <li> <p>Efficient: The system must use computing resources efficiently to meet system requirements and maintain efficiency as the business grows.</p> </li> <li> <p>Maintainable: The world is fluid. Good data architecture should be able to respond to changes within the business and new technologies to unlock more possibilities in the future.</p> </li> <li> <p>Cost-optimized: The system should leverage various payment options to achieve cost efficiency.</p> </li> <li> <p>Secure: The system should be hardened to avoid insider attacks.</p> </li> </ul>"},{"location":"data-architecture/#principle-1-have-an-automation-mindset","title":"Principle 1: Have an automation mindset","text":"<p>Automation has always been good practice for software systems. In traditional environments, automation refers to building, testing, and deploying software through continuous integration/continuous delivery (CI/CD) pipelines.</p> <p>A good cloud architecture takes a step ahead by automating the infrastructure as well as the internal components.</p> <p>The five common areas for automation are shown below:</p> <p></p>"},{"location":"data-architecture/#software","title":"Software","text":"<p>Software has been the most common area for automation regardless of the environment. Automation happens throughout the software's life cycle, from coding and deployment to maintenance and updates.</p>"},{"location":"data-architecture/#infrastructure","title":"Infrastructure","text":"<p>In the cloud, we can apply the same engineering principles we use for applications to the entire environment. This implies the ability to create and manage infrastructure through code.</p> <p>Note</p> <p>Infrastructure as Code (IaC) is a process that enables us to manage infrastructure provisioning and configuration in the same way as we handle application code.</p> <p>Example</p> <p>we first provision a VM in the dev environment and then decide to create the same one in the production environment.</p> <p>Provisioning a server manually through a graphic interface often leads to mistakes.</p> <p>IaC means storing infrastructure configurations in a version-control environment and benefiting from CI/CD pipelines to ensure consistency across environments.</p>"},{"location":"data-architecture/#autoscaling","title":"Autoscaling","text":"<p>The world is fluctuating, and a reliable system must handle the fluctuation in the load accordingly. Autoscaling helps the applications handle traffic increases and reduce costs when the demand is low without disrupting business operations.</p>"},{"location":"data-architecture/#recovery","title":"Recovery","text":"<p>According to Google SRE philosophy, building a system with 100% availability is almost impossible and unnecessary. The team should, instead, embrace the risk and develop mechanisms to allow systems to recover from the failure quickly.</p> <p>Tip</p> <p>Automatic recovery works by monitoring workloads for key indicators and triggering operations when specific thresholds are reached.</p> <p>Example</p> <p>In the event of full memory or disk, the cloud will automatically request more resources and scale the system vertically, instead of just throwing an error and disrupting the system.</p>"},{"location":"data-architecture/#backup","title":"Backup","text":"<p>A backup strategy guarantees the business won't get interrupted during system failure, outage, data corruption, or natural disaster. Cloud backup operates by copying and storing data in a different physical location.</p>"},{"location":"data-architecture/#principle-2-outsource-with-caution","title":"Principle 2: Outsource with caution","text":"<p>Most cloud providers offer different abstract levels of services, namely IaaS, PaaS, and SaaS. Their ever-growing features help us offload day-to-day management to the vendors. However, some organizations are concerned with giving providers access to their internal data for security reasons.</p> <p>Warning</p> <p>The decision of whether or not to use managed services comes down to operational overhead and security.</p> <p>Tip</p> <p>The best practice is to find a cloud provider with a high reputation, express our concerns, and find a solution together. Even if the provider can't solve the problem immediately, the discussion might open the door to future possibilities.</p>"},{"location":"data-architecture/#principle-3-keep-an-eye-on-the-cost","title":"Principle 3: Keep an eye on the cost","text":"<p>Cost control isn\u2019t a prominent concern in traditional architecture because the assets and costs are pretty much fixed. However, in the cloud, the cost can be highly dynamic, and the team might surprisingly end up with a high bill.</p> <p>Warning</p> <p>Implementing cloud financial management is vital, and the organization must allocate time and resources to build knowledge around it and share the best practices with the teams.</p> <p>Fortunately, most cloud providers offer a centralized cost-monitoring tool that helps the team analyze and optimize the costs.</p> <p>A few quick wins on saving the cost:</p> <ul> <li> <p>Only pay what you need. Turn off stale servers and delete stale data.</p> </li> <li> <p>Enable table expiration on temporary data so they won't cost money after the expiration date.</p> </li> <li> <p>Maximize utilization. Implement efficient design to ensure high utilization of the underlying hardware.</p> </li> <li> <p>Query optimization. Learn different query optimization strategies such as incremental load, partitioning, and clustering.</p> </li> </ul>"},{"location":"data-architecture/#principle-4-embrace-changes","title":"Principle 4: Embrace changes","text":"<p>The world is constantly evolving, and that's true for cloud architecture. As the business changes, the landscape of systems also needs to change. Good architecture doesn't stay in the existing state forever. Instead, they are very agile and can respond to business changes and adapt to them with the least effort.</p> <p>Example changes in cloud architecture, as below</p> <p></p>"},{"location":"data-architecture/#principle-5-do-not-neglect-security","title":"Principle 5: Do not neglect security","text":"<p>Last but not least, implementing a strong identity foundation becomes a huge responsibility of the data team.</p> <p>Tip</p> <p>Traditional architectures place a lot of faith in perimeter security, crudely a hardened network perimeter with \"trusted\" things inside and \"untrusted\" things outside. Unfortunately, this approach has always been vulnerable to insider attackers, as well as external threats such as spear phishing.</p> <p>In the cloud environment, all assets are connected to the outside world to some degree. Zero Trust architecture has been created to eliminate the risk from both outside and inside. Zero Trust is a strategy that secures an organization by eliminating implicit trust and validating every stage of digital interaction.</p> <p>Google offers BeyondCorp, an implementation of the zero trust model.</p> <p></p> <p>Another important concept in terms of security is the shared responsibility model. It divides security into the security of the cloud and security in the cloud. Most cloud providers are responsible for the security of the cloud, and it's the user's responsibility to design a custom security model for their applications. Users are responsible for managing sensitive data, internal access to data and services, and ensuring GDPR compliance.</p>"},{"location":"data-architecture/framework-architecture/","title":"Framework Architecture","text":""},{"location":"data-architecture/framework-architecture/#lambda-architecture","title":"Lambda Architecture","text":"<p>In the mid-2010s, the rise of Apache Kafka as a stream-processing platform empowered companies to apply real-time algorithms to data, generating quicker and better insights. Given this real-time data processing trend, data engineers must figure out how to combine batch and streaming processes into one unified data architecture.</p> <p></p> <p>Lambda architecture is designed to handle massive data arriving through both batch and stream processes. The approach tries to balance accuracy, latency, and throughput by using batch data to provide comprehensive and accurate views, while simultaneously using stream data to provide a real-time view of the most recent data.</p>"},{"location":"data-architecture/framework-architecture/#how-it-works","title":"How it works","text":"<ul> <li>The system will dispatch all incoming data to batch and streaming layers.</li> <li>The batch layer will maintain an append-only primary dataset and precompute the batch views</li> <li>The streaming layer will only handle the most recent data to achieve low latency.</li> <li>Both batch and stream views are served in the serving layer to be queried.</li> <li>The result of merging batch and real-time results can answer any incoming query.</li> </ul>"},{"location":"data-architecture/framework-architecture/#challenges","title":"Challenges","text":"<ul> <li>Complexity and cost of running 2 parallel systems instead of 1.</li> <li>This approach often uses systems with different software ecosystems, making it challenging to replicate the business logic across the systems.</li> <li>It's also quite difficult to reconcile the outputs of 2 pipelines at the end.</li> </ul>"},{"location":"data-architecture/framework-architecture/#cloud-solution","title":"Cloud solution","text":"<p>Cloud vendors have developed solutions to improve the traditional lambda architecture. Google introduced DataFlow, proposing a unified approach for both batch and streaming processes, and particularly mentioning the exactly-once processing to enhance the accuracy of streaming views.</p> <p></p> <p>Note</p> <p>The Lambda architecture typically addresses time-related requestions. For instance:</p> <ul> <li> <p>How many new users did we have yesterday?</p> </li> <li> <p>How many new users did we have today so far?</p> </li> </ul> <p>The daily batch processing can respond to the first question but not the second one unless the job is manually triggered.</p> <p>On the other hand, the streaming layer aggregates user data for the current date, providing more real-time views.</p>"},{"location":"data-architecture/framework-architecture/#kappa-architecture","title":"Kappa Architecture","text":"<p>a drawback of Lambda architecture is its complexity. Kappa architecture, proposed by Jay Kreps, is a simplified version of Lambda architecture.</p> <p>In this architecture, the streaming service serves as the only primary data source, thus paving the way for an event-based architecture.</p>"},{"location":"data-architecture/framework-architecture/#advantages","title":"Advantages","text":"<p>In Kappa architecture, a streaming processing engine continuously processes real-time data and ingests it into long-term storage. When code changes occur, developers can recompute using the raw data stored in the event logs database.</p>"},{"location":"data-architecture/framework-architecture/#challenges_1","title":"Challenges","text":"<ul> <li>Streaming remains a challenge for many companies due to its complexity and most likely high cost and maintainance.</li> <li>Managing duplicates and preserving order, for instance, can be more challenging than batch processing.</li> <li>data replay is often trickier than it may seem.</li> </ul>"},{"location":"data-architecture/storage-architecture/","title":"Storage Architecture","text":""},{"location":"data-architecture/storage-architecture/#data-lake","title":"Data Lake","text":"<p>A data lake is a popular data architecture comparable, to a data warehouse. It\u2019s a storage repository that holds a large amount of data, but unlike a data warehouse where data is structured, data in a data lake is in its raw format.</p> Topic Data Lake Data Warehouse Data Format Store unstructured, semi-structured and structured data in its raw format. Store only structured data after the transformation. Schema Schema-on-read: Schema is defined after data is stored. Schema-on-write: Schema is predefined prior to when data is stored. Usecase Data exploration: Unstructured data opens more possibilities for analysis and ML algorithms, A landing place before loading data into a data warehouse. Reporting: Reporting tools and dashboards prefer highly coherent data. Data Quality Data is in its raw format without cleaning, so data quality is not ensured. Data is highly curated, resulting in higher data quality. Cost Both storage and operational costs are lower. Storing data in the data warehouse is usually more expensive and time-consuming. <p>The following graph illustrates the key components of a data lake</p> <p></p> <ul> <li> <p>Ingestion layer: The ingestion layer collects raw data and loads them into the data lake. The raw data is not modified in this layer.</p> </li> <li> <p>Processing layer: Data lake uses object storage to store data. Object storage stores data with metadata tags and a unique identifier, making searching and accessing data easier. Due to the variety and high volume of data, a data lake usually provides tools for features like data catalog, authentication, data quality, etc.</p> </li> <li> <p>Insights layer: The insights layer is for clients to query the data from the data lake. Direct usage could be feeding the reporting tools, dashboards, or a data warehouse.</p> </li> </ul>"},{"location":"data-architecture/storage-architecture/#example-aws-data-lake","title":"Example: AWS data lake","text":"<p>AWS data lake uses Simple Storage Service (S3) as the data lake foundation. We can set up a secure data lake in days with AWS Lake Formation.</p> <p></p> <p>Users can use tools like AWS Glue, a serverless data integration service, to discover, prepare, and combine data for analysis and machine learning algorithms.</p> <p>Other tools like Amazon Athena (an interactive query service), Amazon EMR (big data platform), Amazon Redshift (cloud data warehouse), etc., are also available for users.</p>"},{"location":"data-architecture/storage-architecture/#data-mesh","title":"Data Mesh","text":"<p>The term data mesh was coined by Zhamak Dehghani in 2019 and created the idea of domain-oriented decentralization for analytical data. Centrally managed architectures tend to create data bottlenecks and hold back analytics agility. On the other hand, completely decentralized architectures create silos and duplicates, making management across domains very difficult.</p>"},{"location":"data-architecture/storage-architecture/#how-it-works","title":"How it works","text":"<ul> <li>The data mesh architecture proposes distributed data ownership, allowing teams to own the entire life cycle of their domains and deliver quicker analyses.</li> <li>The organization's IT team is responsible for the overall infrastructure, governance, and efficiency without owning any domain-related business.</li> <li>Adopting data mesh requires some pretty cultural and organizational changes.</li> <li>Currently, no template solutions for a data mesh, so many companies are still trying to figure out if it's a good fit for their organizations.</li> </ul> <ul> <li>Each domain team is responsible for ingesting the operational data and building analytics models.</li> <li>The domain team agrees with the rest on global policies to safely and efficiently interact with the other domains within the mesh.</li> <li>A centralized data platform team builds infrastructures and tools for domain teams to build data products and perform analysis more effectively and quickly.</li> </ul>"},{"location":"data-architecture/storage-architecture/#principles","title":"Principles","text":"<ul> <li> <p>Domain ownership: Each domain team takes responsibility for the entire data life cycle.</p> </li> <li> <p>Data as a product: Treat provided data as a high-quality product, like APIs to other domains.</p> </li> <li> <p>Self-serve data platform: Build an effective data platform for domain teams to build data products quickly.</p> </li> <li> <p>Federated governance: Standardize data policies to create a healthy data ecosystem for domain interoperability.</p> </li> </ul>"},{"location":"data-architecture/storage-architecture/#example-google-dataplex","title":"Example: Google Dataplex","text":"<p>Dataplex is a data management platform that can quickly build data domains within a data mesh. It allows teams to create data domains while maintaining central controls on monitoring and governance.</p> <p>Domain-related assets, such as BigQuery tables, code, logs, pipelines, etc., can be logically grouped in a so-called Dataplex lake without physically moving data assets into a separate storage system.</p> <p>The lakes enable engineers to standardize data, which builds the foundation for managing metadata and setting up policies and monitoring tools.</p> <p></p> <p>Note</p> <p>Data architectures have more variations, such as event-based architecture, data lakehouse, modern data stack, etc.</p> <p>As data engineers, we learn new trends in data architecture and help organizations move faster and save time, effort, and money in a sustainable way.</p>"},{"location":"data-engineering/","title":"Data Engineering Life Cycle","text":"<p>Data engineering is a broad domain. Data engineers are responsible for developing and maintaining end-to-end systems that involve various technologies and require considerable engineering skills. Given the continuous emergence of new technologies, keeping pace with every tool can be challenging.</p>"},{"location":"data-engineering/#what-is-the-data-engineering-life-cycle","title":"What is the data engineering life cycle?","text":"<p>The data engineering life cycle involves a series of processes, which can be categorized into five groups:</p> <ul> <li>Ingestion</li> <li>Transformation</li> <li>Visualization</li> <li>Storage</li> <li>DataOps/security/infrastructure</li> </ul> <p></p> <p>Overview:</p> <ul> <li>The life cycle begins with getting data from multiple sources and ingesting it into the storage.</li> <li>Next, data is transformed from its original format into a format for the downstream use cases.</li> <li>Last, the transformed data is fed into different business intelligence (BI) tools to draw business insights.</li> </ul> <p>Info</p> <p>Reference: Educative - Data Engineering Life Cycle</p>"},{"location":"data-engineering/dataops/","title":"DataOps/Security/Infrastructure","text":"<p>To support all the above activities, we must have a solid infrastructure foundation. Moreover, the field of data engineering is quickly growing with new concepts like data contracts and data products. So, we shouldn't only be concerned with basic infrastructure areas such as CI/CD, architecture, and orchestration, but also new areas like DataOps, data lineage, data catalog, cost optimization, and more.</p> <p></p>"},{"location":"data-engineering/dataops/#dataops","title":"DataOps","text":"<p>DataOps is a methodology that combines automation and a process-oriented perspective to deliver high-quality and on-demand data to the organization. It extends the principles of DevOps, including agile methodology, continuous delivery, and automation to data-related tasks.</p>"},{"location":"data-engineering/dataops/#security","title":"Security","text":"<p>Since 2016, the EU has adopted the General Data Protection Regulation (GDPR) and recognized it as a law across the EU. In summary, GDPR gives individuals more control over how their personal data can be used by third parties.</p> <p>Example</p> <p>Examples of personal data are name, location, IP address, biometric data, and any other data that can lead to the identification of a person.</p> <p>Under GDPR law, individuals have more rights over their data: the right to be informed, the right to be erased, the right to restrict processing, the right to object, and more.</p> <p>But for data engineers, the main challenges come from three aspects of GDPR:</p> <ul> <li> <p>Data privacy: Personally identifiable information (PII) data must be treated as sensitive data. A series of security measures need to be in place to protect PII data from unauthorized access.</p> </li> <li> <p>Right to be forgotten: Users can request the company to delete all the users' data at any time they want. The data team needs to establish a guideline for processing erasure requests on time.</p> </li> <li> <p>Right to request: Users can retrieve all the information the company collected at any time they want. Similarly, a standard procedure needs to be in place for managing such requests.</p> </li> </ul>"},{"location":"data-engineering/dataops/#infrastructure","title":"Infrastructure","text":"<p>Infrastructure is the backbone of everything. A robust infrastructure includes but is not limited to networks, virtual machines, load balancers, IAM roles, and more.</p> <p>Info</p> <p>As the number of cloud resources continues to grow, there arises a need for more efficient and automated ways to manage and deploy infrastructure. In recent years, infrastructure as code (IaC) tools like Ansible and Terraform have been invented to define and manage infrastructure components using code. It ensures consistent deployment of infrastructure across environments and accelerates the deployment process.</p>"},{"location":"data-engineering/ingestion/","title":"Ingestion","text":"<p>Data ingestion is the process of importing data from one or more source systems into the storage layer. The following code from the Google Analytics example creates a BigQuery job to create a table from the raw CSV file:</p> load_to_bigquery.py<pre><code># Ingestion - load raw data into BigQuery\nwith open(\"ga.csv\", \"rb\") as source_file:\n    job = client.load_table_from_file(\n        source_file,\n        src_table_id,\n        job_config=job_config,\n    )\njob.result()\nsrc_table = client.get_table(src_table_id)\nprint(\"Loaded {} rows and {} columns to {}\".format(src_table.num_rows, len(src_table.schema), src_table_id))\n</code></pre> <p>Tip</p> <p>Ingestion is a critical and challenging step because source systems and their data quality are typically out of data engineers' direct control. Therefore, establishing good collaboration with the source table and implementing data quality checks is essential for ensuring smooth integration with the other systems.</p>"},{"location":"data-engineering/ingestion/#saas","title":"SaaS","text":"<p>Applications like Workday, Asana, Google Ads, and HubSpot have become valuable data sources for businesses. Instead of manually fetching the data, many data teams use tools like Stitch, Fivetran, and Segment, which offer out-of-the-box data connectors. These platforms are generally designed with low-code or no-code functionalities and are user-friendly for engineers and non-engineers.</p> <p></p>"},{"location":"data-engineering/ingestion/#open-source-tools","title":"Open-source tools","text":"<p>Another type of ingestion tool is open-sourced tools, such as Python client, Airbyte, Kafka, Flink, Materialize, etc. Compared to SaaS tools, they require more engineering skills for development and maintenance, but they are cost-effective and scalable. As the business grows, the number of data sources will increase exponentially, and open-sourced tools can be easily scaled on-demand and effectively handle fluctuating workloads. Besides, many of them support both batch ingestion and streaming ingestion.</p> <p></p>"},{"location":"data-engineering/storage/","title":"Storage","text":"<p>In many ways, how data is stored determines how it is used. For example, data in a data warehouse is typically used by batch processes and analytics, while frameworks like Apache Kafka facilitate real-time use cases. They offer not only storage capabilities but also function as an ingestion and query system.</p> <p>Info</p> <p>Generally speaking, there are 4 standard storage systems.</p>"},{"location":"data-engineering/storage/#data-warehouse","title":"Data Warehouse","text":"<p>A traditional data warehouse is a central data hub for reporting and analytics. Data in the data warehouse is generally structured and formatted for analytical purposes. Data flows into the data warehouse from transactional systems and other sources regularly.</p> <p></p> <p>A typical data warehouse has 3 tiers:</p> <ul> <li>database server (where data is loaded and stored)</li> <li>analytics engine (where data is transformed for analytics usage, OLAP)</li> <li>Front-end client as BI tools</li> </ul> <p>Note</p> <p>OLAP uses multidimensional database structures, known as cubes, to answer complex business questions by preaggregating underlying datasets.</p>"},{"location":"data-engineering/storage/#data-lake","title":"Data lake","text":"<p>Another comparable storage type is data lake. Rather than only storing structured data, a data lake is capable of storing both structured and unstructured data at any scale. Data can come from applications, IoT devices, mobile apps, and social media in any format. This open format gives data engineers greater flexibility, leading to faster decision-making.</p>"},{"location":"data-engineering/storage/#drawbacks","title":"Drawbacks","text":"<p>However, data lake comes with its own challenges. Due to the free format, data in data lake can easily get \u201cmessy\u201d without good data management strategies, such as data catalog and data security. Therefore, it\u2019s important to keep data organised to avoid turning the lake into a \u201cdata swamp.\u201d</p> <p></p> <p>Tip</p> <p>Data lakes and data warehouses are not mutually exclusive. In many organizations, raw data first lands in the lake. After some exploration and preparation, selected data will be transformed and loaded into the data warehouse for reporting and analysis.</p>"},{"location":"data-engineering/storage/#database","title":"Database","text":"<p>Every storage type we have discussed so far is a database. But the database in this category refers to an OLTP (online transactional processing) database.</p> <p>OLTP databases are application databases that process day-to-day operations in real-time, such as creating a new booking record. It\u2019s well known for its speed. Data engineers would choose OLTP databases as an internal database for orchestration tools like Airflow or data science applications.</p> <p></p> <p>Note</p> <p>Let\u2019s make sure we understand the difference between OLTP and OLAP databases. OLTP is designed for managing transactional data and is better at handling a high volume of short and frequent transactions, while OLAP is better at handling complex queries involving joins and aggregations. Choosing an OLAP database as the application's backend will significantly decrease application performance.</p>"},{"location":"data-engineering/storage/#object-storage","title":"Object Storage","text":"<p>With the rise of cloud computing, object storage gained popularity as a scalable and flexible solution for storing and accessing data across distributed environments. In this context, object refers to files, images, videos, and audio. Each object is a distinct unit bundled with metadata and a unique identifier to locate and retrieve each data unit across regions.</p> <p></p>"},{"location":"data-engineering/transformation/","title":"Transformation","text":"<p>The data from the source system is typically considered raw and requires transformation, enrichment, and aggregation before it can serve downstream use cases.</p> <p>In traditional settings, developers build data pipelines in the order of extract, transform, and load (ETL). In the ETL process, data transformation is performed outside the data warehouse, usually in the source database. The data must be standardized before being loaded into the data warehouse.</p> <p></p> <p>In recent years, the computational power of cloud data warehouses has increased, promoting a shift toward a different pattern: ELT (extract, load, transform). Unlike the traditional ETL approach, where data is transformed externally, ELT involves pushing raw data directly to the data warehouse and performing transformations within the warehouse itself. The scalability and strong computational power of the data warehouse make these internal data processing and transformations much more efficient. As a result, ELT approach has become more widespread than ETL approach.</p>"},{"location":"data-engineering/transformation/#data-tools","title":"Data tools","text":"<p>Data transformation tools have grown in popularity in recent years. They help data engineers automate and simplify the transformation process. One such tool is dbt, which enables data engineers and analytics engineers to transform data by simply writing SQL-like statements. It also follows software engineering best practices like testing, CI/CD, and documentation, making it fast and high-quality. Streaming frameworks like Apache Kafka and Flink support real-time transformation. They immediately process data when it reaches the system.</p> <p></p>"},{"location":"data-engineering/visualization/","title":"Visualization","text":"<p>Once the data has been transformed into the desired format, the next step is creating visual representations that are more easily understood by stakeholders.</p> <p>Note</p> <p>Typically, data analysts are responsible for this.</p> <p>As data engineers, we are often involved in setting up the visualization platforms and ensuring the freshness and availability of the dashboards and reports.</p>"},{"location":"data-engineering/visualization/#data-tools","title":"Data tools","text":"<p>There are many data visualization tools available. These tools can vary considerably. They range from software like Tableau and Looker that emphasizes simplicity and variety of visualizations to more complex frameworks like Plotly and Matplotlib that require higher technical skills.</p> <p></p> <p>Example using <code>matplotlib</code></p> aggregation.py<pre><code># Transformation - perform aggregation\ndf = pd.read_gbq(query=\"SELECT * FROM ga.google_analytics_agg\", project_id=PROJECT_ID)\ndf.plot.bar(x='date', y='num_visitors', rot=0)\nplt.xticks(rotation=90)\nplt.savefig('output/graph.png')\n</code></pre>"},{"location":"data-ingestion/","title":"Data Ingestion","text":"<p>Data Ingestion is the first stage in most data architecture designs. The process has 2 steps. First, it consumes data from assorted sources. Second, it loads data into centralized storage, which can be accessed and used by the organization.</p> <p>Warning</p> <p>it's a critical component in the data engineering because downstream systems rely entirely on the ingestion layer's output.</p> <p></p> <p>The ingestion layer works with various data sources, which data engineers typically don't have full control of.</p> <p>Note</p> <p>A good practice is building a layer of data quality checks and a self-healing system to react to unexpected situations, such as data loss, corruption, system failure, etc.</p> <p>Generally, there are several types of data ingestion, as below:</p> <ul> <li>Batch Ingestion</li> <li>Streaming Ingestion</li> <li>Push vs. Pull</li> </ul> <p>Various methods to perform data ingestion:</p> <ul> <li>Secure File Transfer Protocol (SFTP)</li> <li>Application Programming Interface (API)</li> <li>Object Storage</li> <li>Change Data Capture (CDC)</li> <li>Streaming Platform</li> </ul> <p>Reference: Educative - Data Ingestion</p>"},{"location":"data-ingestion/batch/","title":"Batch Ingestion","text":"<p>Batch Ingestion is a commonly used way to ingest data. It processes data in bulk, meaning that a subset of data from the source system is extracted and loaded into the internal data storage based on the time interval or the size of accumulated data.</p>"},{"location":"data-ingestion/batch/#time-based-vs-size-based-batch-ingestion","title":"Time-based vs. size-based batch ingestion","text":"<p>Time-based batch ingestion often processes data on a fixed time interval (e.g., once a day) to provide periodic reporting. It's often used in traditional business ETL or ELT for data warehousing, such as getting daily transactions from a payment provider.</p> <p></p> <p>Size-based batch ingestion is used when the time interval doesn't really matter, but the size of bulk or the number of records in the batch matters. It ensures a consistent load on the internal storage, reducing the risk of overloading the database with a huge bulk.</p>"},{"location":"data-ingestion/batch/#full-snapshot-vs-incremental-load","title":"Full snapshot vs. Incremental load","text":"<p>When consuming data from the source, we can choose whether we want to take full snapshot of the source each time or consume only the difference.</p> <p>Taking a full snapshot means extracting the entire current state of the source system.</p> <p>Note</p> <p>A benefit is ensuring data completeness and simple application logic.</p> <p>However, the downside would be the resource-intensive operations as the data grows.</p> <p></p> <p>On the other hand, incremental load only consumes a subset of source data, which minimizes resource usage. One implementation method is to load the data from the last interval.</p> <p>Example</p> <p>Extracting the transactions that happened in the last hour.</p> <p>Warning</p> <p>There's 1 caveat. If the ingestion code didn't run last time, we would lose the data in that hour because we only look at the recent/last hour.</p> <p></p> <p>Success</p> <p>An improved version is to look at the target system's latest timestamp and use it as the starting point of the new incremental window.</p> <p>In the previous diagram, the code didn't run from 10 a.m. to 12 p.m.</p> <p>Therefore, the latest timestamp in the target systems is somewhere around 10 a.m.</p> <p>When the schedule runs at 1 p.m., it will read data all the way from 10 a.m. to fill the gap</p> <p></p> <p>This approach mitigates the risk of losing data, but it comes with the complexity of finding out the latest timestamp in the target system.</p> <p>Tip</p> <p>Finding the maximum value of a column can be quite expensive because it has to scan the entire column.</p> <p>A tip is to sort the column or partition the table on that column.</p> <p>Batch ingestion is the most widely used form of data ingestion when real-time data is not required.</p> <p>An example is the payroll and billing systems that need to be processed monthly or quarterly.</p> <p>Warning</p> <p>Keep in mind that batch ingestion requires more hardware resources to process large data batches and has hours or days latency depending on the time interval.</p>"},{"location":"data-ingestion/methods/api/","title":"Application Programming Interface (API)","text":"<p>An Application Programming Interface (API) is a way for a service to communicate with other services without knowing how they are implemented.</p> <p>API works as a contract, an agreement between the service provider and the client. If the client sends a request in a particular format, the provider will respond in the agreed way. Such an agreement helps the provider and client collaborate and simplifies the ingestion process.</p> <p></p> <p>Note</p> <p>The above diagram shows a pull-based API ingestion solution in which the client uses the HTTP <code>GET</code> method to fetch data from the provider.</p>"},{"location":"data-ingestion/methods/api/#challenge-ingest-bitcoin-price-data-into-google-bigquery-from-coindesk-api","title":"Challenge: Ingest Bitcoin price data into Google BigQuery from CoinDesk API","text":"<p>This example creates a data ingestion pipeline using free CoinDesk API. The pipeline pulls real-time Bitcoin price data in JSON format from CoinDesk API and writes into BigQuery table.</p> <p>Warning</p> <p>You need to have Google Cloud Platform account to run below code. You can create here</p> requests.py<pre><code>import requests\nimport pandas as pd\nimport pandas_gbq\n\ntable_id = f\"{dataset_id}.bitcoin_price\"\ntry:\n    res = requests.get(\"https://api.coindesk.com/v1/bpi/currentprice.json\")\n    res.raise_for_status()\n    price = res.json()\n    df = pd.json_normalize(res.json(), sep=\"_\")\n    pandas_gbq.to_gbq(df, table_id, project_id=PROJECT_ID, if_exists=\"append\")\n    print(f\"Data has been loaded to table {table_id}.\")\nexcept Exception as e:\n    raise SystemExit(e)\n</code></pre> <p>Danger</p> <p>if you notice the code above, it's on <code>append</code> mode instead of <code>overwrite</code>.</p> <p>In <code>append</code> mode, a new row is added to the table every time the code runs. With the <code>append</code> mode, deduplication is essential due to possible retries during the ingestion.</p>"},{"location":"data-ingestion/methods/api/#client-library-coindesk","title":"Client library: CoinDesk","text":"<p>Most modern service providers like Meta, Yelp, and Stripe offer APIs to access their data efficiently. However, writing an API client from scratch requires advanced engineering skills and a huge amount of time.</p> <p>To not reinvent the wheel, the community has developed client libraries in different languages that hide complex API implementation details, making API interaction easy and efficient.</p> coindesk.py<pre><code># Library https://pypi.org/project/coindesk/\nfrom coindesk.client import CoinDeskAPIClient\napi_client = CoindeskAPIClient.start('currentprice')\nresponse = api_client.get(raw=True)\n</code></pre>"},{"location":"data-ingestion/methods/api/#streaming-api","title":"Streaming API","text":"<p>What we've seen before is REST API, a client-server architecture. This comes down to the flow of \"request and response\".</p> <p>The client sends a request, and the server responds. The operation is repeated and independent for every data request.</p> <p></p> <p>On the other hand, Streaming API is a subscription mechanism that allows the real-time streaming of event messages.</p> <p>The client only needs to send one request, and the server continues to push data in a single long-lived connection. An advantage of Streaming API is receiving events in near real-time. Applications that constantly do long polling, consuming unnecessary API calls and processing time, would benefit from Streaming API because it reduces the number of calls that return nothing.</p> <p>Example</p> <p>A well-known use case of the Streaming API is Twitter (now known as X). X\u2019s Streaming API allows developers to stream public tweets with different filters in real-time.</p> <p>To connect to the streaming API, developers need to form a HTTP request and read stream as long as it\u2019s valid.</p>"},{"location":"data-ingestion/methods/cdc/","title":"Change Data Capture (CDC)","text":"<p>The process of ingesting changes from a source database. It provides real-time or near real-time data movement by moving data continuously as new database events occur.</p> <p>CDC is a very efficient way to move data across a wide area network, perfect for the cloud.</p> <p>Here are a few examples:</p> <ul> <li>Load real-time data into a data warehouse</li> </ul> <p>Operational databases are not good for heavy analytical workloads.   Therefore, operational data should be moved to a data warehouse to perform analysis.   The traditional batch-based ETL has a latency issue.   But with CDC, we can capture source data changes as they occur and deliver them to the data warehouse in real time.</p> <ul> <li>Load real-time data into real-time frameworks</li> </ul> <p>Database events can be delivered to real-time process engines like Apache Kafka and Apache Flink to apply transformations and provide real-time insights.</p> <ul> <li>Data replication/synchronization</li> </ul> <p>The source database might be located on an on-premises server. We can use CDC to capture and propagate every data change to the cloud. It can also be used to sync servers within the cloud.</p> <p></p>"},{"location":"data-ingestion/methods/cdc/#implementations","title":"Implementations","text":""},{"location":"data-ingestion/methods/cdc/#time-based","title":"Time-based","text":"<p>Every row in the source has a timestamp column named <code>update_timestamp</code> (or other similar names), which stores the timestamp of when the record was recently updated.</p> <p>Every update on the row will overwrite the <code>update_timestamp</code> column.</p> <p>The target system polls on an interval and updates the target database accordingly.</p> <p>3 steps of the time-based CDC approach are:</p> <ul> <li>Get the maximum value of <code>update_timestamp</code> in the target system. This was when the last sync happened.</li> <li>Select all the rows from the source with <code>update_timestamp &gt; the target's maximum timestamp</code>.</li> <li>Insert or modify existing rows in the target system.</li> </ul>"},{"location":"data-ingestion/methods/cdc/#pros","title":"Pros","text":"<ul> <li>The logic can be implemented purely in SQL.</li> <li>No external tool is required.</li> </ul>"},{"location":"data-ingestion/methods/cdc/#cons","title":"Cons","text":"<ul> <li>DML statements such as <code>DELETE</code> won't propagate to the target system because there's no <code>updated_timestamp</code> for a deleted row.</li> <li>For a wide table, the <code>SELECT *</code> statement may scan irrelevant data if only a few columns get updated.   It requires extra hardware resources to update the entire rows in the target table.</li> </ul>"},{"location":"data-ingestion/methods/cdc/#trigger-based","title":"Trigger-based","text":"<p>With a trigger-based approach, three triggers are created for each source table, listening to the <code>INSERT</code>, <code>UPDATE</code>, and <code>DELETE</code> events respectively.</p> <p>For each event, the corresponding trigger inserts one row into a changelog table.</p> <p>The changelog table is stored in the database itself and is a sequence of state-changing events.</p> <p>3 steps of the trigger-based CDC approach:</p> <ul> <li>Create 3 triggers for each source table.</li> <li>Whenever a row's state changes, a corresponding trigger appends a new row to the changelog table.</li> <li>The changelog table propagates the changes to the target table instantly.</li> </ul>"},{"location":"data-ingestion/methods/cdc/#pros_1","title":"Pros","text":"<ul> <li>We can define the trigger function and log table schema as we wish.</li> <li>The changes can be captured almost as soon as they are made.</li> </ul>"},{"location":"data-ingestion/methods/cdc/#cons_1","title":"Cons","text":"<ul> <li>Transactions can be slowed by the extra triggers.</li> <li>Moreover, triggers must be defined for each table, which can cause overhead if the source table is replicated multiple times.</li> </ul>"},{"location":"data-ingestion/methods/cdc/#challenge-build-a-postgres-audit-trigger","title":"Challenge: Build a postgres audit trigger","text":"<p>A use case of the database event trigger is sending changes to an audit log table. The table records the old and new records, the user who made the change, and a timestamp of the change.</p> <p>In this example, we will build a Postgres audit trigger in the GCP environment with its Cloud SQL service</p> <p>Warning</p> <p>You need to have Google Cloud Platform account to run below code. You can create here</p> <ul> <li>Create a postgres instance</li> </ul> <pre><code>gcloud sql instances create my-db \\\n--database-version=POSTGRES_9_6 --cpu=1 \\\n--memory=3840MiB --zone=us-central1-a \\\n--root-password=admin\n</code></pre> <ul> <li>Connect to the Postgres instance   After few minutes, the Postgres database will be created.   Next, let's connect to the database using the <code>gcloud</code> command.</li> </ul> <pre><code>apt-get update\napt-get install postgresql-client\ngcloud sql connect my-db --user=postgres\n</code></pre> <ul> <li>Run SQL to create source table, audit table and triggers</li> </ul> <pre><code>-- createa audit table\nCREATE SCHEMA audit;\nCREATE TABLE audit.logged_actions (\n    op CHAR(1) NOT NULL,\n    stamp TIMESTAMP NOT NULL,\n    user_id CHAR(20) NOT NULL,\n    first_name VARCHAR(20),\n    last_name VARCHAR(20),\n    salary INT\n);\n\n-- create source table\nDROP TABLE IF EXISTS employees;\nCREATE TABLE employees(\n   first_name VARCHAR(40) NOT NULL,\n   last_name VARCHAR(40) NOT NULL,\n   salary INT\n);\n\n-- create function\nCREATE OR REPLACE FUNCTION audit_employee_change() RETURNS TRIGGER AS $employee_audit$\nBEGIN\n    IF (TG_OP = 'DELETE' ) THEN INSERT INTO audit.logged_actions SELECT 'D', now(), user, OLD.*;\n    ELSIF (TG_OP = 'UPDATE') THEN INSERT INTO audit.logged_actions SELECT 'U', now(), user, NEW.*;\n    ELSIF (TG_OP = 'INSERT') THEN INSERT INTO audit.logged_actions SELECT 'I', now(), user, NEW.*;\n    END IF;\n    RETURN NULL;\nEND;\n$employee_audit$\nLANGUAGE plpgsql;\n\n-- create trigger using the function\nCREATE TRIGGER employee_audit_trigger AFTER INSERT OR UPDATE OR DELETE ON employees\nFOR EACH ROW EXECUTE PROCEDURE audit_employee_change();\n</code></pre> <ul> <li>Let's perform the <code>INSERT</code>, <code>UPDATE</code>, and <code>DELETE</code> operations in the <code>employees</code> table</li> </ul> <pre><code>INSERT INTO employees values ('Alice','Liu',5000);\nINSERT INTO employees values ('Bob', 'Li', 7000);\nUPDATE employees SET salary=8000 WHERE first_name='Alice';\nDELETE FROM employees where first_name='Bob';\n</code></pre> <ul> <li>Finally, let's check the audit table.</li> </ul> <pre><code>SELECT * FROM audit.logged_actions;\n</code></pre> <ul> <li>After finishing the Postgres exercise, run this command to delete the instance</li> </ul> <pre><code>gcloud sql instances delete my-db\n</code></pre>"},{"location":"data-ingestion/methods/cdc/#log-based","title":"Log-based","text":"<p>Most online transaction processing (OLTP) uses a transaction log to record all the data changes in the database. In case of a database crash, the transaction log will be used to fully recover the system. With log-based CDC, events are directly read from the source database's native transaction logs instead of creating a separate log table like a trigger-based approach.</p> <p></p>"},{"location":"data-ingestion/methods/cdc/#pros_2","title":"Pros","text":"<ul> <li>It doesn't add additional tables or require additional queries for each transaction, thus having no impact on the data model.</li> </ul>"},{"location":"data-ingestion/methods/cdc/#cons_2","title":"Cons","text":"<ul> <li>Parsing transaction logs is a challenge.</li> </ul> <p>Most databases have different log formats, sometimes they don't document them very well. This would potentially require us to change log parsing logic whenever a new database version is released.</p> <ul> <li>Adding an additional log level can increase performance overhead</li> </ul>"},{"location":"data-ingestion/methods/object-storage/","title":"Object Storage","text":"<p>In a data lake architecture, data is ingested into object storage. Object storage stores data with metadata, making it easier to locate and retrieve data and improving performance. Object storage is also an optimal and secure way to exchange files on the cloud.</p> <p>Warning</p> <p>Dumping all data into the lake will result in a \"data swamp\"</p> <p>Few challenges with the object storage:</p> <ul> <li>optimizing storage</li> <li>improving observability</li> <li>hardening security</li> </ul>"},{"location":"data-ingestion/methods/object-storage/#challenge-ingest-files-into-gcp-cloud-storage","title":"Challenge: Ingest files into GCP cloud storage","text":"<p>In this example, we will ingest both structured and unstructured files into GCP cloud storage. There are many ways to ingest files into GCP cloud storage, and we will use the bash script.</p> <p>Warning</p> <p>You need to have Google Cloud Platform account to run below code. You can create here</p> <ul> <li> <p>Download the CSV files from a dataset called <code>on-time performance data</code>, which is about the air carrier's quality of service.</p> </li> <li> <p>Unzip and inspect the file view some early records of the CSV.</p> </li> </ul> <pre><code>curl https://www.bts.dot.gov/sites/bts.dot.gov/files/docs/legacy/additional-attachment-files/ONTIME.TD.201501.REL02.04APR2015.zip --output data.zip\napt-get install unzip\nunzip data.zip\nhead ontime.td.201501.asc\n</code></pre> <ul> <li>Create a Google Cloud Storage bucket using the command</li> </ul> <pre><code>gsutil mb -l us-central1 gs://{bucket_name}\n</code></pre> <ul> <li>Ingest the file into the bucket using the command</li> </ul> <pre><code>gsutil -m cp ontime.td.201501.asc gs://{bucket_name}/raw\n</code></pre>"},{"location":"data-ingestion/methods/sftp/","title":"Secure File Transfer Protocol (SFTP)","text":"<p>A network protocol that provides file access, file transfer, and file management over any reliable data stream. The SSH protocol supports encryption and other security methods to better protect file transfers.</p> <p>Example</p> <p>Many financial institutions use SFTP to send customer transaction files for security reasons</p>"},{"location":"data-ingestion/methods/sftp/#fetch-files-from-a-local-sftp-server","title":"Fetch files from a local SFTP server","text":"<ul> <li>You need to connect to the local SFTP server: initiate connection to the local SFTP, usually by executing these commands</li> </ul> <pre><code>sudo passwd &lt;password&gt;\nsudo service ssh start\nsftp &lt;username&gt;@&lt;server IP&gt;\n</code></pre> <ul> <li>Enter the new password when prompted. You're connected to SFTP server.</li> <li>Download a sample file: Fetch a sample file from the server using the following command</li> </ul> <pre><code>get -r readme.txt\n</code></pre> <ul> <li> <p>Disconnect: When finished or download a sample file, exit the SFTP session by typing the <code>!</code> symbol.</p> </li> <li> <p>Verify the downloaded file: Confirm that the file has been successfully downloaded by using the following command</p> </li> </ul> <pre><code>cat readme.txt\n</code></pre>"},{"location":"data-ingestion/methods/streaming-platform/","title":"Streaming Platform","text":"<p>Streaming platforms are widespread ways to ingest real-time data. Compared to the CDC, where the data source is mostly databases, a streaming platform is a more universal solution to receive real-time events such as data from IoT sensors, retail, web, and mobile applications. Data is continuously generated by data sources in small batches.</p>"},{"location":"data-ingestion/methods/streaming-platform/#apache-kafka","title":"Apache Kafka","text":"<p>Apache Kafka is a popular distributed streaming platform for building real-time data pipelines. It's well known for its high throughput, high scalability, high availability, fault tolerance, and low latency. Kafka has a variety of use cases, including real-time fraud detection, online activity tracking, and operational metrics collection.</p> <p>Kafka consists of a storage layer and a compute layer. It supports a large number of external data sources such as AWS S3, BigQuery Sink, Github source, etc., and the data is stored as topics. Each topic can be split into several partitions for parallel processing across the cluster.</p> <p>Next to the cluster, there are producers and consumers</p> <p>Info</p> <p>producer act as an interface between data source and topics.</p> <p>consumer read and process data in the topics.</p> <p></p>"},{"location":"data-ingestion/methods/streaming-platform/#challenge-streaming-data-ingestion-pipeline-using-websocket-and-apache-kafka","title":"Challenge: Streaming data ingestion pipeline using WebSocket and Apache Kafka","text":"<p>Kafka producer listens to the Websocket and ingests messages to the topic in real time.</p> <ul> <li>Start a cluster with one ZooKeeper node a one Kafka node in terminal tab 1.   Download the Apache Kafka binary and start the cluster in the background.</li> </ul> <pre><code># download Apache Kafka\nwget https://archive.apache.org/dist/kafka/3.4.0/kafka_2.12-3.4.0.tgz\ntar -xvzf kafka_2.12-3.4.0.tgz\ncd kafka_2.12-3.4.0\nnohup ./bin/zookeeper-server-start.sh config/zookeeper.properties &gt; zookeeper.log &amp;\nnohup ./bin/kafka-server-start.sh config/server.properties &gt; kafka.log &amp;\n# press ctrl C to exit\n</code></pre> <ul> <li>Check if the ZooKeeper 2181 and Kafka port 9092 are in use</li> </ul> <pre><code>netstat -tulpn | grep LISTEN\n# tcp        0      0 0.0.0.0:9092            0.0.0.0:*               LISTEN      2661/java\n# tcp        0      0 0.0.0.0:2181            0.0.0.0:*               LISTEN      2660/java\n</code></pre> <p>Note</p> <p>Since Apache Kafka 3.3.1, we can deploy a kafka cluster without ZooKeeper and use KRaft.</p> <p>KRaft is a consensus protocol developed to manage metadata directly in Kafka.</p> <p>Reference: KRaft: Apache Kafka Without ZooKeeper</p> <p></p> <ul> <li>Create a Kafka topic</li> </ul> <p>Create Kafka topic <code>websocket-events</code> in terminal tab 1. By default, the topic only has 1 partition and 1 replication. They can be configured via flag <code>--partitions</code> and <code>--replication-factor</code>.</p> <pre><code>./bin/kafka-topics.sh --create --topic websocket-events --bootstrap-server localhost:9092\n# Created topic websocket-events.\n</code></pre> <ul> <li>Create a WebSocket and produce messages</li> </ul> <p>Open a new tab, terminal tab 2, and start a WebSocket connection.</p> <pre><code>wget -qO /usr/local/bin/websocat https://github.com/vi/websocat/releases/latest/download/websocat.x86_64-unknown-linux-musl\nchmod a+x /usr/local/bin/websocat\nwebsocat --version\nwebsocat -s 1234\n</code></pre> <ul> <li>Create a Kafka producer that sends WebSocket messages to the topic</li> </ul> producer.py<pre><code>from kafka import KafkaProducer\nfrom websocket import create_connection\nproducer = KafkaProducer(bootstrap_servers='localhost:9092')\n\ndef websocket_source():\n    \"\"\"\n    1. Connect to websocket stream and get live data as json.\n    2. Receives the stream.\n    3. Sends the stream as a producer.\n    \"\"\"\n    ws = create_connection('ws://127.0.0.1:1234/')\n    while True:\n        try:\n            messages = ws.recv()\n            if messages:\n                producer.send(\"websocket-events\", messages.encode('utf-8'))\n        except Exception as e:\n            print(e);\n            websocket_source()\n\nif __name__ == '__main__':\n    websocket_source()\n</code></pre> <ul> <li>Open a new tab, terminal tab 3, and execute the following command to run python code above.</li> </ul> <pre><code>python producer.py\n</code></pre> <ul> <li>Read messages from the topic</li> </ul> <p>Finally, let's check the messages on the topic. Go to terminal tab 1, and run the following command to start a Kafka console consumer. We should see the messages from the WebSocket in real time. <code>--from-beginning</code> means that the consumer always starts reading from the first message in the topic. Otherwise, it will start from where it left off last time.</p> <pre><code>./bin/kafka-console-consumer.sh --topic websocket-events --from-beginning --bootstrap-server localhost:9092\n</code></pre>"},{"location":"data-ingestion/methods/streaming-platform/#other-open-source-streaming-platforms","title":"Other open-source streaming platforms","text":"<p>In fact, Apache Kafka is a message queue with a couple of twists. Other similar message queues are:</p> <ul> <li> <p>RabbitMQ: A push-based message-queueing system. It can be used when web servers need to quickly respond to requests.</p> </li> <li> <p>ActiveMQ: A push-based message-queueing system. It's mostly used in enterprise projects to store multiple instances and support clustering environments.</p> </li> <li> <p>KubeMQ: A Kubernetes native message queue broker. Following the container architecture best practices, KubeMQ nodes are stateless and ephemeral.</p> </li> </ul> <p></p>"},{"location":"data-ingestion/methods/streaming-platform/#google-cloud-pubsub","title":"Google Cloud Pub/Sub","text":"<p>The concept of Pub/Sub is very similar to Apache Kafka, where the data is stored in topics.</p> <p>A subscription is a named entity that is interested in receiving messages on a particular topic. It's similar to the concept of the consumer group in Kafka. Subscribers are consumers who receive part or all of the messages on a specific subscription.</p> <p></p> <p>Info</p> <p>Pub/Sub has native integration with other GCP services, such as BigQuery and DataFlow.</p>"},{"location":"data-ingestion/methods/streaming-platform/#amazon-kinesis","title":"Amazon Kinesis","text":"<p>An AWS service to process large-scale data streams from multiple sources in real time. With Amazon Kinesis, we can ingest real-time data such as video, audio, text, application logs, website clickstreams, IoT devices for analytics, and other use cases.</p>"},{"location":"data-ingestion/methods/streaming-platform/#azure-event-hubs","title":"Azure Event Hubs","text":"<p>Big data streaming platform and event ingestion service in Microsoft Azure</p> <p></p> <p>Tip</p> <p>When it comes to choosing a streaming platform, there are plenty of options, each with its advantages and disadvantages.</p> <p>The key performance indicators for streaming platforms are throughput, latency, availability, and reliability.</p> <p>Client language support can be another factor because developers can build applications in different languages.</p> <p>maintainability should also be considered to have a hassle-free management experience.</p>"},{"location":"data-ingestion/push-pull/","title":"Push vs. Pull","text":"<p>When looking at ingestion from a network communication perspective, there are two main strategies:</p> <ul> <li>push strategy involves a source system sending data to a target.</li> <li>pull strategy involves a target reading data directly from a source.</li> </ul>"},{"location":"data-ingestion/push-pull/#push","title":"Push","text":"<p>In push-based ingestion, data is pushed from the source to the target as soon as it becomes available. The source can be an active source generating a huge amount of data, like an IoT device, or a less active source, like a chat application.</p> <p></p>"},{"location":"data-ingestion/push-pull/#advantages","title":"Advantages","text":"<ul> <li> <p>Real time: Whenever the source receives new data, it immediately pushes it to the destination, and no request is needed.   Push ingestion is more efficient for sources that constantly produce data.</p> </li> <li> <p>Immutable data: With a push-based solution, data is always immutable, which is suitable for auditing purposes.</p> </li> <li> <p>Security: Source systems are more secure in push-based solutions because they don't listen for network connection.   All the requests are authenticated on the consumer side, so there is less chance for the source system to get attacked.</p> </li> </ul>"},{"location":"data-ingestion/push-pull/#disadvantages","title":"Disadvantages","text":"<ul> <li> <p>Replayability: The source system will only publish each message once.   If the consumer misses some messages, it\u2019s hard to get them back.</p> </li> <li> <p>Difficult flow control: In a push system, the source or the intermediate engine controls the flow rate.   Consumers might be overwhelmed if the consumption rate falls far behind the production rate.   It\u2019s also tricky for producers to fine-tune every consumer\u2019s flow rate.</p> </li> <li> <p>Passive consumer: The fact that consumers are not able to control how they receive data introduces other inconveniences like not being able to define batch size.   The producer lacks the knowledge of whether data should be sent one-by-one or batched first.</p> </li> </ul>"},{"location":"data-ingestion/push-pull/#implementations","title":"Implementations","text":""},{"location":"data-ingestion/push-pull/#websocket","title":"WebSocket","text":"<p>A bidirectional connection through which both the server and client can push data. It facilitates the real-time transfers from the source to the target and vice versa. It is underlying technology of many chat applications and real-time platforms.</p> <p></p> <p>A WebSocket connection is established by an HTTP handshake request from the client to the server. Once the server accepts it, a long-lived bidirectional TCP connection is established. In real time, messages are pushed from one side to the other until one party closes the connection.</p> <p>Example</p> <p>we will build a WebSocket connection between a server and a client inside the terminal.</p> <p>This simplified example demonstrates how the server and client interact.</p> <p>Danger</p> <p>This example is compatible to run only on <code>Linux</code></p> <ul> <li>Download the <code>websocat</code> package with the following commands.</li> </ul> <pre><code>sudo wget -qO /usr/local/bin/websocat https://github.com/vi/websocat/releases/latest/download/websocat.x86_64-unknown-linux-musl\nsudo chmod a+x /usr/local/bin/websocat\nwebsocat --version\n</code></pre> <ul> <li>Open a new terminal tab, run</li> </ul> <pre><code>websocat -s 1234\n</code></pre> <p>it will start a server on the port <code>1234</code></p> <ul> <li>Open another a new terminal tab, run</li> </ul> <pre><code>websocat ws://127.0.0.1:1234/\n</code></pre> <p>it will create a client listening to port <code>1234</code></p> <ul> <li>Now the connection has been established between server and client, you can type anything on either side and should receive them on other other side.</li> </ul> <p>Info</p> <p>WebSocket is a simple but powerful technology.</p> <p>Many financial data providers share real-time stock prices through a WebSocket.</p>"},{"location":"data-ingestion/push-pull/#example-use-case","title":"Example use case","text":"<p>Ingest real-time stock prices or bitcoin prices from Twelve Data, a financial data provider, into a database. The following pseudocode implements a client using Twelve Data python client. It opens a socket connection to Twelve Data, consumes data, and ingests into a database through callback function <code>on_event</code>. Inside <code>on_event</code> function, we can batch records before inserting them into database.</p> client.py<pre><code>from twelvedata import TDClient\n\ndef on_event(event):\n    print(event)\n    # code to insert records into DB\n\ntd = TDClient(apikey=\"TWELVE_DATA_APIKEY\")\nws = td.websocket(symbols=[\"BTC/USD\", \"ETH/USD\"], on_event=on_event)\nws.connect()\nws.keep_alive()\n</code></pre>"},{"location":"data-ingestion/push-pull/#http-postput","title":"HTTP <code>POST/PUT</code>","text":"<p>WebSocket is a bidirectional protocol supporting continual data transmission, and it's mainly used in real-time applications. However, WebSocket shouldn't be taken on board if the source generates data only once or in a non-real-time manner.</p> <p>An alternative is the HTTP protocol, specifically the POST and PUT methods.</p> <p>HTTP is a unidirectional protocol functioning above the TCP protocol. In WebSocket, communication occurs on both ends, making it a faster protocol. In HTTP, the connection is only built on one side, making it slower than WebSocket. Another difference is that WebSocket maintains an active connection until one party drops, while HTTP needs to build a distinct connection for separate requests.</p> <p>However, WebSocket is more complex than HTTP. HTTP is a better option if the client only wants to receive updates every 30 minutes or so.</p> <p>HTTP offers different methods, among which the POST and PUT methods are push-style. Although the target can't control the speed of the source, it can set a rate limit not to overload the system. There are endless use cases for using HTTP to ingest data.</p> <p>Example</p> <p>BigQuery offers the <code>POST</code> and <code>PUT</code> endpoints for creating tables and inserting data.</p> <p>Example</p> <p>In a push subscription, the target is an HTTP endpoint.</p> <p>When Pub/Sub delivers a message to the push endpoint, it puts the message in the body of a POST request.</p> <p>After the consumer receives the message, it will return a status code <code>200</code> to acknowledge the message.</p> <p></p>"},{"location":"data-ingestion/push-pull/#pull","title":"Pull","text":"<p>In pull-based ingestion, the target system sends a request to the source or an intermediary and gets a response with data or without any. Unlike push-based solutions, where the target can't control the flow rate, the pull mechanism allows the consumer to fetch data in a controlled manner.</p> <p>The consumer can fall behind and catch up whenever they want.</p> <p></p> <p>There are two pull styles with some nuances: poll and pull.</p> <p>Polling is a quick request that periodically checks whether new information can be pulled.</p> <p>Pulling is a slower request that moves data from the source to the target.</p> <p>In many cases, the client first polls the server and quickly finds out whether new data can be pulled. If so, it will pull the data.</p>"},{"location":"data-ingestion/push-pull/#advantages_1","title":"Advantages","text":"<ul> <li> <p>Scalability: Pull-based solution allows consumers to easily start a new instance as needed, e.g., on our own laptop.   To not overload the source system, we can set up an intermediate queuing system, such as Redis, Pub/Sub, and Apache Kafka, to handle read requests from multiple consumers.</p> </li> <li> <p>Flexible consumer: Each consumer can pull data at their own pace.   They can also have diverse requirements on batch size, time interval, etc.</p> </li> </ul>"},{"location":"data-ingestion/push-pull/#disadvantages_1","title":"Disadvantages","text":"<ul> <li> <p>Less real-time: Pull is a bidirectional communication.   The process of making requests and pulling data can use more bandwidth, resulting in a long waiting time.</p> </li> <li> <p>Operational complexity: Firewalls must be configured to allow bidirectional communication between the consumer and producer.   Besides, authentication happens on the source system, so it's critical to set up a strong security layer to protect source data.</p> </li> </ul>"},{"location":"data-ingestion/push-pull/#implementations_1","title":"Implementations","text":""},{"location":"data-ingestion/push-pull/#http-get","title":"HTTP <code>GET</code>","text":"<p>The HTTP <code>GET</code> method is a common way to pull data from the source. The consumer initiates a request to the source. After the source processes the request, it replies with the result to the consumer.</p> <p></p> <p>Warning</p> <p>Establishing an HTTP connection each time creates an overhead, so it's mostly used by consumers who want less frequent updates than WebSocket.</p>"},{"location":"data-ingestion/push-pull/#apache-kafka","title":"Apache Kafka","text":"<p>Like most messaging systems, Apache Kafka chooses to pull data from brokers to consumers. Consumers request the brokers for all available messages from their current position. In case of falling behind, it will catch up.</p> <p>Note</p> <p>Kafka supports multiple consumers for each topic (queue), and each consumer won\u2019t affect the performance of the other consumers.</p> <p></p>"},{"location":"data-ingestion/push-pull/#example-use-case_1","title":"Example use case","text":"<p>Kafka consumer using the <code>confluent_kafka</code> python library. The consumer is a long-running application that continuously polls kafka for more data. <code>timeout</code> is a timeout interval that controls how long the <code>poll()</code> will block if data is unavailable. If it's set to zero, <code>poll()</code> will immediately return.</p> consumers.py<pre><code>from confluent_kafka import Consumer, TopicPartition\n\nconsumer = Consumer(\n    {\n        'bootstrap.servers': 'localhost:9092',\n        'group.id': 'mygroup',\n        'auto.offset.reset': 'earliest',\n    }\n)\n\n# indefinite loop to pull data from brokers\ndef consume(consumer, timeout):\n    while True:\n        message = consumer.poll(timeout)\n        if message is None:\n            continue\n        if message.error():\n            print(\"Consumer error: {}\".format(msg.error()))\n            continue\n        yield message\n    consumer.close()\n\nconsumer.subscribe(['topic1'])\nfor msg in consume(consumer, 1.0):\n    print(msg)\n</code></pre> <p>Note</p> <p>Apache Kafka uses a variance of pull mechanism called long pulling. The client polls the server requesting new information. The server holds the request until new data is available.</p> <p>Once available, the server sends the new data.</p> <p>When the client receives it, it immediately makes a new request and repeats the process.</p>"},{"location":"data-ingestion/streaming/","title":"Streaming Ingestion","text":"<p>As opposed to batch ingestion, stream ingestion handles real-time events. Data is consumed and loaded as soon as it's created from the source. Streaming solutions are booming in the market because more companies want real-time insights.</p> <p>Example</p> <p>an online retail company wants to provide a personalized user experience by using users' online activities.</p> <p>As users browse the website, we can ingest real-time activities into a streaming framework and show useful and relevant recommendations to users during their online shopping.</p>"},{"location":"data-ingestion/streaming/#drawbacks","title":"Drawbacks","text":"<p>However, ingesting streaming data can be pretty challenging.</p> <ul> <li>There is no staging area as in the batch solution.</li> <li>Data is brought to the consumer instantly, so it\u2019s hard to ensure consistency.</li> <li>After the source generates an event, it may never get to the destination due to network issues, causing data loss.</li> <li>There can also be a scenario where an event is retried multiple times, causing duplication.</li> </ul>"},{"location":"data-ingestion/streaming/#message-delivery-semantics","title":"Message delivery semantics","text":"<p>Streaming platforms like Apache Kafka and Google Pub/Sub support three message delivery semantics:</p> <ul> <li>at-most-once</li> <li>at-least-once</li> <li>exactly-once</li> </ul> <p>Their main difference is the way to handle failures caused by communication errors or other disruptions, which has an impact on the level of data integrity, performance, implementation and cost.</p> <p></p>"},{"location":"data-ingestion/streaming/#at-most-once","title":"at-most-once","text":"<p>at-most-once means a message is delivered zero or at most one time.</p> <p>Danger</p> <p>data might be lost.</p> <p>This semantic is the cheapest, and it has high throughput, and low latency due to its fire-and-forget nature, meaning the transport layer doesn't keep any state. At-most-once is best for applications that need high performance and accept small amounts of data loss, such as monitoring metrics.</p>"},{"location":"data-ingestion/streaming/#at-least-once","title":"at-least-once","text":"<p>at-least-once allows multiple attempts in case of failure, so at least one attempt is successful.</p> <p>It works by maintaining the message state on the producer side, waiting on acknowledgment from the target, and possibly retrying if the message is not successfully delivered, but the same message might be delivered multiple times, and the performance is limited.</p> <p>Danger</p> <p>at-least-once is the default setting of many streaming systems because deduplication is often possible on the target side.</p> <p>Common use cases are chat applications, real-time machine learning models, etc.</p>"},{"location":"data-ingestion/streaming/#exactly-once","title":"exactly-once","text":"<p>exactly-once is the most difficult delivery semantic to implement.</p> <p>With exactly-once, a message will always be delivered only one time, meaning no data loss or duplicate. In addition to maintaining the state on the producer side, exactly-once generates a unique ID for each message and maintains a state on the target side to filter out duplicates.</p> <p>Warning</p> <p>exactly-once carries the highest implementation overhead and possibly the worst performance of all the delivery semantics.</p> <p>That being said, exactly-once is necessary for many financial-related use cases, such as payment, trading, and downstream services that don't support deduplication and idempotency.</p>"},{"location":"data-ingestion/streaming/#comparison-of-delivery-semantics","title":"Comparison of Delivery Semantics","text":"at-most-once at-least-once exactly-once Performance Very high High Medium Duplication No Possible No Data Loss Possible No No Use case Monitoring metrics Chat app, ML models Financial-related (payment, trading, accounting)"},{"location":"data-ingestion/streaming/#error-handling","title":"Error Handling","text":"<p>Most batch-based ingestion performs data transactions, serving as a single unit of work, meaning the entire batch is ingested as a whole or not at all. Any error record will block the entire batch, so there's never a case that part of the records is ingested. In other words, we must fix the data issue before ingesting the batch.</p> <p>Tip</p> <p>However, we don't want a single failure to block streaming ingestion.</p> <p>Messages that cannot be ingested for whatever reason should be rerouted and possibly stored in a separate location.</p> <p>Invalid schema, message size, communication errors, etc cause common ingestion errors.</p>"},{"location":"data-ingestion/streaming/#drop-the-error-records","title":"Drop the error records","text":"<p>The simplest solution is to drop any messages that couldn't be ingested. This is preferred when we handle a high volume of data and don't want to store useless messages.</p> <p>Warning</p> <p>However, rejecting records is risky because the error detection mechanism can be faulty. If so, useful data will be lost forever.</p> <p></p>"},{"location":"data-ingestion/streaming/#send-error-records-to-a-dead-letter-queue","title":"Send error records to a dead-letter queue","text":"<p>One industry-standard approach is sending error records to a dead-letter queue. A dead-letter queue receives messages that meet one or more of the following criteria:</p> <ul> <li>The target system doesn't exist</li> <li>The queue is full</li> <li>The message size exceeds the limit</li> <li>The message is expired due to TTL (time to live)</li> <li>The message cannot be processed due to an invalid format or wrong values</li> </ul> <p></p> <p>Dead-letter queues isolate messages that can't be processed, allowing engineers to investigate why ingestion errors occur and solve pipeline problems if necessary. With at-least-once delivery semantics, the system will retry rejected messages a few attempts before sending them to the dead-letter queue.</p> <p>Note</p> <p>The benefits brought by the streaming pipeline attract many organizations, but it's considerably challenging to develop and maintain.</p> <p>To simplify the architecture and reduce the burden of engineers, many cloud providers have developed out-of-the-box solutions like BigQuery subscription and Dataflow, helping engineers focus on the core business values.</p>"},{"location":"data-modeling/","title":"Data Modeling","text":"<p>As data engineers, one of our main objectives is to drive the business forward. However, sometimes stakeholders know what they want, but they struggle to articulate it and transfer their domain knowledge into a database design. This is where data modeling comes in. It helps us translate business requirements into a visual representation of data and the relationships among data points.</p> <p>Data can be modeled at various levels of abstraction. The process starts by collecting requirements from business users.</p> <p>Info</p> <p>These requirements are then translated into a database design consisting of entities and attributes.</p> <p></p> <p>A good data model captures how work naturally flows within the organization. A better data model can accommodate business changes without disrupting the foundational database design.</p>"},{"location":"data-modeling/#use-case","title":"Use Case","text":"<p>We will build a data model for a large grocery chain. The business has 100 stores spread out worldwide. Each store has roughly 1000 individual products. Customers may or may not use their memberships to purchase products. The business aims to design a database system to keep track of sales and use the data to maximize profits.</p>"},{"location":"data-modeling/#conceptual-data-models","title":"Conceptual Data Models","text":"<p>The data modeling process starts with high-level abstraction, namely a conceptual model.</p> <p>Info</p> <p>A conceptual data model provides a big picture of the system's contents and business rules, without going into the details of the database.</p> <p>It focuses on defining entities and the relationships between them. A typical way to visualize a conceptual model is through <code>Entity-Relationship (ER) diagrams</code>.</p>"},{"location":"data-modeling/#define-business-process","title":"Define business process","text":"<p>To begin the design process, the first step is to determine which business process to model. A business process refers to a low-level activity carried out by the organization, such as taking orders, receiving payments, or processing customer requests.</p> <p>Tip</p> <p>In this case, the business process is sales transactions.</p>"},{"location":"data-modeling/#determine-key-entities","title":"Determine key entities","text":"<p>Next, we will determine key entities. Each sale is linked to a few objects:</p> <ul> <li><code>Store</code>: The store where the transaction happened.</li> <li><code>Product</code>: The product with its unit price and description.</li> <li><code>Customer</code>: The customer who executes the transaction with a membership.</li> </ul> <p></p>"},{"location":"data-modeling/#relationships-and-restrictions-between-entities","title":"Relationships and Restrictions between entities","text":"<p>Few requirements from business:</p> <ul> <li>Each transaction is linked to a maximum of one membership account. If the customer is not a member, it won't be linked to any account.</li> <li>Each transaction contains at least one type of product.</li> <li>Each transaction is linked to one and only one store.</li> </ul> <p></p> <p>A conceptual data model is independent of any specific technology. It is a great tool for creating a shared understanding of data and business requirements. It helps stakeholders identify potential data issues and ensures that data is organized in a consistent way that supports the business.</p> <p>Info</p> <p>A conceptual data model serves as a foundation for creating logical and physical data models.</p>"},{"location":"data-modeling/#logical-data-models","title":"Logical data models","text":"<p>A logical data model is a representation of a possible implementation of the conceptual model without being tied to any specific technology. This stage is crucial because although the conceptual model is easier to communicate, its lack of context can make implementation challenging.</p> <p>Note</p> <p>The logical model expects more details, such as entity attributes, granularity, primary keys, foreign keys, normalization, and column descriptions.</p>"},{"location":"data-modeling/#entity-attributes","title":"Entity attributes","text":"<p>Entity attributes describe the characteristics of an entity. The following is an example of entity attributes:</p> <ul> <li>Store: <code>Store id</code>, <code>Store name</code>, <code>Address</code></li> <li>Customer: <code>Customer id</code>, <code>Customer name</code>, <code>Customer address</code></li> <li>Product: <code>Product id</code>, <code>Product name</code>, <code>Unit price</code>, <code>Description</code></li> <li>Sales: <code>Transaction id</code>, <code>Product id</code>, <code>Customer id</code>, <code>Store id</code>, <code>Price</code></li> </ul>"},{"location":"data-modeling/#primary-key-and-foreign-key","title":"Primary key and foreign key","text":"<p>A primary key is a column that uniquely identifies every row in the table. A foreign key is a column that creates a relationship between two tables.</p> <p>Normally, a primary key comes with a few constraints:</p> <ul> <li>A primary key never accepts NULL values.</li> <li>A primary key must contain unique values.</li> <li>Each table can only have one primary key, can be composite one.</li> </ul> <p></p> <p>In above example, the <code>Person</code> table uses <code>Passport id</code> as the primary key and <code>City id</code> as the foreign key, which is the primary key of <code>City</code> table.</p> <p>Tip</p> <p>If rows cannot be uniquely identified with one primary key, it is possible to create a composite primary key. This key is composed of two or more columns and ensures the uniqueness of each row.</p>"},{"location":"data-modeling/#granularity","title":"Granularity","text":"<p>Granularity refers to the level of detail at which data are stored in a table. The lower the granularity, the more details can be captured in the model.</p> <p>For the <code>Sales</code> table, we have several options for its granularity, ranked from low to high:</p> <ul> <li>One row per scan of an individual product on a customer's sales transactions.</li> <li>One row per total sales of a product type on a customer's sales transactions.</li> <li>One row per total sales on a customer's sales transactions.</li> </ul> <p>For this use case, the 2nd option is more approriate. The 3rd option aggregates all product sales in each transaction, but this would require remodeling the data when business users request sales data per product.</p> <p>Tip</p> <p>Generally, we should aim to model our data at the lowest possible level of granularity. From there, it's easy to aggregate data to any higher grain.</p> <p>Although the 1st option has the lowest grain, it may result in excessive duplicates.</p> <p>Example</p> <p>a purchase of ten apples of the same kind will create 10 rows with identical information.</p> <p>Defining granularity is not an easy task because the model must be flexible enough to answer various analytics questions without providing redundant information.</p>"},{"location":"data-modeling/#normalization","title":"Normalization","text":"<p>Normalization is another database design technique used to remove data redundancy within a relational database. This is accomplished by decomposing a large table into smaller, logical units.</p> <p>Let's see what a <code>Sales</code> table looks like without normalization.</p> <p></p> <p>One downside of using a denormalized table is the increased complexity of inserting, updating, and removing data. Given that the table contains duplicated information, such as customer and store names, inserting and updating data can result in additional overhead and require more storage space.</p> <p>From the use case, we can find multiple ID in the <code>Sales</code> table, such as:</p> <ul> <li><code>Transaction id</code>: identify transaction</li> <li><code>Product id</code>: identifier for each product</li> <li><code>Customer id</code>: identifier for each customer</li> <li><code>Store id</code>: identifier for each store</li> </ul> <p>The idea of normalization is splitting the big table into smaller table, to remove duplicated information and easier, faster to retrieve the data based on analytics use cases.</p> <p>Note</p> <p>There's a risk if we split the table into smaller one, then we would need JOIN method to achieve the goals, and JOIN can cause excessive data reading and need more resources.</p> <p>The wise decision to consider pros and cons between Normalization approach but keeping the data in a good state (faster and easy to retrieve), this is the essence of data modeling.</p> <p></p> <p>As the use case, we will need to normalize the table into 4 tables (<code>Products</code>, <code>Customers</code>, <code>Stores</code> and <code>Transactions</code>)</p> <p>Having said that, here is the visualization of the logical data model.</p> <p></p>"},{"location":"data-modeling/#physical-data-models","title":"Physical data models","text":"<p>Physical data modeling is the final stage of data modeling. It provides a database-specific representation of the logical data model, allowing engineers to visualize the structure of the database before it is built.</p> <p></p> <p>Some deeper topics of data modeling, such as:</p> <ul> <li>Dimensional Modeling</li> <li>DDL vs DML</li> <li>Query Life Cycle</li> </ul> <p>Reference: Data Modeling by Educative</p>"},{"location":"data-modeling/ddl-vs-dml/","title":"DDL vs. DML","text":"<p>SQL statements are broadly categorized into two groups: <code>Data Definition Language (DDL)</code> and <code>Data Manipulation Language (DML)</code>.</p>"},{"location":"data-modeling/ddl-vs-dml/#ddl","title":"DDL","text":"<p>DDL is a set of commands used to create, modify, and delete database objects such as tables, views, indexes, and constraints. Here are a few DDL commands:</p> <ul> <li><code>CREATE</code>: Create new database objects such as tables.</li> <li><code>ALTER</code>: Modify existing database objects such as adding or removing columns from the table.</li> <li><code>DROP</code>: Delete a database object such as a table or index.</li> <li><code>TRUNCATE</code>: Delete all data from a table without deleting the table itself.</li> <li><code>RENAME</code>: Rename an existing database object, like a table or column.</li> </ul> <pre><code>-- Create table\nCREATE TABLE fruits_price (\n    fruit varchar(255),\n    price float\n);\nCREATE TABLE exchange_rate (\n    from_cur varchar(255),\n    to_eur varchar(255),\n    rate float\n);\n\n-- Add column\nALTER TABLE fruits_price\nADD COLUMN category varchar(255) AFTER price;\n\n-- Drop table\nDROP TABLE exchange_rate;\n\n-- Truncate table\nTRUNCATE TABLE fruits_price;\n\n-- Rename column\nALTER TABLE fruits_price\nCHANGE price price_eur float;\n</code></pre>"},{"location":"data-modeling/ddl-vs-dml/#dml","title":"DML","text":"<p>DML is a set of commands used to manipulate the data. Here are a few DML commands:</p> <ul> <li><code>SELECT</code>: Retrieve data from tables or views.</li> <li><code>INSERT</code>: Add one or more records to a table.</li> <li><code>UPDATE</code>: Modify data of one or more records.</li> <li><code>DELETE</code>: Remove one or more records from the table.</li> <li><code>MERGE</code>: Handle inserts, updates, and deletes all in a single transaction without writing separate logic for each of these.</li> </ul> <pre><code>-- Select\nSELECT 'Select statement' as '';\nSELECT * FROM fruits_price;\n\n-- Insert\nSELECT 'INSERT/UPDATE/DELETE statement' as '';\nINSERT INTO fruits_price (fruit, price)\nVALUES ('blueberry',7.0);\n\n-- Update\nUPDATE fruits_price\nSET price = 1.5\nWHERE fruit = 'apple';\n\n-- Delete\nDELETE FROM fruits_price\nWHERE fruit = 'banana';\nSELECT * FROM fruits_price;\n\n-- MYSQL doesn't support merge, here is the alternative\nSELECT 'REPLACE(MERGE) statement' as '';\nCREATE TABLE src_fruits_price (\n    fruit varchar(255),\n    price float\n);\nINSERT INTO src_fruits_price (fruit, price) -- create a source table\nVALUES ('watermelon', 7.0), ('lime', 3.5);\nREPLACE INTO fruits_price (fruit, price)\nSELECT fruit,price FROM src_fruits_price;\nSELECT * FROM fruits_price;\n</code></pre>"},{"location":"data-modeling/dimensional-modeling/","title":"Dimensional Modeling","text":""},{"location":"data-modeling/dimensional-modeling/#operational-database","title":"Operational database","text":"<p>In every organization, information serves two purposes: operational record-keeping and analytical decision-making. Operational systems store operational data, such as taking online orders, resolving customer tickets, and collecting website traffic. Data is generated and continually updated in real-time as the business proceeds.</p> <p>Operational databases, also known as Online transaction processing databases (OLTP), are optimized for processing transactions and providing the best application performance to users.</p> <p>Info</p> <p>Additionally, operational database extensively use normalized structure because an update or insert transaction only affects one table in a database.</p>"},{"location":"data-modeling/dimensional-modeling/#drawbacks","title":"Drawbacks","text":"<ul> <li>While operational databases prioritize performance, they are not the optimal choice for facilitating decision-making.</li> <li>Business users often deal with multiple transactions at once, for instance, comparing data from different time periods and manipulating it in various ways.</li> <li>Using normalized data models only introduces unnecessary complexity because users must navigate and remember all the models and build complicated join logic themselves, which can hinder efficiency and impede company growth.</li> </ul>"},{"location":"data-modeling/dimensional-modeling/#analytical-database","title":"Analytical Database","text":"<p>known as online analytical processing (OLAP) database, are optimized for handling complex SQL logic. This capability is a result of the underlying parallel processing engines, query optimization and elastic hardware resources.</p> <p>In contrast to OLTP, which only preserve the latest object state, OLAP databases maintain the historical data to support various analysis spanning an extended timeframe.</p> <p>Info</p> <p>Moreover, external-facing tables are typically denormalized in OLAP databases because we want to provide an efficient and simplified way for users to retrieve data.</p>"},{"location":"data-modeling/dimensional-modeling/#kimballs-dimensional-modeling","title":"Kimball's Dimensional Modeling","text":"<p>Raplh Kimball introduced the concept of dimensional modeling in the 1980s. Due to its star-like structure, it is also referred as the star schema. The center of the star is the fact table, surrounded by dimension tables.</p> <p></p>"},{"location":"data-modeling/dimensional-modeling/#fact-table","title":"Fact table","text":"<p>The fact table stores the measurements or metrics from the business process. For instance, in the sales example, the business wants to measure the sales units and sales dollars. Typically, these facts are numeric and additive because downstream users rarely look at a single fact. Instead, they add up hundreds, or even millions, of rows.</p> <p></p>"},{"location":"data-modeling/dimensional-modeling/#dimension-table","title":"Dimension table","text":"<p>The dimension table contains the context associated with a business process, referring as <code>attributes</code>. They describe the \"who, what, where, when, how, and why\" associated with the event. These dimension attributes determine the ways users can segment the data, and therefore, they often appear in the <code>GROUP BY</code> query.</p> <p></p> <p>Info</p> <p>Dimension tables tend to have fewer rows than fact tables, but can be wide with text columns. As the business evolves, dimension tables can grow horizontally.</p>"},{"location":"data-modeling/dimensional-modeling/#snowflake-schema","title":"Snowflake schema","text":"<p>Reference: Databricks - Snowflake Schema</p> <p>A snowflake schema is a multi-dimensional data model that is an extension of a star schema, where dimension tables are broken down into subdimensions. Snowflake schemas are commonly used for business intelligence and reporting in OLAP data warehouse, data marts, and relational databases.</p> <p>In a snowflake schema, engineers break down individual dimension tables into logical subdimensions. This makes the data model more complex, but it can be easier for analyst to work with, especially for certain data types.</p> <p></p> <p>Note</p> <p>the main difference is that snowflake schemas are more normalized than star schemas</p>"},{"location":"data-modeling/dimensional-modeling/#benefits","title":"Benefits","text":"<ul> <li>Fast data retrieval for specific data/tables.</li> <li>Enforces data quality, avoiding duplication by normalizing each of dimension tables further.</li> <li>Simple, common data model for data warehousing.</li> </ul>"},{"location":"data-modeling/dimensional-modeling/#drawbacks_1","title":"Drawbacks","text":"<ul> <li>Lots of overhead upon initial setup, lot of tables will be built and normalized.</li> <li>Rigid data model, need to JOIN multiple tables by use cases.</li> <li>High maintenance cost, it will require more storage to save more tables.</li> </ul>"},{"location":"data-modeling/dimensional-modeling/#slowly-changing-dimension-scd","title":"Slowly Changing Dimension (SCD)","text":"<p>Slowly Changing Dimension (SCD) is an important concept in dimensional modeling. It's a strategy to handle attribute changes in the operational world. For example, when a customer changes their home address, how do we represent the information? Do we always store the latest one? Or do we keep history? It's a question for both engineers and business users.</p> <p>There are five ways to handle those changes. Let's check them out.</p>"},{"location":"data-modeling/dimensional-modeling/#type-0-retain-the-original","title":"Type 0: Retain the original","text":"<p>With SCD type 0, the dimension attribute value never changes. For example, when a customer registers for a membership, their registration attributes, such as registration date and registration place, should never change over time.</p>"},{"location":"data-modeling/dimensional-modeling/#type-1-overwrite","title":"Type 1: Overwrite","text":"<p>With SCD type 1, the old attribute value is overwritten by the current value. The attribute always reflects the most recent state. For example, when a customer just registers the membership, not all the fields are filled in, so there will be empty values in the dimension table. Later, when the customer updates their data, these empty values will be overwritten by the actual values.</p> <p></p>"},{"location":"data-modeling/dimensional-modeling/#type-2-add-a-new-row","title":"Type 2: Add a new row","text":"<p>SCD type 2 is the most popular type used in the data warehouse. It solves the problem of SCD type 1 by tracking attribute changes. With the type 2 approach, when the customer changes their home address, a new row will be inserted to reflect the new address value.</p> <p></p>"},{"location":"data-modeling/dimensional-modeling/#type-3-add-new-attributes","title":"Type 3: Add new attribute(s)","text":"<p>With SCD type 3, history is maintained in an extra column. In type 2, each row exclusively holds valid values for that period of time, which is what we want in most cases. However, sometimes, we want to know how the business would perform under the old attribute value for the entire period.</p> <p></p>"},{"location":"data-modeling/dimensional-modeling/#type-4-add-mini-dimension","title":"Type 4: Add mini-dimension","text":"<p>SCD type 2 maintains the history by adding a new row to the dimension table. However, if the changes are rapid, SCD type 2 won\u2019t be scalable within a multimillion-row dimension table. Imagine that the company has 1 million users, and some user attributes change frequently, such as age range and income level. In this context, we don't want to continually add rows to the customer dimension table.</p> <p>With SCD type 4, we break off frequently changing attributes into a separate dimension, called a mini-dimension.</p> <p></p> <p>In the first variation, each row in the Demographics table has age range and income range. We use a range instead of a specific value to limit the rows in the mini-dimension. Every time a sale occurs, the demographics ID in the fact table represents the customer demographic profile at the time of the event.</p> <p>In the second variation, we maintain a mini SCD type 2 table for the same attributes and link it to the fact table using customer id. All the attributes in the table must be rapidly changing.</p> <p>Note</p> <p>Either way, the goal is to ensure the query performance by separating slowly changing or static attributes from rapidly changing attributes. When users retrieve static attributes, the performance won't be impacted by the rapidly changing attributes.</p> <p>Example</p> <p>For further ilustration on how to build dimensional modeling, please go to Challenge. I will explain clearly step by step.</p>"},{"location":"data-modeling/dimensional-modeling/challenge/","title":"Challenge","text":"<p>To illustrate the process, consider the case of an e-scooter rental company. Users can rent a scooter from the street and ride to their destinations. The company wants to use generated data to make strategic decisions on demand forecasting, dynamic pricing, and operational management.</p>"},{"location":"data-modeling/dimensional-modeling/challenge/#step-1-select-the-business-process","title":"Step 1: Select the business process","text":"<p>Business processes are the operational activities carried out by the organization, such as buying products, registering courses, or resolving customer tickets. First, we focus on activities that are most significant to the business.</p> <p>In this example, we have several business processes in mind:</p> <ul> <li>The rider takes a ride from place A to place B.</li> <li>The rider makes a payment.</li> <li>The rider creates a customer support ticket.</li> </ul> <p>For a scooter company, the first activity brings the most business value as it's the primary source of revenue. So, we will model this activity in the next steps.</p> <p>Warning</p> <p>Picking the right business process serves as the foundation for the entire data warehouse.</p>"},{"location":"data-modeling/dimensional-modeling/challenge/#step-2-declare-the-grain","title":"Step 2: Declare the grain","text":"<p>The grain declares the level of detail in each row. We start with atomic grain, referring to the lowest level at which data is captured in every business process.</p> <p>Example</p> <p>every row represents one complete ride with one rider, riding from start to end.</p> <p>Sometimes, rolled-up summary grain is more appropriate due to its query performance. But such high-grain tables must be able to answer most of the business questions, otherwise, they will lead to more issues than benefits. In the previous sales example, the business process is the sales transaction. If we choose the lowest grain where each row represents one individual product, ten apples will generate ten duplicated rows and it is not optimal. A better solution is to roll up the same products on each transaction, which is sufficient for the business.</p>"},{"location":"data-modeling/dimensional-modeling/challenge/#step-3-identify-the-dimension","title":"Step 3: Identify the dimension","text":"<p>Dimensions provide \"what, who, why, how, where, and when\" context surrounding the business process. Most of the dimensions are nouns such as name, address, department, and store. In this example, we need the following dimension tables:</p> <ul> <li>Rider: Personal information such as rider ID, name, and address.</li> <li>City: Everything about the city, such as city name, province, market, and country.</li> <li>Vehicle: Attributes about the vehicle, such as manufacturer number, vehicle type, cost, time of birth, and status.</li> </ul> <p>Next, we choose the SCD type for each table.</p> <p>For instance, the <code>Rider</code> table adopts <code>SCD type 1</code>, given that historical rider data is irrelevant to the business.</p> <p>The <code>City</code> table adopts <code>SCD Type 2</code> because the business is interested in comparing metrics before and after any attribute change in this table.</p> <p>The <code>Vehicle</code> table is more complicated because attributes like <code>status</code> can change rapidly, so it falls into <code>SCD type 4</code>, and needs an extra mini-dimension table for rapidly changing attributes.</p> <p></p>"},{"location":"data-modeling/dimensional-modeling/challenge/#step-4-identify-the-facts","title":"Step 4: Identify the facts","text":"<p>Facts are numeric measurements derived from business processes. Here are a few example facts for the <code>Ride</code> fact table:</p> <ul> <li>Duration of the ride</li> <li>Distance of the ride</li> <li>The total cost of the ride</li> <li>Number of pauses during the ride</li> <li>Rating of the ride</li> </ul> <p>Warning</p> <p>While it's possible to create an extensive list of facts, it's important to collaborate with the business to only include those metrics that hold value for the company. Otherwise, managing \"dead\" metrics can become a burden during future maintenance.</p> <p></p>"},{"location":"data-modeling/query-lifecycle/","title":"Query Life Cycle","text":"<p>Both data engineers and data users write queries every day, but not everyone is familiar with how query works internally. While we trust the database engine to do all the magic and return us the expected result, it's still worth understanding what goes on behind the scenes, and the knowledge can further help us enhance query performance.</p>"},{"location":"data-modeling/query-lifecycle/#imperative-language-vs-declarative-language","title":"Imperative Language vs. Declarative Language","text":"<p>There are two programming paradigms: imperative and declarative. The most commonly used programming languages, like Python and Java, are imperative.</p> <p>Info</p> <p>They instruct the computer on how the code should run by giving a sequence of steps.</p> <p>On the other hand, when using a declarative programming language like SQL, we instruct the computer what kind of result we expect, such as the conditions and transformation tasks that need to be performed, and the language figures out how to execute the operations to achieve that result.</p>"},{"location":"data-modeling/query-lifecycle/#the-life-cycle-of-a-query","title":"The Life Cycle of a query","text":"<ul> <li>A SQL statement is submitted to the database via a transport layer protocol such as TCP.</li> <li>To proceed, some kind of API requests must occur, such as authenticating and authorizing the request.</li> <li>Next, the SQL statement is sent to a query parser that analyzes the syntactic and semantic meaning. As a result, a query tree is generated.</li> <li>Once the query is parsed, it is sent to a query optimizer that looks for the most efficient way to execute the query, and a query execution plan is generated afterward.</li> <li>Once the plan is in place, the database will walk through it and execute each stage in the plan.</li> <li>Finally, when the query has finished execution, the result will be sent back to users.</li> </ul>"},{"location":"data-modeling/query-lifecycle/#query-parser","title":"Query Parser","text":"<p>The query parser first performs lexical analysis where it scans the raw SQL statement and converts it into a series of tokens. It extracts relevant information such as keywords, operators, separators, literals, and comments. Then, the parser consumes those tokens and builds an Abstract Syntax Tree (AST).</p> <p></p>"},{"location":"data-modeling/query-lifecycle/#query-execution-plan","title":"Query execution plan","text":"<p>A query execution plan is a set of steps employed by a database to retrieve data in an efficient way. The plan outlines the sequence of operations that the database will follow to retrieve, manipulate, and present the requested data. The plan includes decisions on how to access tables, how to join multiple tables, and any optimizations to enhance performance.</p> <p>Note</p> <p>When receiving AST from the query parsers, the query optimizer comes up with multiple strategies and selects the one that offers the best performance in terms of time and resources, and it is the final query execution plan.</p> <p></p>"},{"location":"data-modeling/query-lifecycle/#query-processing-in-bigquery","title":"Query processing in BigQuery","text":"<p>BigQuery is a fully managed, serverless, cloud-based data warehouse, meaning that users can offload a lot of operational work that we normally do in the on-premise data warehouse to BigQuery.</p> <p>Steps:</p> <ul> <li>BigQuery allocates a set of slots to access the distributed storage and read data from the table.    BigQUery uses columnar storage, meaning that each column's data resides within one or multiple files in the distributed file system.    When running the query, only the columns needed for the query will be selected, which minimizes I/O operations.</li> </ul> <p>Info</p> <p>Besides, columnar storage allows for better compression, resulting in reduced storage space and improved data retrieval speed.</p> <ul> <li>A slot is a virtual CPU used by BigQuery to execute queries.    In this example, each slot reads one input file, applying filters on <code>start_station_name</code> and subsequently, append the count of the valid records in this file to the shuffle</li> </ul> <p></p> <ul> <li>Next, a separate slot reads from the shuffle and aggregates the counts from different input files together as the final count.    Ultimately, this result is written into an output file that is accessible to the user.</li> </ul>"},{"location":"data-modeling/query-lifecycle/#benefits","title":"Benefits","text":"<p>BigQuery has tons of internal optimization strategies to optimize query cost and performance. This can include using cached results and rewriting queries to use different algorithms. However, it doesn't optimize everything. A query execution plan can help us understand which stage uses the most resources so that we can optimize it ourselves.</p> <p>Example</p> <p>in a JOIN statement, it's wiser to do the filter before the join, instead of after the join. If there are a lot of CPU-intensive tasks, it's recommended to use BigQuery approximate aggregate functions, <code>APPROX_COUNT_DISTINCT</code> which are often within 1% of the exact function.</p>"},{"location":"data-orchestration/","title":"Data Orchestration","text":"<p>To support business continuity, data models need to be regularly refreshed. In the past, engineers used the cron tool in Linux systems to schedule ELT jobs.</p> <p>However, as data volume and system complexity increase exponentially, creating cron jobs becomes a bottleneck and eventually hits the limitation of scalability and maintainability.</p> <p>To be more specific, the problems are:</p> <ul> <li>Dependencies between jobs: In a large-scale data team, it's expected to have many dependencies between data models.</li> </ul> <p>Example</p> <p>updating the revenue table should be done only after the sales table has been updated.</p> <p>In more complicated scenarios, a table can have multiple upstream dependencies, each with a different schedule. Managing all these dependencies manually is time-consuming and error-prone.</p> <ul> <li> <p>Performance: If not managed well, cron jobs can consume a significant amount of system resources such as CPU, memory, and disk space. With the ever-increasing volume of data, performance can quickly become an issue.</p> </li> <li> <p>Engineering efforts: To maintain the quality of dozens of cron jobs or apps and process a variety of data formats, data engineers have to spend a significant amount of time writing low-level code rather than creating new data pipelines.</p> </li> <li> <p>Data silos: Scattered cron jobs can easily lead to data silos, resulting in duplicated efforts, conflicting data, and inconsistent data quality. Enforcing data governance policies can also be difficult, leading to potential security issues.</p> </li> </ul> <p></p> <p>The emergence of data orchestration marks a significant step in the evolution of modern data stacks.</p> <p>Data orchestration is an automated process that combines and organizes data from multiple sources, making it ready for use by data consumers. Orchestrators ease the workload of data engineering teams by providing prebuilt solutions for scheduling, monitoring, and infrastructure setup.</p>"},{"location":"data-orchestration/#tasks-of-data-orchestration","title":"Tasks of data orchestration","text":"<p>The actual tasks of data orchestration can vary from system to system, but essentially it consists of 3 parts as described:</p> <ul> <li>Data collection</li> <li>Data unification</li> <li>Data activation</li> </ul> <p>Note</p> <p>it's worth noting that data orchestration is not a database engine.</p> <p>It's a platform that schedules different jobs to run at the right time, in the right order and in the right way.</p> <p>It's common for a company to use multiple data orchestration platforms. This is because each platform may perform different tasks and target different users.</p> <p>Example</p> <p>For instance, tools like Airflow and Prefect are used for creating and managing complex workflows and are, therefore, mostly used by data engineers.</p> <p>On the other hand, **dbt* is focused on the data unification stage and is heavily adopted by data analysts and data scientists.</p> <p>Note</p> <p>One of the challenges of using multiple data orchestration platforms is managing the dependency and lineage between the platforms, if any exist.</p> <p>Fortunately, tool integration is becoming increasingly common.</p> <p></p>"},{"location":"data-orchestration/#data-collection","title":"Data collection","text":"<p>Within an organization, data may come from multiple sources and in various formats. Adapting every single format from every single source can be time-consuming. Data orchestration automates the process of collecting data from disparate sources with little to no human effort.</p> <p>Example</p> <p>Many data orchestration platforms have easy integration with various tools such as Google Sheets, CSV files, BigQuery, Zendesk support, which speeds up the onboarding process of a new data source.</p> <p>This is expandable to other complex data format, such as Parquet, Avro, ORC, or delta format.</p> <p>Data orchestration provides the additional advantage of reducing data silos. The platform can swiftly access data from anywhere, whether it's from legacy systems, data warehouses, or the cloud.</p> <p>This prevents data from being trapped in a single location and makes it easily accessible.</p>"},{"location":"data-orchestration/#data-unification","title":"Data unification","text":"<p>Data inevitably needs to be unified and converted into a standard format.</p> <p>There are three types of platforms in this category, and all the platforms allow users to schedule jobs.</p> <ul> <li>Platforms that don't interfere with the data transformation logic.   Users can use any method to transform data, such as SQL, Python, or Bash scripts.</li> </ul> <p>Example</p> <p>Airflow is only responsible for submitting the data transformation job created by the user to a data warehouse and waiting for the result. It doesn't care how the user implements the job.</p> <ul> <li> <p>Platforms that manage the data transformation logic.   Platforms like dbt define their own way of transforming data and managing model dependency within the tool. They guide users to use optimized methods to transform data.</p> </li> <li> <p>Platforms that manage the data transformation logic through UI.   Certain low-code or no-code tools only require users to define transformation logic through user-friendly UI. These tools open the doors to non-engineers, but their functionalities may be limited.</p> </li> </ul>"},{"location":"data-orchestration/#data-activation","title":"Data activation","text":"<p>Business users who use the dashboards can define Service Level Agreements (SLA), meaning that the data must be ready before a certain time.</p> <p>Note</p> <p>If not, the orchestrator will notify the stakeholders through one of the communication channels.</p> <p>In addition to the above steps, orchestration platforms monitor the data life cycle in real time through a UI, where users can manually intervene with the process if needed.</p> <p>Another critical feature is dependency management. Before going into specific platforms, it's important to understand an important concept in data orchestration: directed acyclic graph (DAG).</p>"},{"location":"data-orchestration/#directed-acyclic-graph-dag","title":"Directed Acyclic Graph (DAG)","text":"<p>DAG is the bedrock of managing dependencies between different tasks.</p> <p>A DAG is a graphical representation of a series of tasks in the data pipeline. By its name, we can tell that it is a graph, but with two conditions:</p> <ul> <li> <p>Directed: Every edge in the graph points in one direction.</p> </li> <li> <p>Acyclic: The graph doesn't have directed cycles.</p> </li> </ul> <p>Considered the DAG below. In this DAG, data can go from A to B, but never from B to A. Nodes from which an edge extends are upstream nodes, while nodes at the receiving end of the edge are downstream nodes. In a DAG, nodes can have multiple upstream and downstream nodes.</p> <p></p> <p>Danger</p> <p>In addition, nodes can never inform themselves because this could create an infinite loop.</p> <p>For example, data cannot go from E to A. Otherwise, A becomes the downstream of E as well as the upstream.</p>"},{"location":"data-orchestration/#advantages","title":"Advantages","text":"<p>DAG is a fundamental concept in data orchestration for a few reasons:</p> <ul> <li> <p>It ensures that there is no infinite loop in the data pipeline. The scenario where the pipeline could run forever will not happen.</p> </li> <li> <p>It ensures the consistent execution order of tasks. Tasks will be executed in the same order every day.</p> </li> <li> <p>The graph helps us to visualize the dependencies in a user-friendly way. We can break down a complex task into smaller subtasks, making it easier to debug and maintain.</p> </li> <li> <p>It opens the possibility of parallelism. Independent tasks can be executed in parallel. For example, nodes B and C can run simultaneously.</p> </li> </ul> <p>Info</p> <p>In general, data orchestration connects the dots and enables data to flow in a consistent and manageable way.</p> <p>With a solid orchestration platform in place, organizations can quickly and efficiently scale their data volumes and deliver high-quality data to stakeholders for better decision-making.</p> <p>Reference: Educative - Data Orchestration</p>"},{"location":"data-orchestration/airflow/","title":"Airflow","text":"<p>We will learn about most famous data orchestration platform in nowadays: Airflow.</p> <p>When it comes to learning data orchestration tools, Airflow is a must-have. Airflow is a widely recognized open-source workflow management tool for data engineering pipelines. It has become the go-to choice for many data teams to streamline their data orchestration processes.</p>"},{"location":"data-orchestration/airflow/#history","title":"History","text":"<ul> <li>In 2014, Airflow was started at Airbnb as a solution to manage Airbnb\u2019s increasingly complex data workflows.</li> <li>In 2015, Airbnb open-sourced Airflow. Since then, it has gained significant popularity among data engineering teams looking for a reliable workflow management solution.</li> <li>In 2019, the Apache Software Foundation officially adopted Airflow as an incubation project and in 2020, it became a top-level project with a stable codebase and a strong community.</li> </ul>"},{"location":"data-orchestration/airflow/#architecture","title":"Architecture","text":"<p>At its core, Airflow creates workflows in the form of DAGs.</p> <p>Each DAG consists of individual units of work called Tasks, each representing a single unit of work that needs to be performed.</p> <p>The DAG also declares the dependencies between tasks to ensure they are executed in the correct order.</p> <p>In general, a DAG is a workflow or a process such as:</p> <ul> <li> <p>ETL pipeline: Perform the ETL process to extract data from different databases, transform it into a unified format, and load it into a data warehouse for further analysis.</p> </li> <li> <p>Machine learning pipeline: Perform the entire process of model training, evaluation, and deployment.</p> </li> </ul> <p>Here are two examples.</p> <p>The first DAG involves loading files from a remote SFTP server to a GCS bucket, followed by ingestion into BigQuery.</p> <p>The second DAG generates various types of reports depending on the date and sends them out.</p> <p></p>"},{"location":"data-orchestration/airflow/#infrastructure","title":"Infrastructure","text":"<p>When working with Airflow, it's important to understand the underlying infrastructure components and their functions.</p> <p>This knowledge can help us develop DAGs efficiently and troubleshoot issues.</p> <p>Airflow architecture consists of the following components:</p> <ul> <li> <p>Scheduler: The Airflow scheduler continuously scans a designated DAG folder to identify any new or modified DAG files and processes them.   Once parsed, the scheduler determines whether any active DAGs can be triggered at any moment.   The scheduler also keeps track of task status until the end of the life cycle and triggers a new task once all the dependencies have been met.</p> </li> <li> <p>Executor: Once a task is ready, the scheduler uses an executor to run the task.   Airflow provides two types of executors: local executors for single-machine installations and remote executors for a multi-machine installation.   CeleryExecutor is a popular remote executor that works as a distributed task queue, executing tasks in parallel across multiple worker nodes.</p> </li> <li> <p>Workers: A worker node is responsible for pulling the task from the queue and executing it.   The more worker nodes we have, the more tasks can be executed in parallel. The type of worker node depends on the executor.</p> </li> </ul> <p>Example</p> <p>the local executor executes tasks on the same node as the scheduler, while the Kubernetes executor executes tasks in containers.</p> <ul> <li> <p>Metadata database: The metadata database is a relational database that stores the state and metadata of DAGs and their tasks, including the start time, end time, and duration of each task to identify errors during execution.   It also stores Airflow's operational configuration, available connections, pools, variables, and other settings.</p> </li> <li> <p>Webserver: The web server is a web-based interface for users to interact with their workflows.</p> </li> </ul> <p></p>"},{"location":"data-orchestration/airflow/#task-level-operator","title":"Task level: Operator","text":"<p>In Airflow, there's a task which perform 1 unique job across the DAG, the task is usually calling Airflow Operator.</p> <ul> <li>An operator is essentially a Python class that defines what a task should do and how it should be executed within the context of the DAG.</li> <li>Operators can perform a wide range of tasks, such as running a Bash script or Python script, executing SQL, sending an email, or transferring files between systems.</li> <li>Operators provide a high-level abstraction of the tasks, letting us focus on the logic and flow of the workflow without getting bogged down in the implementation details.</li> </ul> <p>Tip</p> <p>While Airflow provides a wide variety of operators out of the box, we may still need to create custom operators to address our specific use cases.</p> <p>All operators are extended from <code>BaseOperator</code> and we need to override two methods: <code>__init__</code> and <code>execute</code>.</p> <p>The <code>execute</code> method is invoked when the runner calls the operator.</p> <p>The following example creates a custom operator <code>DataAnalysisOperator</code> that performs the requested type of analysis on an input file and saves the results to an output file.</p> <p>Warning</p> <p>It's advised not to put expensive operations in <code>__init__</code> because it will be instantiated once per scheduler cycle.</p> <pre><code>from airflow.models import BaseOperator\nfrom airflow.utils.decorators import apply_defaults\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass DataAnalysisOperator(BaseOperator):\n    @apply_defaults\n    def __init__(self, dataset_path, output_path, analysis_type, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.dataset_path = dataset_path\n        self.output_path = output_path\n        self.analysis_type = analysis_type\n\n    def execute(self, context):\n        # Load the input dataset into a pandas DataFrame.\n        data = pd.read_csv(self.dataset_path)\n\n        # Perform the requested analysis.\n        if self.analysis_type == 'mean':\n            result = np.mean(data)\n        elif self.analysis_type == 'std':\n            result = np.std(data)\n        else:\n            raise ValueError(f\"Invalid analysis type '{self.analysis_type}'\")\n\n        # Write the result to a file.\n        with open(self.output_path, 'w') as f:\n            f.write(str(result))\n</code></pre> <p>The extensibility of the operator is one of many reasons why Airflow is so powerful and popular.</p>"},{"location":"data-orchestration/airflow/#sensor-operator","title":"Sensor Operator","text":"<p>A special type of operator is called a sensor operator. It's designed to wait until something happens and then succeed so their downstream tasks can run. The DAG has a few sensors that are dependent on external files, time, etc.</p> <p>Common sensor types are:</p> <ul> <li> <p>TimeSensor: Wait for a certain amount of time to pass before executing a task.</p> </li> <li> <p>FileSensor: Wait for a file to be in a location before executing a task.</p> </li> <li> <p>HttpSensor: Wait for a web server to become available or return an expected result before executing a task.</p> </li> <li> <p>ExternalTaskSensor: Wait for an external task in another DAG to complete before executing a task.</p> </li> </ul> <p>We can create a custom sensor operator by extending the <code>BaseSensorOperator</code> class and overriding two methods: <code>__init__</code> and <code>poke</code>.</p> <p>The <code>poke</code> method performs the necessary checks to determine whether the condition has been satisfied. If so, return <code>True</code> to indicate that the sensor has succeeded. Otherwise, return <code>False</code> to continue checking in the next interval.</p> <p>Here is an example of a custom sensor operator that pulls an endpoint until the response matches with <code>expected_text</code>.</p> <pre><code>from airflow.sensors.base_sensor_operator import BaseSensorOperator\nfrom airflow.utils.decorators import apply_defaults\nimport requests\n\nclass EndpointSensorOperator(BaseSensorOperator):\n\n    @apply_defaults\n    def __init__(self, url, expected_text, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.url = url\n        self.expected_text = expected_text\n\n    def poke(self, context):\n        response = requests.get(self.url)\n        if response.status_code != 200:\n            return False\n        return self.expected_text in response.text\n</code></pre>"},{"location":"data-orchestration/airflow/#poke-mode-vs-reschedule-mode","title":"Poke mode vs. Reschedule mode","text":"<p>There are two types of modes in a sensor:</p> <ul> <li> <p>poke: the sensor repeatedly calls its <code>poke</code> method at the specified <code>poke_interval</code>, checks the condition, and reports it back to Airflow.   As a consequence, the sensor takes up the worker slot for the entire execution time.</p> </li> <li> <p>reschedule: Airflow is responsible for scheduling the sensor to run at the specified <code>poke_interval</code>.   But if the condition is not met yet, the sensor will release the worker slot to other tasks between two runs.</p> </li> </ul> <p>Tip</p> <p>In general, poke mode is more appropriate for sensors that require short run-time and <code>poke_interval</code> is less than five minutes.</p> <p>Reschedule mode is better for sensors that expect long run-time (e.g., waiting for data to be delivered by an external party) because it is less resource-intensive and frees up workers for other tasks.</p>"},{"location":"data-orchestration/airflow/#backfilling","title":"Backfilling","text":"<p>Backfilling is an important concept in data processing. It refers to the process of populating or updating historical data in the system to ensure that the data is complete and up-to-date.</p> <p>This is typically required in two use cases:</p> <ul> <li> <p>Implement a new data pipeline: If the pipeline uses an incremental load, backfilling is needed to populate historical data that falls outside the reloading window.</p> </li> <li> <p>Modify an existing data pipeline: When fixing a SQL bug or adding a new column, we also want to backfill the table to update the historical data.</p> </li> </ul> <p>Warning</p> <p>When backfilling the table, we must ensure that the new changes are compatible with the existing data; otherwise, the table needs to be recreated from scratch.</p> <p>Sometimes, the backfilling job can consume significant resources due to the high volume of historical data.</p> <p>It's also worth checking any possible downstream failure before executing the backfilling job.</p> <p>Airflow provides the backfilling process in its cli command.</p> <pre><code>airflow backfill [dag name] -s [start date] -e [end date]\n</code></pre> <p>For better understanding of Airflow, lets deep dive into Airflow DAG Design</p>"},{"location":"data-orchestration/airflow/airflow-dag-design/","title":"Airflow DAG Design","text":"<p>To create DAGs, we just need basic knowledge of Python. However, to create efficient and scalable DAGs, it's essential to master Airflow's specific features and nuances.</p>"},{"location":"data-orchestration/airflow/airflow-dag-design/#create-a-dag-object","title":"Create a DAG object","text":"<p>A DAG file starts with a <code>dag</code> object. We can create a <code>dag</code> object using a context manager or a decorator.</p> <pre><code>from airflow.decorators import dag\nfrom airflow import DAG\nimport pendulum\n\n# dag1 - using @dag decorator\n@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n)\ndef random_dag1():\n    pass\n\nrandom_dag1()\n\n# dag2 - using context manager\nwith DAG(\n    dag_id=\"random_dag2\",\n    schedule=\"@daily\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n) as dag2:\n    pass\n</code></pre> <p>Either way, we need to define a few parameters to control how a DAG is supposed to run.</p> <p>Some of the most-used parameters are:</p> <ul> <li> <p>start_date: If it's a future date, it's the timestamp when the scheduler starts to run. If it's a past date, it's the timestamp from which the scheduler will attempt to backfill.</p> </li> <li> <p>catch_up: Whether to perform scheduler catch-up. If set to true, the scheduler will backfill runs from the start date.</p> </li> <li> <p>schedule: Scheduling rules. Currently, it accepts a cron string, time delta object, timetable, or list of dataset objects.</p> </li> <li> <p>tags: List of tags helping us search DAGs in the UI.</p> </li> </ul>"},{"location":"data-orchestration/airflow/airflow-dag-design/#create-a-task-object","title":"Create a task object","text":"<p>A DAG object is composed of a series of dependent tasks. A task can be an operator, a sensor, or a custom Python function decorated with <code>@task</code>. Then, we will use <code>&gt;&gt;</code> or the opposite way, which denotes the dependencies between tasks.</p> <pre><code># dag3\n@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n)\ndef random_dag3():\n\n    s1 = TimeDeltaSensor(task_id=\"time_sensor\", delta=timedelta(seconds=2))\n    o1 = BashOperator(task_id=\"bash_operator\", bash_command=\"echo run a bash script\")\n    @task\n    def python_operator() -&gt; None:\n        logging.info(\"run a python function\")\n    o2 = python_operator()\n\n    s1 &gt;&gt; o1 &gt;&gt; o2\n\nrandom_dag3()\n</code></pre> <p>A task has a well-defined life cycle, including 14 states, as shown in the following graph:</p> <p></p>"},{"location":"data-orchestration/airflow/airflow-dag-design/#pass-data-between-tasks","title":"Pass data between tasks","text":"<p>One of the best design practices is to split a heavy task into smaller tasks for easy debugging and quick recovery.</p> <p>Example</p> <p>we first make an API request and use the response as the input for the second API request. To do so, we need to pass a small amount of data between tasks.</p> <p>XComs (cross-communications) is a method for passing data between Airflow tasks. Data is defined as a key-value pair, and the value must be serializable.</p> <ul> <li> <p>The <code>xcom_push()</code> method pushes data to the Airflow metadata database and is made available for other tasks.</p> </li> <li> <p>The <code>xcom_pull()</code> method retrieves data from the database using the key.</p> </li> </ul> <p></p> <p>Every time a task returns a value, the value is automatically pushed to XComs.</p> <p>We can find them in the Airflow UI <code>Admin</code> -&gt; <code>XComs</code>. If the task is created using <code>@task</code>, we can retrieve XComs by using the object created from the upstream task.</p> <p>In the following example, the operator <code>o2</code> uses the traditional syntax, and the operator <code>o3</code> uses the simple syntax:</p> <pre><code>@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n)\ndef random_dag4():\n\n    @task\n    def python_operator() -&gt; None:\n        logging.info(\"run a python function\")\n        return str(datetime.now()) # return value automatically stores in XCOMs\n    o1 = python_operator()\n    o2 = BashOperator(task_id=\"bash_operator1\", bash_command='echo \"{{ ti.xcom_pull(task_ids=\"python_operator\") }}\"') # traditional way to retrieve XCOM value\n    o3 = BashOperator(task_id=\"bash_operator2\", bash_command=f'echo {o1}') # make use of @task feature\n\n    o1 &gt;&gt; o2 &gt;&gt; o3\n\nrandom_dag4()\n</code></pre> <p>Warning</p> <p>Although nothing stops us from passing data between tasks, the general advice is to not pass heavy data objects, such as pandas DataFrame and SQL query results because doing so may impact task performance.</p>"},{"location":"data-orchestration/airflow/airflow-dag-design/#use-jinja-templates","title":"Use Jinja templates","text":"<p>Info</p> <p>Jinja is a templating language used by many Python libraries, such as Flask and Airflow, to generate dynamic content.</p> <p>It allows us to embed variables within the text and then have those variables replaced with actual values during runtime.</p> <p>Airflow's Jinja templating engine provides built-in functions that we can use between double curly braces, and the expression will be evaluated at runtime.</p> <p>Users can also create their own macros using user_defined_macros and the macro can be a variable as well as a function.</p> <pre><code>def days_to_now(starting_date):\n    return (datetime.now() - starting_date).days\n\n@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"],\n    user_defined_macros={\n        \"starting_date\": datetime(2015, 5, 1),\n        \"days_to_now\": days_to_now,\n    })\ndef random_dag5():\n\n    o1 = BashOperator(task_id=\"bash_operator1\", bash_command=\"echo Today is {{ execution_date.format('dddd') }}\")\n    o2 = BashOperator(task_id=\"bash_operator2\", bash_command=\"echo Days since {{ starting_date }} is {{ days_to_now(starting_date) }}\")\n\n    o1 &gt;&gt; o2\n\nrandom_dag5()\n</code></pre> <p>Note</p> <p>The full list of built-in variables, macros and filters in Airflow can be found in the Airflow Documentation</p> <p>Another feature around templating is the <code>template_searchpath</code> parameter in the DAG definition.</p> <p>It's a list of folders where Jinja will look for templates.</p> <p>Example</p> <p>A common use case is invoking an SQL file in a database operator such as BigQueryInsertJobOperator. Instead of hardcoding the SQL query, we can refer to the SQL file, and the content will be automatically rendered during runtime.</p> <pre><code>@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"],\n    template_searchpath=[\"/usercode/dags/sql\"])\ndef random_dag6():\n\n    BigQueryInsertJobOperator(\n        task_id=\"insert_query_job\",\n        configuration={\n            \"query\": {\n                \"query\": \"{% include 'sample.sql' %}\",\n                \"useLegacySql\": False,\n            }\n        }\n    )\n\nrandom_dag6()\n</code></pre>"},{"location":"data-orchestration/airflow/airflow-dag-design/#manage-cross-dag-dependencies","title":"Manage cross-DAG dependencies","text":"<p>In principle, every DAG is an independent workflow. However, sometimes, it's necessary to create dependencies between DAGs.</p> <p>Example</p> <p>a DAG performs an ETL job that produces a table sales. The sales table is the source of two downstream DAGs, where one generates revenue reports, and the other one uses it to train a machine learning model.</p> <p>There are several ways to implement cross-DAG dependencies in Airflow.</p> <ul> <li><code>TriggerDagOperator</code> is an operator that triggers a downstream DAG from any point in the DAG. It's similar to a push mechanism where the producer decides when to notify the consumers.</li> </ul> <pre><code>from airflow.operators.trigger_dagrun import TriggerDagRunOperator\n\n@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n)\ndef random_dag7():\n\n    TriggerDagRunOperator(\n        task_id=\"trigger_dagrun\",\n        trigger_dag_id=\"random_dag1\",\n        conf={},\n    )\n\nrandom_dag7()\n</code></pre> <ul> <li><code>ExternalTaskSensor</code> is a sensor operator for downstream DAGs to pull states of the upstream DAG, similar to a pull mechanism. The downstream DAG will wait until the task is completed in the upstream DAG.</li> </ul> <pre><code>from airflow.sensors.external_task import ExternalTaskSensor\n\n@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n)\ndef random_dag8():\n\n    ExternalTaskSensor(\n        task_id=\"external_sensor\",\n        external_dag_id=\"random_dag3\",\n        external_task_id=\"python_operator\",\n        allowed_states=[\"success\"],\n        failed_states=[\"failed\", \"skipped\"],\n    )\n\nrandom_dag8()\n</code></pre> <p>Another method introduced in <code>version 2.4</code> uses datasets to create data-driven dependencies between DAGs.</p> <p>An Airflow dataset is a logical grouping of data updated by upstream tasks. The upstream task defines the output dataset via <code>outlets</code> parameter. The completion of the task means the successful update of the dataset.</p> <p>In downstream DAGs, instead of using a time-based schedule, the DAG refers to the corresponding dataset produced by the upstreams. Therefore, the downstream DAG will be triggered in a data-driven manner rather than a scheduled-based manner.</p> <pre><code>dag1_dataset = Dataset(\"s3://dag1/output_1.txt\", extra={\"hi\": \"bye\"})\n\n@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n)\ndef random_dag9_producer():\n    BashOperator(outlets=[dag1_dataset], task_id=\"producer\", bash_command=\"sleep 5\")\n\nrandom_dag9_producer()\n\n@dag(\n    schedule=[dag1_dataset],\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n)\ndef random_dag9_consumer():\n    BashOperator(task_id=\"consumer\", bash_command=\"sleep 5\")\n\nrandom_dag9_consumer()\n</code></pre>"},{"location":"data-orchestration/airflow/airflow-dag-design/#best-practices","title":"Best practices","text":"<p>When working with Airflow, there are several best practices to keep in mind that help ensure our pipelines run smoothly and efficiently.</p>"},{"location":"data-orchestration/airflow/airflow-dag-design/#idempotency","title":"Idempotency","text":"<p>Idempotency is a fundamental concept for data pipelines. In the context of Airflow, idempotency means running the same DAG Run multiple times has the same effect as running it only once. When a DAG is designed to be idempotent, it can be executed repeatedly without causing unexpected changes to the pipeline's output.</p> <p>This is especially necessary when a DAG Run might be rerun due to failures or errors in the processing.</p> <p>Example</p> <p>An example to make DAG idempotent is to use templates such as variable <code>{{ execution_date }}</code>.</p> <p>It's associated with the expected scheduled time of each run, and the date won't be changed even if we rerun the DAG Run a few hours later.</p>"},{"location":"data-orchestration/airflow/airflow-dag-design/#avoid-top-level-code-in-the-dag-file","title":"Avoid top-level code in the DAG file","text":"<p>By default, Airflow reads the dag folder every 30 seconds, including the top-level code that is outside of DAG context.</p> <p>Because of this, having expensive top-level code, such as making requests to external APIs, can cause performance issues because they are called every 30 seconds rather than only when DAG is scheduled.</p> <p>The general advice is to limit the amount of top-level code in the DAG file and move it within the DAG context or operators.</p> <p>This can help reduce unnecessary overheads and allow Airflow to focus on executing the right things.</p> <p>The following example shows both good and bad ways of making an API request:</p> <pre><code># Bad example - requests will be made every 30 seconds instead of everyday at 4:30am\nres = requests.get(\"https://api.sampleapis.com/coffee/hot\")\n\n@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n)\ndef random_dag7():\n\n    @task\n    def python_operator() -&gt; None:\n        logging.info(f\"API result {res}\")\n    python_operator()\n\nrandom_dag7()\n\n# Good example\n\n@dag(\n    schedule=\"30 4 * * *\",\n    start_date=pendulum.datetime(2023, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=[\"random\"]\n)\ndef random_dag7():\n\n    @task\n    def python_operator() -&gt; None:\n        res = requests.get(\"https://api.sampleapis.com/coffee/hot\") # move API request within DAG context\n        logging.info(f\"API result {res}\")\n    python_operator()\n\nrandom_dag7()\n</code></pre> <p>Reference: Airflow Documentation</p>"},{"location":"data-orchestration/dbt/","title":"Data Build Tool (dbt)","text":"<p>Airflow is great data orchestration tools, but it requires significant engineering efforts. It can become a bottleneck for data analysts and analytics engineers to build data transformation pipelines using SQL as the primary language.</p> <p>Info</p> <p>To some extent, <code>dbt</code> is a transformation workflow, rather than a pure orchestration tool.</p>"},{"location":"data-orchestration/dbt/#history","title":"History","text":"<ul> <li>In 2016, Dbt was created by Tristan Handy while he was working as a data analyst at RJMetric, a SaaS-based data analytics company.   He built dbt, which was an internal tool back then to address complex data transformation pipelines.</li> <li>In 2018, Handy founded Fishtown Analytics, a data engineering and analytics consulting company, with the goal of helping other organizations improve data analytics workflows.</li> <li>Since then, dbt has become a widely adopted open-source tool, with a growing community of users and contributors.   It has been used by companies of all sizes to manage their data transformation pipelines and improve their data analytics workflows.</li> </ul>"},{"location":"data-orchestration/dbt/#dbt-products","title":"Dbt products","text":"<p>At the time of writing this, dbt offers two types of products: dbt Core and dbt Cloud.</p> <ul> <li>The first product, dbt Core, is an open-source tool enabling data teams to transform data using best engineering practices.   We can install dbt Core with its database adapter using command <code>pip install dbt-&lt;adapter&gt;</code>.</li> </ul> <p>Example</p> <p>we can use the <code>dbt-bigquery</code> adapter.</p> <p></p> <ul> <li>On the other hand, dbt cloud is a cloud environment that deploys dbt in a reliable and low-maintenance way.   We can develop, test, and schedule data models all in one web-based UI.   While dbt Core is a free and open-source tool, dbt Cloud is a paid service that offers additional features and support for enterprise-level data teams.</li> </ul> <p></p>"},{"location":"data-orchestration/dbt/#features","title":"Features","text":"<p>Unlike Airflow, whose main programming language is Python, dbt is a tool that is centered around SQL. It opens the door to many data analysts who have been working with SQL for decades.</p> <p>As a dbt user, the primary focus will be on writing models in SQL using <code>SELECT</code> statements that reflect business logic.</p> <p>There is no need to write boilerplate code to create tables, update tables, which is normally the trickiest part, or define the order of model execution.</p>"},{"location":"data-orchestration/dbt/#avoid-writing-boilerplate-sql-using-materialization","title":"Avoid writing boilerplate SQL using materialization","text":"<p>Materialization is a key concept in dbt that refers to the process of persisting data transformations in a database.</p> <p>In other words, materialization determines how dbt converts a <code>SELECT</code> statement into a physical table or view. It avoids boilerplate SQL around table creation and update.</p> <p>Info</p> <p>In contrast, to build a table in Airflow, we need to manage a few process-related SQL.</p> <p>This includes writing SQL statements such as <code>CREATE OR REPLACE</code> and <code>MERGE</code> (a complex one), as well as maintaining table schema and incremental logic.</p> <p>This can create maintenance overhead for nontechnical users who may not have a deep understanding of SQL and data engineering, making it more difficult and time-consuming to manage data transformations.</p> <p>Dbt streamlines this part by providing materialization types through a config section in each data model.</p> <pre><code>{{ config(materialized='table') }}\n\nselect *\nfrom ...\n</code></pre> <ul> <li> <p>Table: The model is rebuilt as a table each time. Under the hood, dbt runs the <code>CREATE TABLE AS</code> statement.</p> </li> <li> <p>View: The model is rebuilt as a view each time. Under the hood, dbt runs the <code>CREATE VIEW AS</code> statement.</p> </li> <li> <p>Incremental: Dbt inserts or updates records into a table since the last time.   It's useful for big tables where recreating the table each time is time-consuming and expensive. For BigQuery, under the hood, dbt runs the <code>MERGE</code> statement.</p> </li> <li> <p>Ephemeral: The model is not built into the database.   Instead, dbt interpolates the code into the dependent model as a CTE. The ephemeral model is a powerful type for resuing CTEs across multiple data models.</p> </li> </ul>"},{"location":"data-orchestration/dbt/#determine-the-order-of-model-execution","title":"Determine the order of model execution","text":"<p>When referring to another model within dbt, dbt uses <code>ref</code> keyword instead of referencing the physical table name in the data warehouse.</p> <p>Example</p> <p>instead of referencing a physical table like <code>my_database.my_schema.my_table</code>, it's better to use <code>ref('my_table')</code> and <code>my_table</code> as the model name.</p> <p>This is because using <code>ref</code> allows dbt to automatically manage the dependencies between models and ensures that they are executed in the correct order.</p> <p>Another advantage is that using <code>ref</code> makes the dbt project more modular and easier to maintain because we can update the underlying physical table names without having to update all the downstream models.</p> <p>Dbt Cloud offers the dependency graph, which is useful for understanding how our models are related to each other and for easy debugging. It also helps us to ensure that our models are executed in the correct order.</p> <p></p>"},{"location":"data-orchestration/dbt/#use-jinja-to-create-reusable-sql","title":"Use Jinja to create reusable SQL","text":"<p>Another great feature that makes dbt a productive tool is its use of Jinja templates. Similar to Airflow, dbt utilizes Jinja to enable dynamic SQL generation and create reusable code snippets in macros.</p> <p>Here is an example of a dbt model that uses Jinja with the <code>if</code> and <code>for</code> loops:</p> <pre><code>{% set products = ['apple', 'pear', 'banana'] %}\n\nselect\n    date,\n    {% for product in products %}\n        SUM(case when product = '{{ product }}' then quantity * price else 0 end) as {{ product }}_revenue,\n    {% endfor %}\n    SUM(quantity * price) AS total_revenue\n    from {{ ref('table_sales') }}\ngroup by 1\n</code></pre> <p>That model will be compiled into an executable query:</p> <pre><code>select\n  date,\n  SUM(case when product = 'apple' then quantity * price else 0 end) as apple_revenue,\n  SUM(case when product = 'pear' then quantity * price else 0 end) as pear_revenue,\n  SUM(case when product = 'banana' then quantity * price else 0 end) as banana_revenue,\n  SUM(quantity * price) AS total_revenue\n  from `your_project_id`.`your_dataset_id`.`table_sales`\ngroup by 1\n</code></pre> <p>Apart from built-in Jinja templates, we can create macros to generate customized SQL code that can be shared across multiple data models.</p> <p>This approach helps standardize best practices within our data team, much like creating utility functions in Python.</p> <pre><code>{% macro convert_date(column_name) %}\n  CONCAT(EXTRACT(YEAR from {{ column_name }} at time zone \"UTC\"), '-', EXTRACT(MONTH from {{ column_name }} at time zone \"UTC\"))\n{% endmacro %}\n</code></pre> <p>Then, the macro is used in the data model like this:</p> <pre><code>{{ config(materialized='table') }}\n\nSELECT\n  {{ convert_date('date') }} as date\nFROM my_table\n</code></pre>"},{"location":"data-orchestration/dbt/#other-features","title":"Other features","text":"<ul> <li>In addition to the above, even seemingly minor features in dbt can have a big impact on the data team's workflow.</li> </ul> <p>Example</p> <p>package management in dbt allows us to install packages like dbt_utils from the community as well as create and distribute packages to other teams.</p> <ul> <li>Working with external data in CSV or JSON in a common scenario.   With dbt's seed functionality, we can quickly and easily load small CSV or JSON files into the data warehouse using the <code>dbt seed</code> command.   This can be particularly useful for populating small tables with test data, for quickly prototyping a new data model, or storing a config file coming from an external system.</li> </ul> <p>Tip</p> <p>You can find project example of running dbt: Running dbt and Google BigQuery using Docker</p> <p>Reference: dbt Documentation</p>"},{"location":"data-processing/","title":"Data Processing","text":"<p>Over past few decades, numerous programming models have emerged to address the challenge of processing big data at scale. Undoubtedly, MapReduce stands out as one of the most popular and effective approaches.</p>"},{"location":"data-processing/#what-is-mapreduce","title":"What is MapReduce","text":"<p>MapReduce is a distributed programming framework originally developed at Google by Jeffrey Dean and Sanjay Ghemawat, back in 2004 and was inspired by fundamental concepts of functional programming.</p> <p>Their proposal invloved a parallel data processing model consisting of two steps; map and reduce.</p> <p>In simple terms,</p> <ul> <li>map involves the division of the original data into small chunks such that transformation logic can be applied to individual data blocks.</li> <li>Data processing can therefore be applied in parallel across the created chunks.</li> <li>Finally, reduce will then aggregate/consolidate the processed blocks and return the end result back to the caller.</li> </ul>"},{"location":"data-processing/#how-does-mapreduce-work","title":"How does MapReduce work?","text":"<p>A MapReduce system is usually composed of 3 steps (even though it's generalized as the combination of Map and Reduce operations/functions).</p> <p>The MapReduce operations are:</p>"},{"location":"data-processing/#map","title":"Map","text":"<p>The input data is first split into smaller blocks. The Hadoop framework then decides how many mappers to use, based on the size of the data to be processed and the memory block available on each mapper server.</p> <p>Each block is then assigned to a mapper for processing. Each <code>worker</code> node applies the map function to the local data, and writes the output to temporary storage.</p> <p>The primary (master) node ensures that only a single copy of the redundant input data is processed.</p>"},{"location":"data-processing/#shuffle-combine-and-partition","title":"Shuffle, combine and partition","text":"<p> worker nodes redistribute data based on the output keys (produced by the map function), such that all data belonging to one key is located on the same worker node.</p> <p>As an optional process the combiner (a reducer) can run individually on each mapper server to reduce the data on each mapper even further making reducing the data footprint and shuffling and sorting easier.</p> <p>Partition (not optional) is the process that decides how the data has to be presented to the reducer and also assigns it to a particular reducer.</p>"},{"location":"data-processing/#reduce","title":"Reduce","text":"<p>A reducer cannot start while a mapper is still in progress. Worker nodes process each group of  pairs output data, in parallel to produce  pairs as output. <p>All the map output values that have the same key are assigned to a single reducer, which then aggregates the values for that key.</p> <p>Unlike the map function which is mandatory to filter and sort the initial data, the reduce function is optional.</p>"},{"location":"data-processing/#some-considerations-with-mapreduce","title":"Some considerations with MapReduce","text":"<ul> <li>Rigid Map Reduce programming paradigm</li> </ul> <p>While exposing Map and Reduce interfaces to programmers has simplified the creation of distributed applications in Hadoop,  it is difficult to express a broad range of logic in a Map Reduce programming paradigm.</p> <p>Iterative process is an example of logic that does not work well in Map Reduce.</p> <ul> <li>Read/Write intensive</li> </ul> <p>MapReduce jobs store little data in memory as it has no concept of a distributed memory structure for user data. Data must be read and written to HDFS.</p> <p>More complex MapReduce applications involve chaining smaller MapReduce jobs together.</p> <p>Since data cannot be passed between these jobs, it will require data sharing via HDFS. This introduces a processing bottleneck.</p> <ul> <li>Java focused</li> </ul> <p>MapReduce is Java-based, and hence the most efficient way to write applications for it will be using java.</p> <p>Its code must be compiled in a separate development environment, then deployed into the Hadoop cluster.</p> <p>This style of development is not widely adopted by Data Analysts nor Data Scientists who are used to other technologies like SQL or interpreted languages like Python.</p> <p>MapReduce does have the capability to invoke Map/Reduce logic written in other languages like C, Python, or Shell Scripting.</p> <ul> <li>Phased out from big data offerings</li> </ul> <p>MapReduce is slowly being phased out of Big Data offerings. While some vendors still include it in their Hadoop distribution, it is done so to support legacy applications.</p> <p>Customers have moved away from creating MapReduce applications, instead adopting simpler and faster frameworks like Apache Spark.</p>"},{"location":"data-processing/#mapreduce-and-hadoop","title":"MapReduce and Hadoop","text":"<p>MapReduce is part of the Apache Hadoop framework that is used to access data stored in Hadoop Distributed File System (HDFS).</p> <p>Hadoop consists of four basic modules:</p> <ul> <li> <p>Hadoop Distributed File System (HDFS): This ia a distributed file system that can store large datasets in a fault-tolerant fashion</p> </li> <li> <p>Yet Another Resource Negotiation (YARN): This is the node manager that monitors cluster and resources. It also acts as the scheduler of jobs</p> </li> <li> <p>MapReduce</p> </li> <li> <p>Hadoop Common: This is a module that provides commonly used Java libraries</p> </li> </ul>"},{"location":"data-processing/#what-is-mapreduce-used-for","title":"What is MapReduce used for?","text":"<p>Legacy applications and Hadoop native tools like Sqoop and Pig leverage MapReduce today.</p> <p>There is very limited MapReduce application development nor any significant contributions being made to it as an open source technology.</p> <p>But, in nowadays, MapReduce is becoming a great breakthrough to the updated data processing framework, such as Apache Spark, Apache Storm, Google BigQuery, etc.</p> <p>We will deep dive into some updated framework, such as:</p> <ul> <li>Apache Spark</li> <li>Google BigQuery</li> </ul>"},{"location":"data-processing/#references","title":"References","text":"<ul> <li>MapReduce</li> <li>Processing Data at Scale with MapReduce</li> <li>Apache Hadoop</li> </ul>"},{"location":"data-processing/apache-spark/","title":"Apache Spark","text":"<p>Apache Spark is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.</p> <p>Some of the key features:</p> <ul> <li>Batch/streaming data</li> </ul> <p>Unify the processing of data in batches and real-time streaming, using preferred languages: Python, SQL, Scala, Java or R.</p> <ul> <li>SQL analytics</li> </ul> <p>Execute fast, distributed ANSI SQL queries for dashboarding and ad-hoc reporting.</p> <ul> <li>Data science at scale</li> </ul> <p>Perform Exploratory Data Analysis (EDA) on petabyte-scale data without having to resort to downsampling.</p> <ul> <li>Machine learning</li> </ul> <p>Train machine learning algorithms on a laptop and use the same code to scale to fault-tolerant clusters of thousands of machines.</p> <p>Example in pyspark:</p> <pre><code>docker run -it --rm spark:python3 /opt/spark/bin/pyspark\n</code></pre> <pre><code>df = spark.createDataFrame(\n    [(\"person_1\", \"20\"), (\"person_2\", \"56\"), (\"person_3\", \"89\"), (\"person_4\", \"20\")],\n    [\"name\",\"age\"]\n)\ndf.where(\"age &gt; 21\").select(\"name\").show()\n</code></pre>"},{"location":"data-processing/apache-spark/#how-does-apache-spark-work","title":"How does Apache Spark work?","text":"<p>Hadoop MapReduce is a programming model for processing big data sets with a parallel, distributed algorithm. Developers can write massively parallelized operators, without having to worry about work distribution, and fault tolerance.</p> <p>However, a challenge to MapReduce is the sequential multi-step process it takes to run a job.</p> <p>With each step, MapReduce reads data from the cluster, performs operations, and writes the results back to HDFS.</p> <p>Because each step requires a disk read, and write, MapReduce jobs are slower due to the latency of disk I/O.</p> <p>Spark was created to address the limitations to MapReduce, by</p> <ul> <li>doing processing in-memory</li> <li>reducing the number of steps in a job</li> <li>by reusing data across multiple parallel operations</li> </ul> <p>Info</p> <p>With Spark, only one-step is needed where data is read into memory, operations performed, and the results written back\u2014resulting in a much faster execution.</p> <p>Spark also reuses data by using an in-memory cache to greatly speed up machine learning algorithms that repeatedly call a function on the same dataset.</p> <p>Data re-use is accomplished through the creation of DataFrames, an abstraction over Resilient Distributed Dataset (RDD), which is a collection of objects that is cached in memory, and reused in multiple Spark operations.</p> <p>This dramatically lowers the latency making Spark multiple times faster than MapReduce, especially when doing machine learning, and interactive analytics.</p>"},{"location":"data-processing/apache-spark/#what-are-apache-spark-workloads","title":"What are Apache Spark workloads?","text":"<p>The Spark framework includes:</p> <ul> <li>Spark Core as the foundation for the platform</li> <li>Spark SQL for interactive queries</li> <li>Spark Streaming for real-time analytics</li> <li>Spark MLlib for machine learning</li> <li>Spark GraphX for graph processing</li> </ul> <p></p>"},{"location":"data-processing/apache-spark/#spark-architecture","title":"Spark Architecture","text":"<p>Applications of Spark architecture which exists in above diagram:</p>"},{"location":"data-processing/apache-spark/#spark-driver","title":"Spark driver","text":"<p>The driver is the program or process responsible for coordinating the execution of the Spark application. It runs the main function and creates the SparkContext, which connects to the cluster manager.</p>"},{"location":"data-processing/apache-spark/#spark-executors","title":"Spark executors","text":"<p>Executors are worker processes responsible for executing tasks in Spark applications. They are launched on worker nodes and communicate with the driver program and cluster manager. Executors run tasks concurrently and store data in memory or disk for caching and intermediate storage.</p>"},{"location":"data-processing/apache-spark/#cluster-manager","title":"Cluster manager","text":"<p>The cluster manager is responsible for allocating resources and managing the cluster on which the Spark application runs. Spark supports various cluster managers like Apache Mesos, Hadoop YARN, and standalone cluster manager.</p>"},{"location":"data-processing/apache-spark/#sparkcontext","title":"sparkContext","text":"<p>SparkContext is the entry point for any Spark functionality. It represents the connection to a Spark cluster and can be used to create RDDs (Resilient Distributed Datasets), accumulators, and broadcast variables. SparkContext also coordinates the execution of tasks.</p>"},{"location":"data-processing/apache-spark/#task","title":"Task","text":"<p>A task is the smallest unit of work in Spark, representing a unit of computation that can be performed on a single partition of data. The driver program divides the Spark job into tasks and assigns them to the executor nodes for execution.</p>"},{"location":"data-processing/apache-spark/#useful-things-to-note-about-this-architecture","title":"Useful things to note about this architecture","text":"<ul> <li>Each application gets its own executor processes, which stay up for the duration of the whole application and run tasks in multiple threads</li> </ul> <p>This has the benefit of isolating applications from each other, on both the scheduling side (each driver schedules its own tasks) and executor side (tasks from different applications run in different JVMs).</p> <ul> <li>Spark is agnostic to the underlying cluster manager</li> </ul> <p>As long as it can acquire executor processes, and these communicate with each other, it is relatively easy to run it even on a cluster manager that also supports other applications (e.g. Mesos/YARN/Kubernetes).</p> <ul> <li>The driver program must listen for and accept incoming connections from its executors throughout its lifetime</li> </ul> <p>As such, the driver program must be network addressable from the worker nodes.</p> <ul> <li>Because the driver schedules tasks on the cluster, it should be run close to the worker nodes, preferably on the same local area network</li> </ul> <p>If you\u2019d like to send requests to the cluster remotely, it\u2019s better to open an RPC to the driver and have it submit operations from nearby than to run a driver far away from the worker nodes.</p>"},{"location":"data-processing/apache-spark/#cluster-manager-types","title":"Cluster Manager Types","text":"<p>The system currently supports several cluster managers:</p> <ul> <li>Standalone \u2013 a simple cluster manager included with Spark that makes it easy to set up a cluster.</li> <li>Apache Mesos (Deprecated) \u2013 a general cluster manager that can also run Hadoop MapReduce and service applications.</li> <li>Hadoop YARN \u2013 the resource manager in Hadoop 3.</li> <li>Kubernetes \u2013 an open-source system for automating deployment, scaling, and management of containerized applications.</li> </ul>"},{"location":"data-processing/apache-spark/#monitoring","title":"Monitoring","text":"<p>Each driver program has a web UI, typically on port 4040, that displays information about running tasks, executors, and storage usage. Simply go to <code>http://&lt;driver-node&gt;:4040</code> in a web browser to access this UI.</p>"},{"location":"data-processing/apache-spark/#job-scheduling","title":"Job Scheduling","text":"<p>Spark gives control over resource allocation both across applications (at the level of the cluster manager) and within applications (if multiple computations are happening on the same SparkContext).</p>"},{"location":"data-processing/apache-spark/#spark-optimization","title":"Spark Optimization","text":"<p>There are several techniques to more optimize spark jobs, we can look into Spark Optimization</p>"},{"location":"data-processing/apache-spark/#references","title":"References","text":"<ul> <li>Apache Spark</li> <li>AWS - What is Apache Spark?</li> <li>Apache Spark - Cluster Overview</li> <li>Spark Architecture: A Deep Dive</li> </ul>"},{"location":"data-processing/apache-spark/spark-optimization/","title":"Spark Optimizations","text":"<p>Spark is commonly used to apply transformations on data, structured in most cases. There are 2 scenarios in which it is particularly useful.</p> <p>Tip</p> <p>When the data to be processed is too large for the available computing and memory resources.</p> <p>Finally, it is also an alternative when one wants to accelerate calculation by using several machines within the same network. In both cases, a major concern is to optimize the calculation time of a Spark job.</p> <p>In response to this problem, we often increase the resources allocated to a computing cluster.</p> <p>But, in order to avoid an exhaustive search for the best configuration settings, which is naturally very costly, we will exhibit actionable solutions to maximise our chances of reducing computation time.</p> <p>Each step will be materialized by a recommandation, as justified as possible.</p> <p>More other table join strategies are available in Spark Databricks, Optimizing Apache Spark SQL Joins.</p>"},{"location":"data-processing/apache-spark/spark-optimization/#use-the-apache-parquet-file-format","title":"Use the Apache Parquet file format","text":"<p>The Apache Parquet format is officially a column-oriented storage. Actually, it's more of a hybrid format between row and column storage.</p> <p>It's used for tabular data. Data in the same column are stored contiguously.</p> <p></p> <p>This format is particularly suitable when performing queries (transformations) on a subset of columns and on a large dataframe. This is because it loads only the data associated with the required columns into memory.</p> <p>Moreover, as the compression scheme and the encoding are specific to each column according to the typing, it improves the reading / writing of these binary files and their size on disk.</p> <p>These advantages make it a very interesting alternative to the CSV format. This is the format recommended by Spark and the default format for writing.</p>"},{"location":"data-processing/apache-spark/spark-optimization/#maximise-parallelism-in-spark","title":"Maximise parallelism in Spark","text":"<p>Spark\u2019s efficiency is based on its ability to process several tasks in parallel at scale. Therefore, the more we facilitate the division into tasks, the faster they will be completed. This is why optimizing a Spark job often means reading and processing as much data as possible in parallel.</p> <p>And to achieve this goal, it is necessary to split a dataset into several partitions.</p> <p>Partitioning a dataset is a way of arranging the data into configurable, readable subsets of contiguous data blocks on disk. These partitions can then be read and processed independently and in parallel. It is this independence that enables massive data processing.</p> <p>Ideally, Spark organises one thread per task and per CPU core. Each task is related to a single partition. Thus, a first intuition is to configure a number of partitions at least as large as the number of available CPU cores.</p> <p>All cores should be occupied most of the time during the execution of the Spark job. If one of them is available at any time, it should be able to process a job associated with a remaining partition.</p> <p>The goal is to avoid bottlenecks by splitting the Spark job stages into a large number of tasks. This fluidity is crucial in a distributed computing cluster.</p> <p>The following diagram illustrates this division between the machines in the network.</p> <p></p> <p>Partitions can be created:</p> <ul> <li>When reading the data by setting the <code>spark.sql.files.maxPartitionBytes</code> parameter (default is 128 MB).</li> </ul> <p>A good situation is when the data is already stored in several partitions on disk. For example, a dataset in parquet format with a folder containing data partition files between 100 and 150 MB in size.</p> <ul> <li>Directly in the Spark application code using the Dataframe API.</li> </ul> <pre><code>dataframe.repartition(100)\ndataframe.coalesce(100)\n</code></pre> <p>This last method <code>coalesce</code> decreases the number of partitions while avoiding a <code>shuffle</code> in the network.</p> <p>Warning</p> <p>One might be tempted to increase the number of partitions by lowering the value of the <code>spark.sql.files.maxPartitionBytes</code> parameter.</p> <p>However, this choice can lead to the small file problem.</p> <p>There is a deterioration of I/O performance due to the operations performed by the file system (e.g. opening, closing, listing files), which is often amplified with a distributed file system like HDFS.</p> <p>Scheduling problems can also be observed if the number of partitions is too large.</p>"},{"location":"data-processing/apache-spark/spark-optimization/#beware-of-shuffle-operations","title":"Beware of Shuffle Operations","text":"<p>There is a specific type of partition in Spark called a shuffle partition.  These partitions are created during the stages of a job involving a shuffle, i.e. when a wide transformation (e.g. <code>groupBy()</code>, <code>join()</code>) is performed. The setting of these partitions impacts both the network and the read/write disk resources.</p> <p>The value of <code>spark.sql.shuffle.partitions</code> can be modified to control the number of partitions.</p> <p>Info</p> <p>By default, this is set to 200, which may be too high for some processing, and results in too many partitions being exchanged in the network between the executing nodes.</p> <p>This parameter should be adjusted according to the size of the data.</p> <p>Tip</p> <p>An intuition might be to start with a value at least equal to the number of CPU cores in the cluster.</p> <p>Spark stores the intermediate results of a shuffle operation on the local disks of the executor machines, so the quality of the disks, especially the I/O quality, is really important.</p> <p>Example</p> <p>the use of SSD disks will significantly improve performance for this type of transformation.</p> <p>The table below describes the main parameters that we can also influence.</p> <p></p>"},{"location":"data-processing/apache-spark/spark-optimization/#use-broadcast-hash-join","title":"Use Broadcast Hash Join","text":"<p>A join between several dataframe is a common operation.</p> <p>In a distributed context, a large amount of data is exchanged in the network between the executing nodes to perform the join.</p> <p>Depending on the size of the tables, this exchange causes network latency, which slows down processing.</p> <p>Spark offers several join strategies to optimise this operation. One of them is particularly interesting if it can be chosen: <code>Broadcast Hash Join (BHJ)</code>.</p> <p>Tip</p> <p>This technique is suitable when one of the merged dataframe is \u201csufficiently\u201d small to be duplicated in memory on all the executing nodes (broadcast operation). The diagram below illustrates how this strategy works.</p> <p></p> <p>By duplicating the smallest table, the join no longer requires any significant data exchange in the cluster apart from the broadcast of this table beforehand. </p> <p>This strategy greatly improves the speed of the join. The Spark configuration parameter to modify is <code>spark.sql.autoBroadcastHashJoin</code>.</p> <p>Tip</p> <p>The default value is 10 MB, i.e. this method is chosen if one of the two tables is smaller than this size.</p> <p>If sufficient memory is available, it may be very useful to increase this value or set it to -1 to force Spark to use it.</p>"},{"location":"data-processing/apache-spark/spark-optimization/#cache-intermediate-results","title":"Cache intermediate results","text":"<p>To optimise its computations and manage memory resources, Spark uses lazy evaluation and a DAG to describe a job.</p> <p>This offers the possibility of quickly recalculating the steps before an action if necessary, and thus executing only part of the DAG.</p> <p>To take full advantage of this functionality, it is very wise to store expensive intermediate results if several operations use them downstream of the DAG.</p> <p>Indeed, if an action is run, its computation can be based on these intermediate results and thus only replay a sub-part of the DAG before this action.</p> <p>following DAG as example</p> <p></p> <p>To obtain the results of the 2 actions, the treatments are described in the 2 DAGs below.</p> <p></p> <p>In order to speed up the execution, one can decide to cache intermediate results (e.g. the result of a join)</p> <p></p> <p>The processing of the second action is now simplified.</p> <p>Note</p> <p>During the first action, the results have not yet been stored in memory.</p> <p></p> <p>Tip</p> <p>If this caching can speed up execution of a job, we pay a cost when these results are written to memory and/or disk.</p> <p>It should be tested at different locations in the processing pipeline whether the total time saving outweighs the cost.</p> <p>This is especially relevant when there are several paths on the DAG.</p> <p>Note</p> <p>Caching, like any Spark transformation, is performed when an action is run. </p> <p>If the computation of this action involves only a sub-part of the data, then only the intermediate results for that sub-part will be stored.</p> <p>A table can be cached using the following command</p> <pre><code>dataframe.persist(storageLevel=\"MemoryOnly\")\n</code></pre> <p>The different caching options are described in the table below</p> <p></p>"},{"location":"data-processing/apache-spark/spark-optimization/#manage-the-memory-of-the-executor-nodes","title":"Manage the memory of the executor nodes","text":"<p>The memory of a Spark executor is broken down as follows</p> <p></p> <p>By default, the <code>spark.memory.fraction</code> parameter is set to 0.6.</p> <p>This means that 60% of the memory is allocated for execution and 40% for storage, once the reserved memory is removed.</p> <p>This is 300 MB by default and is used to prevent out of memory (OOM) errors.</p> <p>We can modify the following 2 parameters:</p> <ul> <li><code>spark.executor.memory</code></li> <li><code>spark.memory.fraction</code></li> </ul>"},{"location":"data-processing/apache-spark/spark-optimization/#partitioning","title":"Partitioning","text":"<p>Partitioning in Spark refers to the division of data into smaller, more manageable chunks known as <code>partitions</code>. Partitions are the basic units of parallelism in Spark, and they allow the framework to process different portions of the data simultaneously on different nodes in a cluster.</p>"},{"location":"data-processing/apache-spark/spark-optimization/#why-partition-data","title":"Why partition data?","text":"<p>Partitioning serves several essential purposes in Spark:</p> <ul> <li>Parallelism</li> </ul> <p>By dividing data into partitions, Spark can distribute these partitions across multiple nodes in a cluster. This enables parallel processing, significantly improving the performance of data operations.</p> <ul> <li>efficient data processing</li> </ul> <p>Smaller partitions are easier to manage and manipulate. When a specific operation is performed on a partition, it affects a smaller subset of the data, reducing memory overhead.</p> <ul> <li>data locality</li> </ul> <p>Spark aims to process data where it resides. By creating partitions that align with the distribution of data across nodes, Spark can optimize data locality, minimizing data transfer over the network.</p> <p>Spark uses 2 types of partitioning:</p>"},{"location":"data-processing/apache-spark/spark-optimization/#partitioning-in-memory","title":"Partitioning in memory","text":"<p>While transfrorming data Spark allows users to control partitioning explicitly by using <code>repartition</code> or <code>coalesce</code>.</p>"},{"location":"data-processing/apache-spark/spark-optimization/#repartition","title":"repartition","text":"<ul> <li>It allows to specify the desired number of partitions and the column(s) to partition by.</li> <li>It shuffles the data to create the specified number of partitions.</li> </ul>"},{"location":"data-processing/apache-spark/spark-optimization/#coalesce","title":"coalesce","text":"<ul> <li>It reduces the number of partitions by merging them.</li> <li>It\u2019s useful when you want to decrease the number of partitions for efficiency.</li> </ul> <p>Info</p> <p>By default, Spark/PySpark creates partitions that are equal to the number of CPU cores in the machine.</p> <p>Example</p> <p>We repartition data into 4 partitions based on a column named <code>sex</code>.</p> <pre><code>df = df.repartition(4, \"sex\")\nprint(df.rdd.getNumPartitions())\n</code></pre>"},{"location":"data-processing/apache-spark/spark-optimization/#partitioning-on-disk","title":"Partitioning on disk","text":"<p>When writing data in Spark, the <code>partitionBy()</code> method is used to partition the data into a file system, resulting in multiple sub-directories.</p> <p>This partitioning enhances the read performance for downstream systems.</p> <p>The function can be applied to one or multiple column values while writing a DataFrame to the disk.  Based on these values, Spark splits the records according to the partition column and stores the data for each partition in a respective sub-directory.</p> <p>In the following code, we are saving data to the file system in parquet format, partitioning it based on the <code>sex</code> column.</p> <pre><code>df.write.mode(\"overwrite\").partitionBy(\"sex\").parquet(\"data/output\")\n</code></pre> <p>Output</p> <p></p>"},{"location":"data-processing/apache-spark/spark-optimization/#bucketing","title":"Bucketing","text":"<p>Bucketing is a technique used in Spark for optimizing data storage and querying performance, especially when dealing with large datasets. It involves dividing data into a fixed number of buckets and storing each bucket as a separate file.</p>"},{"location":"data-processing/apache-spark/spark-optimization/#why-bucket-data","title":"Why bucket data?","text":"<ul> <li>Efficient Data Retrieval</li> </ul> <p>When querying data, Spark can narrow down the search by reading only specific buckets, reducing the amount of data to scan. This results in faster query performance.</p> <ul> <li>Uniform Bucket Sizes</li> </ul> <p>Bucketing ensures that each bucket contains approximately the same number of records, preventing data skew.</p>"},{"location":"data-processing/apache-spark/spark-optimization/#how-to-bucket-data","title":"How to bucket data","text":"<p>Bucketing is typically applied to DataFrames or tables using the <code>bucketBy</code> method in Spark SQL. You specify the number of buckets and the column to bucket by.</p> <p>Example</p> <p>we create 5 buckets based on a column named <code>age</code> then we write data to a table named <code>bucketed_table</code></p> <pre><code>df.write.bucketBy(5, \"age\").saveAsTable(\"bucketed_table\")\n</code></pre> <p>You can also specify sorting within buckets, which can further optimize certain query types</p> <pre><code>df.write.bucketBy(10, \"age\")\\\n    .sortBy(\"name\")\\\n    .saveAsTable(\"sorted_bucketed_table\")\n</code></pre>"},{"location":"data-processing/apache-spark/spark-optimization/#choosing-between-partitioning-and-bucketing","title":"Choosing between partitioning and bucketing","text":"<p>The decision of whether to use partitioning or bucketing (or both) in your Spark application depends on the specific use case and query patterns.</p> <p></p> <p>Here are some guidelines:</p> <ul> <li>Partitioning</li> </ul> <p>when you need to distribute data for parallel processing and optimize data locality. It\u2019s beneficial for <code>filtering</code> and <code>joins</code> where the filter condition aligns with the partition key.</p> <ul> <li>Bucketing</li> </ul> <p>when you have large datasets and want to optimize query performance, especially for equality-based filters. It\u2019s beneficial for scenarios where data is frequently queried with specific criteria. You are supposed to use bucketing while the column cardinality is quite high, means data variance is a lot, using partitioning in this type of data will create a lot of files, leads to <code>small files problem</code>.</p> <ul> <li>Combining Both</li> </ul> <p>In some cases, combining partitioning and bucketing can yield the best results. You can partition data by a high-level category and then bucket it within each partition.</p>"},{"location":"data-processing/apache-spark/spark-optimization/#pyspark-code","title":"PySpark code","text":"<p>Warning</p> <p>to run following code, you need to have write access to the writing directory or external locations, such as: <code>S3 bucket</code> or <code>GCS</code>.</p> <p>You also might need to setup connections to the <code>Hive Metastore</code>.</p> <p>Here is an example of pyspark code</p> <pre><code>import pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('partition-bucket-app').getOrCreate()\n\ndf = spark.createDataFrame(\n    [(\"person_1\", \"20\"), (\"person_2\", \"56\"), (\"person_3\", \"89\"), (\"person_4\", \"20\")],\n    [\"name\",\"age\"]\n)\n\n# Partitionning\ndf = df.repartition(4, \"age\")\nprint(df.rdd.getNumPartitions())\n\n# Coalesce\ndf = df.coalesce(2)\nprint(df.rdd.getNumPartitions())\n\n# PartitionBy\ndf.write.mode(\"overwrite\").partitionBy(\"age\").csv(\"data/output\")\n\n# Bucketing \ndf.write.bucketBy(5, \"age\").saveAsTable(\"bucketed_table\")\n\ndf.write.bucketBy(10, \"age\")\\\n    .sortBy(\"name\")\\\n    .saveAsTable(\"sorted_bucketed_table\")\n</code></pre>"},{"location":"data-processing/apache-spark/spark-optimization/#references","title":"References","text":"<ul> <li>Recommendations for Optimizing a Spark job</li> <li>Apache Spark Partitioning and Bucketing</li> </ul>"},{"location":"data-processing/bigquery/","title":"Google BigQuery","text":"<p>BigQuery is a fully managed enterprise data warehouse that helps you manage and analyze your data with built-in features like machine learning, geospatial analysis, and business intelligence.</p> <p>Its serverless architecture lets you use SQL queries to answer your organization's biggest questions with zero infrastructure management. Federated queries let you read data from external sources while streaming supports continuous data updates.</p> <p>BigQuery's scalable, distributed analysis engine lets you query terabytes in seconds and petabytes in minutes.</p>"},{"location":"data-processing/bigquery/#bigquery-architecture","title":"BigQuery Architecture","text":"<p>BigQuery\u2019s serverless architecture decouples storage and compute and allows them to scale independently on demand.</p> <p>This structure offers both immense flexibility and cost controls for customers because they don\u2019t need to keep their expensive compute resources up and running all the time.</p> <p>This is very different from traditional node-based cloud data warehouse solutions or on-premise massively parallel processing (MPP) systems.</p> <p>This approach also allows customers of any size to bring their data into the data warehouse and start analyzing their data using Standard SQL without worrying about database operations and system engineering.</p> <p></p> <p>Under the hood, BigQuery employs a vast set of multi-tenant services driven by low-level Google infrastructure technologies like Dremel, Colossus, Jupiter and Borg.</p> <p></p>"},{"location":"data-processing/bigquery/#dremel-the-execution-engine","title":"Dremel: The Execution Engine","text":"<p>Compute is Dremel, a large multi-tenant cluster that executes SQL queries</p> <ul> <li>Dremel turns SQL queries into execution trees</li> </ul> <p>The leaves of the tree are called slots and do the heavy lifting of reading data from storage and any necessary computation. The branches of the tree are \u2018mixers\u2019, which perform the aggregation.</p> <ul> <li>Dremel dynamically apportions slots</li> </ul> <p>it will dynamically apportions slots to queries on an as-needed basis, maintaining fairness for concurrent queries from multiple users. A single user can get thousands of slots to run their queries.</p>"},{"location":"data-processing/bigquery/#colossus-distributed-storage","title":"Colossus: Distributed Storage","text":"<p>Storage is Colossus, Google's global storage system.</p> <ul> <li>BigQuery leverages the columnar storage format and compression algorithm to store data in Colossus</li> </ul> <p>It is optimized for reading large amounts of structured data.</p> <ul> <li>Colossus also handles replication, recovery (when disks crash) and distributed management (so there is no single point of failure)</li> </ul> <p>Colossus allows BigQuery users to scale to dozens of petabytes of data stored seamlessly, without paying the penalty of attaching much more expensive compute resources as in traditional data warehouses.</p>"},{"location":"data-processing/bigquery/#borg-compute","title":"Borg: Compute","text":"<p>To give you thousands of CPU cores dedicated to processing your task, BigQuery takes advantage of Borg, Google\u2019s large-scale cluster management system.</p> <p>Borg clusters run on dozens of thousands of machines and hundreds of thousands of cores, so your query which used 3300 CPUs only used a fraction of the capacity reserved for BigQuery, and BigQuery\u2019s capacity is only a fraction of the capacity of a Borg cluster.</p> <p>Borg assigns server resources to jobs; the job in this case is the Dremel cluster.</p>"},{"location":"data-processing/bigquery/#jupiter-the-network","title":"Jupiter: The Network","text":"<p>Jupiter networking infrastructure might be the single biggest differentiator in Google Cloud Platform.</p> <p>It provides enough bandwidth to allow 100,000 machines to communicate with any other machine at 10 Gbs. The networking bandwidth needed to run our query would use less than 0.1% of the total capacity of the system.</p> <p>This full-duplex bandwidth means that locality within the cluster is not important. If every machine can talk to every other machine at 10 Gbps, racks don\u2019t matter.</p>"},{"location":"data-processing/bigquery/#how-you-get-started-with-bigquery","title":"How you get started with BigQuery?","text":"<p>You can start using BigQuery simply by loading data and running SQL commands. There\u2019s no need to build, deploy, or provision clusters; no need to size VMs, storage, or hardware resources; no need to setup disks, define replication, configure compression and encryption, or any other setup or configuration work necessary to build a traditional data warehouse.</p> <p>you can try to play around in BigQuery Sandbox</p> <p>Query example</p> <pre><code>SELECT \n  EXTRACT(YEAR FROM creation_date) AS year,\n  EXTRACT(MONTH FROM creation_date) AS month,\n  COUNT(creation_date) AS number_posts\nFROM\n  `bigquery-public-data.stackoverflow.stackoverflow_posts`\nWHERE\n  answer_count &gt; 0\nGROUP BY year, month\nORDER BY year ASC, month ASC\n</code></pre> <p>Output</p> <p></p>"},{"location":"data-processing/bigquery/#references","title":"References","text":"<ul> <li>Google BigQuery Overview</li> <li>BigQuery under the hood</li> <li>Dremel: Interactive Analysis of Web-Scale Datasets</li> <li>Storage Architecture and Challenges by Google</li> <li>GCP: A look inside Google's Data center networks</li> <li>Large-scale cluster management at Google with Borg</li> </ul>"},{"location":"data-quality/","title":"Data Quality","text":"<p>Data engineering is all about delivering high-quality data to the right people at the right time.</p> <p>Note</p> <p>High-quality data is essential for making accurate and reliable decisions.</p> <p>Poor data quality can lead to poor business decisions, which can lead to lost revenue, decreased customer satisfaction, increased costs, and damaged reputation.</p> <p>So, what does high-quality data mean, why is it important, and how to evaluate and measure it?</p>"},{"location":"data-quality/#what-is-bad-data","title":"What is bad data?","text":"<p>One effective approach to grasping the concept of quality data is to consider its opposite: what is bad data? Which types of data consistently get complaints from stakeholders?</p> <p>Let's look at a few real-life examples:</p> <ul> <li> <p>Data accuracy: For example, in the computation of net revenue, there's a risk of overlooking a specific cost type, resulting in an inaccurately calculated revenue figure.</p> </li> <li> <p>Data freshness: Stakeholders engage in daily analyses relying on the previous day's figures. If they discover that the numbers haven't been updated, frustration ensues, causing a delay in their decision-making process.</p> </li> <li> <p>Breaking schema changes: Data users encounter challenges when a column or table has been deleted or renamed, disrupting the functionality of their scripts.</p> </li> <li> <p>Column description: The column names or descriptions are not descriptive enough for users to comprehend their meaning effectively.</p> </li> <li> <p>Data duplication: A bug in the SQL logic can cause duplicates.</p> </li> <li> <p>Data availability: The table or the database is not reachable.</p> </li> </ul> <p>The list can go on and on, but we can summarize the above examples into the following dimensions of data quality.</p>"},{"location":"data-quality/#data-quality-dimensions","title":"Data quality dimensions","text":"<p>Data quality dimensions can be broadly grouped into 2 categories:</p> <ul> <li>business dimensions</li> <li>technical dimensions</li> </ul>"},{"location":"data-quality/#business-dimensions","title":"Business dimensions","text":"<p>Note</p> <p>Ultimately, the output of data engineering work is to derive data insights, whether it\u2019s used to generate some metric dashboards, or it is used to directly power some product features, or it is used as input data to machine learning models.</p> <p>Either way, it must be enriched with a business context that aids downstream users. Having a business-driven mindset means that we should understand the business problems and ensure that data quality is evaluated from a business perspective before focusing on technical aspects.</p> <p></p>"},{"location":"data-quality/#descriptive","title":"Descriptive","text":"<p>Having clear and informative column names and descriptions in a data warehouse is more important than most people realize.</p> <p>Danger</p> <p>A wrong interpretation of the metric can have serious consequences for decision-making and people would only realize it after some time.</p> <p>Example</p> <p>Suppose a company has a metric <code>ticket_resolved_time</code> that measures the time it takes for customer support tickets to be resolved.</p> <p>Without descriptions, it is difficult to make the correct \"guess.\"</p> <p>Some people believe it is the time elapsed between when the customer first opens a ticket and when it is resolved, while others think it should be measured from when the support team picks up the ticket until it is resolved.</p> <p>Clear column descriptions provide rich context and understanding of the data. Further down the line, it also facilitates data governance and management.</p>"},{"location":"data-quality/#user-driven","title":"User-driven","text":"<p>Any data engineering system is built to solve specific business problems. In a way, the downstream use cases set expectations for the metric's definition and data quality.</p> <p>It's common that metrics become redundant after a year or so due to decreased demand from the business side.</p> <p>Therefore, monitoring metric usability to keep metrics as relevant to business as possible is an essential task.</p>"},{"location":"data-quality/#technical-dimensions","title":"Technical dimensions","text":"<p>Technical dimensions are a set of measurable dimensions that can be incorporated into data pipelines because, as the saying goes, \u201cwhat gets measured gets improved.\u201d</p> <p>These dimensions serve as a blueprint for developing a comprehensive data quality framework across the entire company.</p> <p>There are several data tools that provide pre-built solutions to ensure the quality of data in these areas.</p> <p></p>"},{"location":"data-quality/#availability","title":"Availability","text":"<p>High data availability means that the data is easily and readily accessible to authorized users at any time.</p> <p>If a table is not available, the reason can be attributed to either an issue with the cloud provider, such as BigQuery, or a problem with the data pipeline. Most cloud providers provide an SLA (service level agreement), a contract that specifies the level of service and performance guarantees that the provider will deliver to its customers.</p> <p>Example</p> <p>For instance, BigQuery promises an uptime of 99.99%. Whenever a table is unavailable due to BigQuery issues, as BigQuery users, we can get certain compensation.</p>"},{"location":"data-quality/#freshness","title":"Freshness","text":"<p>Fresh data refers to the most up-to-date information that is available for analysis.</p> <p>This means that the data is as recent as possible and reflects the latest changes that have occurred in the source system.</p> <p>The freshness of the data is an important factor in making timely and informed business decisions.</p> <p>Example</p> <p>In the case of a daily revenue dashboard, missing data from yesterday can cause delays in the financial analysis.</p> <p>Freshness has become an indispensable measurement as more companies aim to achieve near real-time analysis.</p>"},{"location":"data-quality/#uniqueness","title":"Uniqueness","text":"<p>One of the common issues in the source data is duplication which can lead to misleading analysis.</p> <p>It is worth noting that the definition of uniqueness varies from case to case.</p> <p>Example</p> <p>It could be unique on the column level such as <code>customer_id</code>.</p> <p>This means each row should have a unique customer ID, while the remaining columns may or may not be the same. It could also be unique at the level of a couple of columns such as the combination of <code>order_id</code> and <code>product_id</code>, sometimes even at the level of the entire row.</p>"},{"location":"data-quality/#completeness","title":"Completeness","text":"<p>In reality, data points could be missing for many reasons, whether it is caused by a source issue or a bug in the business logic, or an inappropriate join or filter condition.</p> <p>Data completeness is a dimension that captures the number of data points containing values.</p> <p>Note</p> <p>In certain situations, incomplete data is expected given the nature of the business.</p> <p>However, in other cases, incomplete data may be a sign of a data issue that requires attention.</p>"},{"location":"data-quality/#schema-validity","title":"Schema validity","text":"<p>Data schema is a fundamental component in a data warehouse, and is crucial for meeting users' expectations.</p> <p>Users are typically the first to report the issue when there is a breaking change in the data schema.</p> <p>Generally, data schema includes a few aspects:</p> <ul> <li>Format \u2014 whether it's a CSV file, JSON file, or BigQuery table</li> <li>File/table name</li> <li>Column name and column type</li> </ul> <p>Tip</p> <p>A well-structured data schema communicates the purpose of the data and specifies its constraints.</p> <p>The more constraints included in the schema, the less likely inaccurate data will result. Before any major change happens, it's extremely important to communicate effectively with data users.</p> <p>Since 2022, data contracts have become a popular technique for ensuring that data always adheres to the schema.</p> <p>Every company should establish its own data quality framework, including the abovementioned dimensions.</p> <p>A solid framework standardizes the approach to managing data quality across the company and ensures that everybody has the same understanding of what good data is.</p> <p>Some open source frameworks can provide data quality based on use cases:</p> <ul> <li>dbt test from https://docs.getdbt.com/docs/build/data-tests</li> <li>soda.io</li> <li>great expectations</li> <li>open metadata</li> <li>many more</li> </ul> <p>Reference: Educative - Data Quality</p>"},{"location":"data-quality/dbt-test/","title":"Dbt test","text":"<p>To achieve high data quality, testing stands as an indispensable component within the data pipeline. In this context, we will discover the testing framework in dbt, which empowers developers to verify various data metrics in an automated fashion.</p>"},{"location":"data-quality/dbt-test/#data-test-vs-unit-test","title":"Data test vs. unit test","text":"<p>Before writing a test, let's make sure we understand the distinction between a data test and a unit test.</p> <ul> <li>A data test is a runtime evaluation conducted within the production pipeline that assesses data quality from various perspectives like freshness and uniqueness.   It typically runs after the table is updated with the latest data.</li> </ul> <p>Example</p> <p>For instance, if the source data introduces a new <code>category</code> value, the data test should detect the change and send an alert if necessary.</p> <p>Its role extends to detecting both data issues coming from the source as well as logic errors within the data warehouse.</p> <ul> <li>a unit test is conducted before deploying the code change to production, and it focuses on testing the transformation logic rather than the data.</li> </ul> <p>Note</p> <p>During unit test, a dedicated test dataset with expented input and output is prepared instead of using unpredictable production data.</p> <p>The goal is to identify logic bugs, such as inaccuracies in metric formulas or the absence of a filter before the release.</p>"},{"location":"data-quality/dbt-test/#testing-in-dbt","title":"testing in dbt","text":"<p>The tests in dbt are <code>select</code> statements that search for failing records, records that don't meet certain conditions.</p> <p>Out of the box, dbt provides a few built-in tests, such as <code>unique</code>, <code>not_null</code>, <code>accepted_values</code>, etc.</p> <p>The dbt_utils package provides more advanced tests such as <code>at_least_one</code>, <code>not_null_proportion</code>, etc. It's also supported by the communities.</p> <p>Here is an example of the <code>dbt test</code>. Tests are defined in the model configuration file.</p> <pre><code>version: 2\n\nmodels:\n  - name: table_sales\n    columns:\n      - name: quantity\n        tests:\n          - not_null\n      - name: product\n        tests:\n          - accepted_values:\n              values: [\"apple\", \"pear\", \"banana\"]\n</code></pre> <p>the command to run dbt test is</p> <pre><code>dbt test -s &lt;model name&gt;\n</code></pre> <p>Under the hood, dbt runs the following SQL statement:</p> <pre><code>select\n      count(*) as failures,\n      count(*) != 0 as should_warn,\n      count(*) != 0 as should_error\nfrom (\n\n    with all_values as (\n        select\n            product as value_field,\n            count(*) as n_records\n        from `&lt;project_id&gt;`.`dbt`.`table_sales`\n        group by product\n\n    )\n    select *\n    from all_values\n    where value_field not in (\n        'apple','pear','banana'\n    )\n) dbt_internal_test\n</code></pre> <p>Note</p> <p>In most circumstances, the <code>dbt test</code> is as important as the model because it validates the data quality at every stage of the pipeline, certainly boosting confidence in data accuracy.</p> <p>In addition, dbt provides a common framework for testing, making it easier for teams to collaborate and share knowledge.</p> <p>If you want to run some example, refer to Running dbt and Google BigQuery using Docker</p>"},{"location":"learning-python/","title":"learning Python repository","text":"<p>This is intended to cover Python for Data Engineering, but it won't be fully covered, for some basic python as like data structure, creating functions, comprehensions, you must understand it by exploring from Internet. This learning materials will mostly cover about underrated/not being used frequently python knowledge, so people might be unaware of it.</p> <ul> <li>Object Oriented Programming</li> <li>Functional Programming</li> <li>Unit Testing</li> <li>Scaling Python</li> </ul>"},{"location":"learning-python/functional-programming/","title":"Functional Programming in Python","text":"<p>A pure function is a function whose output value follows solely from its input values, without any observable side effects. In functional programming, a program consists entirely of evaluation of pure functions. Computation proceeds by nested or composed function calls, without changes to state or mutable data.</p>"},{"location":"learning-python/functional-programming/#how-it-works","title":"How it works","text":"<p>In functional programming, it's useful to have these 2 abilities:</p> <ul> <li>to take another function as an argument</li> <li>to return another function to its caller</li> </ul> <p>This suits very well with Python as everything is object.</p> <p>example</p> <pre><code>def func():\n    print(\"I am function func()!\")\n\nfunc()\n# Output: I am function func()!\n\nanother_name = func\nanother_name()\n# Output: I am function func()!\n</code></pre>"},{"location":"learning-python/functional-programming/#advantanges","title":"Advantanges","text":"<ul> <li>High level: You\u2019re describing the result you want rather than explicitly specifying the steps required to get there. Single statements tend to be concise but pack a lot of punch.</li> <li>Transparent: The behavior of a pure function depends only on its inputs and outputs, without intermediary values. That eliminates the possibility of side effects, which facilitates debugging.</li> <li>Parallelizable: Routines that don\u2019t cause side effects can more easily run in parallel with one another.</li> </ul>"},{"location":"learning-python/functional-programming/#anonymous-function","title":"Anonymous Function","text":"<p>Functional programming is all about calling functions and passing them around, so it naturally involves defining a lot of functions. You can always define a function in the usual way, using the <code>def</code> keyword as you have seen.</p> <p>Sometimes, though, it\u2019s convenient to be able to define an anonymous function on the fly, without having to give it a name. In Python, you can do this with a <code>lambda</code> expression.</p> <pre><code>lambda &lt;parameter list&gt;: &lt;expression&gt;\n</code></pre> <p>example:</p> <pre><code>reverse = lambda s: s[::-1]\nreverse(\"I am a string\")\n# Output: 'gnirts a ma I'\n</code></pre>"},{"location":"learning-python/functional-programming/#applying-function-to-an-iterable-with-map","title":"Applying Function to an Iterable with <code>map()</code>","text":"<p><code>map()</code> is Python built-in function. With <code>map()</code>, you can apply a function to each element in an iterable in turn, and <code>map()</code> will return an iterator that yields the results.</p> <pre><code>map(&lt;function&gt;, &lt;iterable&gt;)\n</code></pre> <p>Example:</p> <pre><code>def reverse(s):\n    return s[::-1]\n\nanimals = [\"cat\", \"dog\", \"hedgehog\", \"gecko\"]\niterator = map(reverse, animals)\n\nfor a in iterator:\n    print(a)\n\n# tac\n# god\n# gohegdeh\n# okceg\n</code></pre>"},{"location":"learning-python/functional-programming/#selecting-elements-from-iterable-with-filter","title":"Selecting Elements from Iterable with <code>filter()</code>","text":"<p><code>filter()</code> allows you to select or filter items from an iterable based on evaluation of the given function. It\u2019s called as follows:</p> <pre><code>filter(&lt;f&gt;, &lt;iterable&gt;)\n</code></pre> <p>filter(, ) applies function  to each element of  and returns an iterator that yields all items for which  is truthy. <p>Example:</p> <pre><code>def greater_than_100(x):\n    return x &gt; 100\n\nlist(filter(greater_than_100, [1, 111, 2, 222, 3, 333]))\n# Output: [111, 222, 333]\n</code></pre>"},{"location":"learning-python/functional-programming/#reducing-an-iterable-to-a-single-value-with-reduce","title":"Reducing an Iterable to a Single Value with <code>reduce()</code>","text":"<p><code>reduce()</code> applies a function to the items in an iterable two at a time, progressively combining them to produce a single result.</p> <p>The most straightforward reduce() call takes one function and one iterable, as shown below:</p> <pre><code>reduce(&lt;f&gt;, &lt;iterable&gt;)\n</code></pre> <p>reduce(, ) uses , which must be a function that takes exactly two arguments, to progressively combine the elements in . To start, reduce() invokes  on the first two elements of . That result is then combined with the third element, then that result with the fourth, and so on until the list is exhausted. Then reduce() returns the final result. <pre><code>from functools import reduce\n\n\ndef f(x, y):\n    return x + y\n\n\nreduce(f, [1, 2, 3, 4, 5])\n# Output: 15\n</code></pre>"},{"location":"learning-python/functional-programming/#challenge","title":"Challenge","text":"<ul> <li>Follow on <code>Task</code> below for the problem statement.</li> <li>Check <code>Solution</code></li> </ul> TaskSolution <pre><code>\"\"\"\n    Filter ONLY Even numbers\n\n    task 1\n        Create a function to identify even number\n        Hint: use modulo '%'\n\n    task 2\n        use filter() to map the function into iterable\n\"\"\"\n\nnumbers = list(range(100))\n\ndef is_even(x):\n    # task 1 here\n    pass\n\n# task 2 here\n</code></pre> <pre><code>\"\"\" Solution: Filter ONLY Even numbers \"\"\"\n\nnumbers = list(range(100))\n\ndef is_even(x):\n    return x % 2 == 0\n\nprint(list(filter(is_even, numbers)))\n</code></pre> <p>Reference: Functional Programming in Python</p>"},{"location":"learning-python/oop/","title":"Object-oriented programming","text":"<p>Object-oriented programming, also referred to as OOP, is a programming paradigm that includes, or relies, on the concept of classes and objects.</p> <p>Note</p> <p>The basic entities in object-oriented programming are classes and objects.</p> <p>Programming isn\u2019t much use if you can\u2019t model real-world scenarios using code, right? This is where object-oriented programming comes.</p> <p>Note</p> <p>The basic idea of OOP is to divide a sophisticated program into a number of objects that talk to each other.</p> <p>Objects in a program frequently represent real-world objects.</p> <p></p> <p>It is also possible for objects to serve application logic and have no direct, real-world parallels. They manage things like authentication, templating, request handling, or any of the other myriad features needed for a practical application.</p> <p>There are couple of items to completely understand Object Oriented Programming concept in Python:</p> <ul> <li>Class</li> <li>Encapsulation</li> <li>Inheritance</li> <li>Polymorphism</li> <li>Object Relationship</li> </ul> <p>Reference: Educative: Learn Object Oriented Programming in Python</p>"},{"location":"learning-python/oop/class/","title":"Class","text":"<p>In Python, classes are defined as follows:</p> <pre><code>class ClassName:\n    pass\n</code></pre> <p>The <code>class</code> keyword tells the compiler that we are creating a custom class, which is followed by the class name and the <code>:</code> sign.</p> <p>All the properties and methods of the class will be defined within the class scope.</p>"},{"location":"learning-python/oop/class/#what-is-an-initializer","title":"What is an initializer?","text":"<p>As the name suggests, the initializer is used to initialize an object of a class. It\u2019s a special method that outlines the steps that are performed when an object of a class is created in the program. It\u2019s used to define and assign values to instance variables.</p> <p>The initialization method is similar to other methods but has a pre-defined name, <code>__init__</code>.</p> <p>Info</p> <p>The double underscores mean this is a special method that the Python interpreter will treat as a special case.</p> <p>The initializer is a special method because it does not have a return type. The first parameter of <code>__init__</code> is <code>self</code>, which is a way to refer to the object being initialized.</p> <p>It is always a good practice to define it as the first member method in the class definition.</p>"},{"location":"learning-python/oop/class/#defining-initializers","title":"Defining initializers","text":"<p>Initializers are called when an object of a class is created. See the example below:</p> <pre><code>class Employee:\n    # defining the properties and assigning them None\n    def __init__(self, ID, salary, department):\n        self.ID = ID\n        self.salary = salary\n        self.department = department\n\n\n# creating an object of the Employee class with default parameters\nSteve = Employee(3789, 2500, \"Human Resources\")\n\n# Printing properties of Steve\nprint(\"ID :\", Steve.ID)\nprint(\"Salary :\", Steve.salary)\nprint(\"Department :\", Steve.department)\n</code></pre> <p>The initializer is automatically called when an object of the class is created. Now that we will be using initializers to make objects, a good practice would be to initialize all of the object properties when defining the initializer.</p> <p>Warning</p> <p>It is important to define the initializer with complete parameters to avoid any errors. Similar to methods, initializers also have the provision for optional parameters.</p>"},{"location":"learning-python/oop/class/#challenges","title":"Challenges","text":"<ul> <li>Take a look on Task: Calculate Student Performance and its solution</li> </ul> Task 1Solution <pre><code>\"\"\"\n    Calculate Student Performance\n\n    Problem statement\n        Implement a class - Student - that has four properties and two methods. All these attributes (properties and methods) should be public. This problem can be broken down into three tasks.\n\n    Task 1\n        Implement a constructor to initialize the values of four properties: name, phy, chem, and bio.\n\n    Task 2\n        Implement a method \u2013 totalObtained \u2013 in the Student class that calculates total marks of a student.\n\n    Task 3\n        Using the totalObtained method, implement another method, percentage, in the Student class that calculates the percentage of students marks.\n        Assume that the total marks of each subject are 100. The combined marks of three subjects are 300.\n        Formula: percentage = (marks_obtained/total_marks) * 100\n\"\"\"\n\nclass Student:\n    def __init__(self):\n        pass # to fill\n\n    def totalObtained(self):\n        pass # to fill\n\n    def percentage(self):\n        pass # to fill\n</code></pre> <pre><code>class Student:\n    def __init__(self, name, phy, chem, bio):\n        self.name = name\n        self.phy = phy\n        self.chem = chem\n        self.bio = bio\n\n    def totalObtained(self):\n        return(self.phy + self.chem + self.bio)\n\n    def percentage(self):\n        return((self.totalObtained() / 300) * 100)\n</code></pre> <ul> <li>Let's continue to Task: Implement Calculator Class and its solution</li> </ul> Task 2Solution <pre><code>\"\"\"\n    Implement Calculator Class\n\n    Write a Python class called Calculator by completing the tasks below:\n\n    Task 1\n        Initializer\n        Implement an initializer to initialize the values of num1 and num2.\n\n    Properties\n        num1\n        num2\n\n    Task 2\n        Methods\n            add() is a method that returns the sum of num1 and num2.\n            subtract() is a method that returns the subtraction of num1 from num2.\n            multiply() is a method that returns the product of num1 and num2.\n            divide() is a method that returns the division of num2 by num1.\n\n    Input\n        Pass numbers (integers or floats) in the initializer.\n\n    Output\n        addition, subtraction, division, and multiplication\n\n    Sample input\n        obj = Calculator(10, 94);\n        obj.add()\n        obj.subtract()\n        obj.multiply()\n        obj.divide()\n\n    Sample output\n        104\n        84\n        940\n        9.4\n\"\"\"\n\nclass Calculator:\n    def __init__(self):\n        pass\n\n    def add(self):\n        pass\n\n    def subtract(self):\n        pass\n\n    def multiply(self):\n        pass\n\n    def divide(self):\n        pass\n</code></pre> <pre><code>class Calculator:\n    def __init__(self, num1, num2):\n        self.num1 = num1\n        self.num2 = num2\n\n    def add(self):\n        return (self.num2 + self.num1)\n\n    def subtract(self):\n        return (self.num2 - self.num1)\n\n    def multiply(self):\n        return (self.num2 * self.num1)\n\n    def divide(self):\n        return (self.num2 / self.num1)\n\n\ndemo1 = Calculator(10, 94)\nprint(\"Addition:\", demo1.add())\nprint(\"Subtraction:\", demo1.subtract())\nprint(\"Mutliplication:\", demo1.multiply())\nprint(\"Division:\", demo1.divide())\n</code></pre>"},{"location":"learning-python/oop/encapsulation/","title":"Encapsulation","text":"<p>Encapsulation is a fundamental programming technique used to achieve data hiding in OOP.</p> <p>Info</p> <p>Encapsulation in OOP refers to binding data and the methods to manipulate that data together in a single unit, that is, class.</p> <p>Depending upon this unit, objects are created. Encapsulation is usually done to hide the state and representation of an object from outside. A class can be thought of as a capsule having methods and properties inside it.</p>"},{"location":"learning-python/oop/encapsulation/#get-and-set","title":"Get and Set","text":"<p>In order to allow controlled access to properties from outside the class, getter and setter methods are used.</p> <p>Note</p> <p>A getter method allows reading a property\u2019s value.</p> <p>A setter method allows modifying a property\u2019s value.</p>"},{"location":"learning-python/oop/encapsulation/#challenge","title":"Challenge","text":"<ul> <li>Follow on Task: Implement Complete Student Class and its Solution</li> </ul> TaskSolution <pre><code>\"\"\"\n    Implement Complete Student Class\n\n    Task\n        Implement the following properties as private:\n            name\n            rollNumber\n\n        Include the following methods to get and set the private properties above:\n            getName()\n            setName()\n            getRollNumber()\n            setRollNumber()\n\n        Implement this class according to the rules of encapsulation.\n\n    Input\n        Checking all the properties and methods\n\n    Output\n        Expecting perfectly defined fields and getter/setters\n\n    Note: Do not use initializers to initialize the properties. Use the set methods to do so. If the setter is not defined properly, the corresponding getter will also generate an error even if the getter is defined properly.\n\"\"\"\n\nclass Student:\n    def setName(self):\n        pass\n\n    def getName(self):\n        pass\n\n    def setRollNumber(self):\n        pass\n\n    def getRollNumber(self):\n        pass\n</code></pre> <pre><code>class Student:\n    __name = None\n    __rollNumber = None\n\n    def setName(self, name):\n        self.__name = name\n\n    def getName(self):\n        return self.__name\n\n    def setRollNumber(self, rollNumber):\n        self.__rollNumber = rollNumber\n\n    def getRollNumber(self):\n        return self.__rollNumber\n\n\ndemo1 = Student()\ndemo1.setName(\"Alex\")\nprint(\"Name:\", demo1.getName())\ndemo1.setRollNumber(3789)\nprint(\"Roll Number:\", demo1.getRollNumber())\n</code></pre>"},{"location":"learning-python/oop/inheritance/","title":"Inheritance","text":""},{"location":"learning-python/oop/inheritance/#definition","title":"Definition","text":"<p>Inheritance provides a way to create a new class from an existing class. The new class is a specialized version of the existing class such that it inherits all the non-private fields (variables) and methods of the existing class. The existing class is used as a starting point or as a base to create the new class.</p>"},{"location":"learning-python/oop/inheritance/#the-is-a-relationship","title":"The IS A Relationship","text":"<p>After reading the above definition, the next question that comes to mind is this: when do we use inheritance? Wherever we come across an IS A relationship between objects, we can use inheritance.</p> <p></p> <p>In the above illustration, we can see the objects have a IS A relationship between them. We can write it as:</p> <ul> <li>Square IS A shape</li> <li>Python IS A programming language</li> <li>Car IS A vehicle</li> </ul>"},{"location":"learning-python/oop/inheritance/#what-is-the-super-function","title":"What is the <code>super()</code> function?","text":"<p>The use of <code>super()</code> comes into play when we implement inheritance. It is used in a child class to refer to the parent class without explicitly naming it. It makes the code more manageable, and there is no need to know the name of the parent class to access its attributes.</p> <p>Note</p> <p>Make sure to add parenthesis at the end to avoid a compilation error.</p> <pre><code>class Vehicle:  # defining the parent class\n    fuelCap = 90\n\n\nclass Car(Vehicle):  # defining the child class\n    fuelCap = 50\n\n    def display(self):\n        # accessing fuelCap from the Vehicle class using super()\n        print(\"Fuel cap from the Vehicle Class:\", super().fuelCap)\n\n        # accessing fuelCap from the Car class using self\n        print(\"Fuel cap from the Car Class:\", self.fuelCap)\n\n\nobj1 = Car()  # creating a car object\nobj1.display()  # calling the Car class method display()\n</code></pre>"},{"location":"learning-python/oop/inheritance/#advantages","title":"Advantages","text":"<ul> <li>Reusability: Inheritance makes the code reusable</li> <li>Less code modification, less risky</li> <li>Extensibility: Using inheritance, one can extend the base class as per the requirements of the derived class</li> <li>Important data hiding: The base class can keep some data private so that the derived class cannot alter it. This concept is called encapsulation.</li> </ul>"},{"location":"learning-python/oop/inheritance/#challenge","title":"Challenge","text":"<ul> <li>Follow on Task: Handling Bank Account and its solution</li> </ul> TaskSolution <pre><code>\"\"\"\n    Handling Bank Account\n\n    Task 1\n        In the Account class, implement the getBalance() method that returns balance.\n\n    Task 2\n        In the Account class, implement the deposit(amount) method that adds amount to the balance. It does not return anything.\n\n        Input\n            balance = 2000\n            deposit(500)\n            getbalance()\n\n        Output: 2500\n\n    Task 3\n        In the Account class, implement the withdrawal(amount) method that subtracts the amount from the balance. It does not return anything.\n\n        Input\n            balance = 2000\n            withdrawal(500)\n            getbalance()\n\n        Output: 1500\n\n    Task 4\n        In the SavingsAccount class, implement an interestAmount() method that returns the interest amount of the current balance. Below is the formula for calculating the interest amount:\n        interest_amount = (interest_rate * balance) / 100\n\n        Input\n            balance = 2000\n            interestRate = 5\n            interestAmount()\n\n        Output: 100\n\"\"\"\n\nclass Account:\n    def __init__(self, title=None, balance=0):\n        self.title = title\n        self.balance = balance\n\n    def withdrawal(self, amount):\n        # write code here\n        pass\n\n    def deposit(self, amount):\n        # write code here\n        pass\n\n    def getBalance(self):\n        # write code here\n        pass\n\n\nclass SavingsAccount(Account):\n    def __init__(self, title=None, balance=0, interestRate=0):\n        super().__init__(title, balance)\n        self.interestRate = interestRate\n\n    def interestAmount(self):\n        # write code here\n        pass\n\n\n# code to test - do not edit this\ndemo1 = SavingsAccount(\"Mark\", 2000, 5)  # initializing a SavingsAccount object\n</code></pre> <pre><code>class Account:  # parent class\n    def __init__(self, title=None, balance=0):\n        self.title = title\n        self.balance = balance\n\n    # withdrawal method subtracts the amount from the balance\n    def withdrawal(self, amount):\n        self.balance = self.balance - amount\n\n    # deposit method adds the amount to the balance\n    def deposit(self, amount):\n        self.balance = self.balance + amount\n\n    # this method just returns the value of balance\n    def getBalance(self):\n        return self.balance\n\n\nclass SavingsAccount(Account):\n    def __init__(self, title=None, balance=0, interestRate=0):\n        super().__init__(title, balance)\n        self.interestRate = interestRate\n\n    # computes interest amount using the interest rate\n    def interestAmount(self):\n        return (self.balance * self.interestRate / 100)\n\n\nobj1 = SavingsAccount(\"Steve\", 5000, 10)\nprint(\"Initial Balance:\", obj1.getBalance())\nobj1.withdrawal(1000)\nprint(\"Balance after withdrawal:\", obj1.getBalance())\nobj1.deposit(500)\nprint(\"Balance after deposit:\", obj1.getBalance())\nprint(\"Interest on current balance:\", obj1.interestAmount())\n</code></pre>"},{"location":"learning-python/oop/object-relationship/","title":"Object Relationship","text":""},{"location":"learning-python/oop/object-relationship/#interaction-between-class-objects","title":"Interaction between class objects","text":"<p>While inheritance represents a relationship between classes, there are situations where there are relationships between objects.</p> <p>Now we have to use different class objects to create the design of an application. This means that independent class objects will have to find a way to interact with each other.</p> <p></p>"},{"location":"learning-python/oop/object-relationship/#challenge","title":"Challenge","text":"<ul> <li>Follow on Task: Implementing Sports team and its solution</li> </ul> TaskSolution <pre><code>\"\"\"\n    Implementing Sports team\n\n    You have to implement 3 classes, School, Team, and Player, such that an instance of a School should contain instances of Team objects.\n    Similarly, a Team object can contain instances of Player class.\n\n    You have to implement a School class containing a list of Team objects and a Team class comprising a list of Player objects.\n\n    Task 1\n        The Player class should have three properties that will be set using an initializer:\n            ID\n            name\n            teamName\n\n    Task 2\n        The Team class will have two properties that will be set using an initializer:\n            name\n            players: a list with player class objects in it\n\n        It will have two methods:\n            addPlayer(), which will add new player objects in the players list\n            getNumberOfPlayers(), which will return the total number of players in the players list\n\n    Task 3\n        The School class will contain two properties that will be set using an initializer:\n            teams, a list of team class objects\n            name\n\n        It will have two methods:\n            addTeam, which will add new team objects in the teams list\n            getTotalPlayersInSchool(), which will count the total players in all of the teams in the School and return the count\n\"\"\"\n\n# Player class\nclass Player:\n    pass\n    # Complete the implementation\n\n\n# Team class contains a list of Player\n# Objects\nclass Team:\n    pass\n\n    # Complete the implementation\n\n\n# School class contains a list of Team\n# objects.\nclass School:\n    pass\n\n\n# Complete the implementation\n</code></pre> <pre><code>class Player:\n    def __init__(self, ID, name, teamName):\n        self.ID = ID\n        self.name = name\n        self.teamName = teamName\n\n\nclass Team:\n    def __init__(self, name):\n        self.name = name\n        self.players = []\n\n    def getNumberOfPlayers(self):\n        return len(self.players)\n\n    def addPlayer(self, player):\n        self.players.append(player)\n\n\nclass School:\n    def __init__(self, name):\n        self.name = name\n        self.teams = []\n\n    def addTeam(self, team):\n        self.teams.append(team)\n\n    def getTotalPlayersInSchool(self):\n        length = 0\n        for n in self.teams:\n            length = length + (n.getNumberOfPlayers())\n        return length\n\n\np1 = Player(1, \"Harris\", \"Red\")\np2 = Player(2, \"Carol\", \"Red\")\np3 = Player(1, \"Johnny\", \"Blue\")\np4 = Player(2, \"Sarah\", \"Blue\")\n\nred_team = Team(\"Red Team\")\nred_team.addPlayer(p1)\nred_team.addPlayer(p2)\n\nblue_team = Team(\"Blue Team\")\nblue_team.addPlayer(p2)\nblue_team.addPlayer(p3)\n\nmySchool = School(\"My School\")\nmySchool.addTeam(red_team)\nmySchool.addTeam(blue_team)\n\nprint(\"Total players in mySchool:\", mySchool.getTotalPlayersInSchool())\n</code></pre>"},{"location":"learning-python/oop/polymorphism/","title":"Polymorphism","text":"<p>Assume there is a parent class named <code>Shape</code> from which the child classes <code>Rectangle</code>, <code>Circle</code>, <code>Polygon</code>, and <code>Diamond</code> are derived.</p> <p>Suppose your application will need methods to calculate the area of each specific shape. The area for each shape is calculated differently, which is why you can\u2019t have a single implementation. You could throw in separate methods in each class (for instance, <code>getSquareArea()</code>, <code>getCircleArea()</code> etc.). But this makes it harder to remember each method\u2019s name.</p>"},{"location":"learning-python/oop/polymorphism/#make-things-simpler-with-polymorphism","title":"Make things simpler with polymorphism","text":"<p>It would be better if all specific area calculation methods could be called getArea(). You would only have to remember one method name and when you call that method, the method specific to that object would be called. This can be achieved in object-oriented programming using polymorphism. The base class declares a function without providing an implementation. Each derived class inherits the function declaration and can provide its own implementation</p> <p>Consider that the Shape class has a method called <code>getArea()</code>, which is inherited by all subclasses mentioned. With polymorphism, each subclass may have its own way of implementing the method. For example, when the <code>getArea()</code> method is called on an object of the <code>Rectangle</code> class, the method will respond by displaying the area of the rectangle. On the other hand, when the same method is called on an object of the <code>Circle</code> class, the circle\u2019s area will be calculated and displayed on the screen.</p>"},{"location":"learning-python/oop/polymorphism/#polymorphism-through-method","title":"Polymorphism through <code>method</code>","text":"<p>Consider two shapes that are defined as classes: Rectangle and Circle. These classes contain the <code>getArea()</code> method that calculates the area for the respective shape depending on the values of their properties.</p> <pre><code>class Rectangle():\n\n    # initializer\n    def __init__(self, width=0, height=0):\n        self.width = width\n        self.height = height\n        self.sides = 4\n\n    # method to calculate Area\n    def getArea(self):\n        return (self.width * self.height)\n\n\nclass Circle():\n    # initializer\n    def __init__(self, radius=0):\n        self.radius = radius\n        self.sides = 0\n\n    # method to calculate Area\n    def getArea(self):\n        return (self.radius * self.radius * 3.142)\n\n\nshapes = [Rectangle(6, 10), Circle(7)]\nprint(\"Sides of a rectangle are\", str(shapes[0].sides))\nprint(\"Area of rectangle is:\", str(shapes[0].getArea()))\n\nprint(\"Sides of a circle are\", str(shapes[1].sides))\nprint(\"Area of circle is:\", str(shapes[1].getArea()))\n</code></pre>"},{"location":"learning-python/oop/polymorphism/#polymorphism-through-inheritance","title":"Polymorphism through Inheritance","text":"<p>Consider the example of a <code>Shape</code> class, which is the base class while many shapes like <code>Rectangle</code> and <code>Circle</code> extending from the base class are derived classes. These derived classes inherit the <code>getArea()</code> method and provide a shape-specific implementation, which calculates its area.</p> <pre><code>class Shape:\n    def __init__(self):  # initializing sides of all shapes to 0\n        self.sides = 0\n\n    def getArea(self):\n        pass\n\n\nclass Rectangle(Shape):  # derived from Shape class\n    # initializer\n    def __init__(self, width=0, height=0):\n        self.width = width\n        self.height = height\n        self.sides = 4\n\n    # method to calculate Area\n    def getArea(self):\n        return (self.width * self.height)\n\n\nclass Circle(Shape):  # derived from Shape class\n    # initializer\n    def __init__(self, radius=0):\n        self.radius = radius\n\n    # method to calculate Area\n    def getArea(self):\n        return (self.radius * self.radius * 3.142)\n\n\nshapes = [Rectangle(6, 10), Circle(7)]\nprint(\"Area of rectangle is:\", str(shapes[0].getArea()))\nprint(\"Area of circle is:\", str(shapes[1].getArea()))\n</code></pre>"},{"location":"learning-python/oop/polymorphism/#challenges","title":"Challenges","text":"<ul> <li>Follow on Task: Implement Animal Class and its solution</li> </ul> TaskSolution <pre><code>\"\"\"\n    Implement Animal Class\n\n    A parent class named Animal.\n\n    Inside it, define:\n        name\n        sound\n        __init__()\n        Animal_details() function\n            It prints the name and sound of the Animal.\n\n    Then there are two derived classes\n\n    Dog class\n        Has a property family\n        Has an initializer that calls the parent class initializer in it through super()\n        Has an overridden method named Animal_details() which prints detail of the dog.\n\n    Sheep class\n        Has a property color\n        Has an initializer that calls the parent class initializer in it through super()\n        Has an overridden method named Animal_details(), which prints detail of the sheep\n\n    The derived classes should override the Animal_details() method defined in the Animal class.\n        The overridden method in Dog class should print the value of family as well as the name and sound.\n        The overridden method in Sheep class should print the value of color as well as the name and sound\n\n    Input\n        name of Dog is set to Pongo, sound is set to Woof Woof, and family is set to Carnivore in the initializer of Dog object.\n        name of Sheep is set to Billy, sound is set to Baaa Baaa, and color is set to White in the initializer of Sheep object.\n        Now, call Animal_details() from their respective objects.\n\n    Sample Input\n        d = Dog(\"Pongo\", \"Woof Woof\", \"Husky\")\n        d.Animal_details()\n        print(\" \")\n        s = Sheep(\"Billy\", \"Baaa Baaa\", \"White\")\n        s.Animal_details()\n\n    Sample Output\n        Name: Pongo\n        Sound: Woof Woof\n        Family: Husky\n\n        Name: Billy\n        Sound: Baa Baa\n        Color: White\n\"\"\"\n\nclass Animal:\n    pass\n    # write your class here\n\nclass Dog(Animal):\n    pass\n    # write your class here\n\nclass Sheep(Animal):\n    pass\n# write your class here\n</code></pre> <pre><code>class Animal:\n    def __init__(self, name, sound):\n        self.name = name\n        self.sound = sound\n\n    def Animal_details(self):\n        print(\"Name:\", self.name)\n        print(\"Sound:\", self.sound)\n\n\nclass Dog(Animal):\n    def __init__(self, name, sound, family):\n        super().__init__(name, sound)\n        self.family = family\n\n    def Animal_details(self):\n        super().Animal_details()\n        print(\"Family:\", self.family)\n\n\nclass Sheep(Animal):\n    def __init__(self, name, sound, color):\n        super().__init__(name, sound)\n        self.color = color\n\n    def Animal_details(self):\n        super().Animal_details()\n        print(\"Color:\", self.color)\n\n\nd = Dog(\"Pongo\", \"Woof Woof\", \"Husky\")\nd.Animal_details()\nprint(\"\")\ns = Sheep(\"Billy\", \"Baa Baa\", \"White\")\ns.Animal_details()\n</code></pre>"},{"location":"learning-python/scaling-python/","title":"Scaling Python","text":"<p>We are all aware that processors are not becoming fast at a rate where a single threaded application could, one day, be fast enough to handle any size workload. That means we need to think about using more than just one processor. Building scalable applications implies that you distribute the workload across multiple workers using multiple processing units.</p>"},{"location":"learning-python/scaling-python/#types-of-applications","title":"Types of applications","text":"<ul> <li>Single-threaded application</li> </ul> <p>This should be your first pick, and indeed it implies no distribution.</p> <ul> <li>Multi-threaded application</li> </ul> <p>Most computers, even your smartphone, are now equipped with multiple processing units.   If an application can overload an entire CPU, it needs to spread its workload over other processors by spawning new threads (or new processes).</p> <ul> <li>Network distributed application</li> </ul> <p>This is your last resort when your application needs to scale significantly,   and not even one big computer with plenty of CPUs is enough.   These are the most complicated applications to write because they involve a network.</p>"},{"location":"learning-python/scaling-python/#multithreading","title":"Multithreading","text":"<p>Scaling across processors is usually done using multithreading. Multithreading is the ability to run code in parallel using threads. Since they run in parallel, that means they can be executed on separate processors even if they are contained in a single process.</p> <p>Therefore, when writing a multithreaded application, the code always runs concurrently but runs in parallel only if there is more than one CPU available.</p>"},{"location":"learning-python/scaling-python/#drawbacks","title":"Drawbacks","text":"<p>If you have been in the Python world for a long time, you have probably encountered the word <code>GIL</code>, and know how hated it is. The GIL is the Python <code>Global Interpreter Lock</code>, a lock that must be acquired each time CPython needs to execute byte-code. Unfortunately, this means that if you try to scale your application by making it run multiple threads, this global lock always limits the performance of your code.</p> <p>The reason that the GIL is required in the first place is that it makes sure that some basic Python objects are thread-safe. For example, the code in the following example would not be thread-safe without global Python lock</p> <pre><code>import threading\n\nx = []\n\ndef append_two(l):\n  l.append(2)\n\nthreading.Thread(target=append_two, args=(x,)).start()\nx.append(1)\nprint(x)\n</code></pre> <p>That code prints either <code>[2,1]</code> or <code>[1,2]</code> no matter what, while there's no way to know which thread appends 1 or 2 before the other. This will be difficult to achieve Idempotency.</p>"},{"location":"learning-python/scaling-python/#distributed-systems","title":"Distributed Systems","text":"<p>When an application uses all the CPU power of a node, and you can't add more processors to your server or switch to a bigger server, you need a plan B. The next step usually involves multiple servers, linked together via a network of some sort.</p> <p></p> <ul> <li><code>Horizontal scalability</code></li> </ul> <p>the ability to add more nodes as more traffic comes in.</p> <ul> <li><code>Fault tolerance</code></li> </ul> <p>If a node goes down, another one can pick up the traffic of the dysfunctioning one.</p> <p>All of this means that an application which is going the distributed route expands its complexity while potentially increasing its throughput. Making this kind of architecture decision requires great wisdom.</p>"},{"location":"learning-python/scaling-python/#service-oriented-architecture","title":"Service-Oriented Architecture","text":"<p>If you've never heard of it, service-oriented architecture is an architectural style where a software design is made up of several independent components communicating over a network. Each service is a discrete unit of functionality that can work autonomously.</p> <p></p>"},{"location":"learning-python/scaling-python/#statelessness","title":"Statelessness","text":"<p>Service built for this kind of architecture should follow a few principles among them being stateless. That means services must either modify and return the requested value (or an error) while separating their functioning from the state of data. This is an essential property, as it makes it easier to scale the services horizontally.</p>"},{"location":"learning-python/scaling-python/#caveats-of-splitting","title":"Caveats of splitting","text":"<p>Having too many services has a cost, as they come with some overhead. Think of all of the costs associated, such as maintenance and deployment, not only development time. Splitting an application should always be a well-thought out decision.</p>"},{"location":"learning-python/scaling-python/#several-effort-to-scaling-python-in-its-code","title":"Several Effort to scaling Python in its code","text":"<ul> <li>CPU Scaling</li> <li>Asynchronous Events</li> <li>Queue-Based Distribution</li> </ul> <p>Reference: The Hacker's Guide to Scaling Python by Educative</p>"},{"location":"learning-python/scaling-python/async-solution/","title":"Async Solution","text":""},{"location":"learning-python/scaling-python/async-solution/#blocking-nature-of-readwrite-operations","title":"Blocking nature of read/write operations","text":"<p>The most used source of events is I/O readiness. Most read and write operations are, by default, blocking in nature which slows down the program execution speed. If the program has to wait several seconds for a read to be completed, it cannot do anything else during that time. <code>read</code> is a synchronous call and when performed on a file, socket, etc., that has no data ready to be read, it blocks the program.</p>"},{"location":"learning-python/scaling-python/async-solution/#solution-dont-just-wait-use-asyncio","title":"Solution: Don't just wait! Use <code>asyncio</code>","text":"<p>Asyncio is centered on the concept of event loops. Once <code>asyncio</code> has created an event loop, an application registers the functions to call back when a specific event happens: as time passes, a file descriptor is ready to be read, or a socket is ready to be written.</p> <p>That type of function is called a <code>coroutine</code>. It is a particular type of function that can give back control to the caller so that the event loop can continue running.</p> <p></p> <pre><code>import asyncio\n\nasync def hello_world():\n    print(\"hello world!\")\n    return 42\n\nhello_world_coroutine = hello_world()\nprint(hello_world_coroutine)\n\nevent_loop = asyncio.get_event_loop()\ntry:\n    print(\"entering event loop\")\n    result = event_loop.run_until_complete(hello_world_coroutine)\n    print(result)\nfinally:\n    event_loop.close()\n</code></pre> <p>Above example shows a very straightforward implementation of an event loop using a coroutine. The coroutine <code>hello_word</code> is defined as a function, except that the keyword to start its definition is <code>async def</code> rather than just <code>def</code>. This coroutine just prints a message and returns a result.</p>"},{"location":"learning-python/scaling-python/async-solution/#run-coroutine-cooperatively","title":"Run <code>coroutine</code> cooperatively","text":"<p>We can also run <code>coroutine</code> in a flow manner.</p> <p></p> <pre><code>import asyncio\n\nasync def add_42(number):\n    print(\"Adding 42\")\n    return 42 + number\n\nasync def hello_world():\n    print(\"hello world!\")\n    result = await add_42(23)\n    return result\n\nevent_loop = asyncio.get_event_loop()\ntry:\n    result = event_loop.run_until_complete(hello_world())\n    print(result)\nfinally:\n    event_loop.close()\n</code></pre> <p>The <code>await</code> keyword is used to run the <code>coroutine</code> cooperatively. <code>await</code> gives the control back to the event loop, registering the coroutine <code>add_42(23)</code> into it.</p>"},{"location":"learning-python/scaling-python/async-solution/#sleep-and-gather","title":"<code>sleep</code> and <code>gather</code>","text":"<ul> <li><code>asyncio.sleep</code> is the asynchronous implementation of <code>time.sleep</code>.</li> </ul> <p>It is a coroutine that sleeps some number of seconds.</p> <ul> <li><code>asyncio.gather</code> allows you to wait for several coroutines at once using a single <code>await</code> keyword.</li> </ul> <pre><code>import asyncio\n\nasync def hello_world():\n    print(\"hello world!\")\n\nasync def hello_python():\n    print(\"hello Python!\")\n    await asyncio.sleep(0.1)\n\nevent_loop = asyncio.get_event_loop()\ntry:\n    result = event_loop.run_until_complete(asyncio.gather(\n        hello_world(),\n        hello_python(),\n    ))\n    print(result)\nfinally:\n    event_loop.close()\n</code></pre>"},{"location":"learning-python/scaling-python/async-solution/#async-in-http-call","title":"Async in HTTP call","text":"<p>1 more example of asynchronous process, calling HTTP, it's not depended to each other. The <code>aiohttp</code> library provides an asynchronous HTTP.</p> <pre><code>import aiohttp\nimport asyncio\n\nasync def get(url):\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as response:\n            return response\n\nloop = asyncio.get_event_loop()\ncoroutines = [get(\"http://example.com\") for _ in range(8)]\nresults = loop.run_until_complete(asyncio.gather(*coroutines))\n\nprint(\"Results: %s\" % results)\n</code></pre>"},{"location":"learning-python/scaling-python/async-solution/#challenge","title":"Challenge","text":"<ul> <li>Follow below task and its solution.</li> </ul> TaskSolution <pre><code>\"\"\"\n    Problem\n        When a webpage loads, its base HTML page has a bunch of resources that the browser needs to fetch in order to display the page completely.\n        However, the time that the browser takes to fetch all those resources determines its quality.\n\n    In this challenge, suppose you are writing code for building a browser.\n    You have to fetch three URLs: http://educative.io, http://educative.io/blog, and http://youtube.com.\n\n    Hint: You can use asyncio and aiohttp.\n\"\"\"\n\n## Import other libraries if you want (asyncio and aiohttp are supported)\nimport time\n\nurls = [\"http://educative.io\", \"http://educative.io/blog\", \"http://youtube.com\"]\n\n##########################################\n### Start your code here\n\n\n### End code here\n##########################################\nprint(\"Results: %s\" % results)\n</code></pre> <pre><code>import aiohttp\nimport asyncio\nimport time\n\nurls = [\"http://educative.io\", \"http://educative.io/blog\", \"http://youtube.com\"]\n\nasync def get(url):\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as response:\n            return response\n\nloop = asyncio.get_event_loop()\n\ncoroutines = []\n\nfor URL in urls:\n    coroutines.append( get(URL))\n\nstart_time = time.time()\nresults = loop.run_until_complete(asyncio.gather(*coroutines))\nprint(\"--- %s milliseconds ---\" % ((time.time() - start_time)*1000))\n\nprint(\"Results: %s\" % results)\n</code></pre> <ul> <li>run command</li> </ul> <pre><code>docker build -t async docs/learning-python/scaling-python/async-solution/ &amp;&amp; docker run async\n</code></pre>"},{"location":"learning-python/scaling-python/cpu-scaling/","title":"CPU Scaling","text":"<p>As CPUs are not getting infinitely faster, using multiple CPUs is the best path towards scalability. That means introducing concurrency and parallelism into your program.</p>"},{"location":"learning-python/scaling-python/cpu-scaling/#threads","title":"Threads","text":"<p>Threads in Python are a good way to run a function concurrently with other functions. If your system does not support multiple processors, the threads will be executed one after another as scheduled by the operating system. However, if multiple CPUs are available, threads could be scheduled on multiple processing units, once again as determined by the operating system.</p> <p></p> <pre><code>import threading\n\ndef print_something(something):\n    print(something)\n\nt = threading.Thread(target=print_something, args=(\"hello\",))\nt.start()\nprint(\"thread started\")\nt.join()\n</code></pre>"},{"location":"learning-python/scaling-python/cpu-scaling/#drawbacks","title":"Drawbacks","text":"<p>If you specifically expected any one of the outputs each time, then you forgot that there's no guarantee regarding the order of execution for the threads. If you do not join all your threads and wait for them to finish, it is possible that the main thread finishes and exits before the other threads. If this happens, your program will appear to be blocked and will not respond to even a simple <code>KeyboardInterrupt</code> signal.</p>"},{"location":"learning-python/scaling-python/cpu-scaling/#threads-as-daemons","title":"Threads as <code>daemons</code>","text":"<p>To avoid this, and because your program might not be in a position to wait for the threads, you can configure threads as daemons. When a thread is a daemon, it's considered as a background thread by Python and is terminated as soon as the main thread exits.</p> <pre><code>import threading\n\ndef print_something(something):\n    print(something)\n\nt = threading.Thread(target=print_something, args=(\"hello\",))\nt.daemon = True # Specifying as Daemon here\nt.start()\nprint(\"thread started\") # No longer a need to use the JOIN method\n</code></pre>"},{"location":"learning-python/scaling-python/cpu-scaling/#using-processes","title":"Using <code>Processes</code>","text":"<p>Since multithreading is not a perfect scalability solution because of GIL, using <code>processes</code> instead of threads is a good alternative.</p> <p><code>multiprocessing</code> library is a good, higher-level alternative. It provides an interface that starts new processes, whatever your operating system might be. It also provides a pool mechanism that is useful in more functional manner.</p> <pre><code>import multiprocessing\nimport random\n\ndef compute(n):\n    return sum(\n        [random.randint(1, 100) for i in range(1000000)])\n\nif __name__ == \"__main__\":\n    # Start 8 workers\n    pool = multiprocessing.Pool(processes=8)\n    print(\"Results: %s\" % pool.map(compute, range(8)))\n</code></pre> <p></p>"},{"location":"learning-python/scaling-python/cpu-scaling/#challenge","title":"Challenge","text":"<ul> <li>Follow below task and its solution.</li> </ul> TaskSolution <pre><code>\"\"\"\n    Problem\n        You are given a list of one million integers named mylist.\n\n        However, since we have studied that parallelism can be used to utilize more cores of the CPU to compute the result faster we know that the simplest way to parallelize a program is to use use threads.\n\n    One thing to keep in mind while using threads is that the tasks assigned to the threads should be independent of the task of other threads.\n    In other words, you have to divide your problem into smaller non-overlapping chunks that can be done independently.\n\n    Hint: A list can be divided into many partitions and the min of all the chunks can be computed separately.\n    Then, the min of the computed mins of all chunks will be the global minimum.\n\"\"\"\n\n# Relevant libraries are imported already\nimport random\nimport threading\n\n# mylist contains 1 million entries ranging from 1 to 100000000\nmylist = [random.randint(1, 100000000) for i in range(1000000)]\nminimum = 0\n########\n# Your code goes here #\n\n\n#   Code until here   #\n########\n\n# Result:\nprint(\"Global Minimum: \", minimum)\n</code></pre> <pre><code>\"\"\"\n    First of all the mylist can be divided into non-overlaping chunks.\n    Then, the minimum of each chunk can be computed separately.\n    We use threads to compute the minimum of each of the four chunks and store all four minimums in the mins list.\n    Finally, we calculate the minimum of those four minimums which is the global minimum of the list.\n\"\"\"\n\nimport random\nimport threading\n\nmylist = [random.randint(1, 100000000) for i in range(1000000)]\nmins = []\n\ndef calc_min(li):\n    minimum = li[0]\n    for x in li:\n        if x &lt; minimum:\n            minimum = x\n\n    mins.append(minimum)\n\n# Dividing list in halves\nl1 = mylist[:len(mylist)//2]\nl2 = mylist[len(mylist)//2:]\n\n# Dividing list in quaters\nq1 = l1[:len(l1)//2]\nq2 = l1[len(l1)//2:]\nq3 = l2[:len(l2)//2]\nq4 = l2[len(l2)//2:]\n\nworkers = []\nworkers.append( threading.Thread(target=calc_min, args=(q1,)) )\nworkers.append( threading.Thread(target=calc_min, args=(q2,)) )\nworkers.append( threading.Thread(target=calc_min, args=(q3,)) )\nworkers.append( threading.Thread(target=calc_min, args=(q4,)) )\n\nfor worker in workers:\n    worker.start()\nfor worker in workers:\n    worker.join()\n\nprint(\"Global Minimum: \", min(mins))\n</code></pre> <p>Tip</p> <p>This challenge is using <code>threads</code> as its simplest approach, if you want to challenge yourself further, you can try to use <code>multiprocessing</code> library, which I believe, it should be simpler and cleaner code.</p>"},{"location":"learning-python/scaling-python/queue-distribution/","title":"Queue-Based Distribution","text":"<p>In some cases, the best response is to accept a job and promise to execute it later. Implementing a distributed system using queues is a trade-off, improving throughput with the drawback of increased latency.</p> <p>Looking at basic architecture, it's composed of two elements, steps as follow:</p> <ul> <li>a queue that stores jobs and workers that consume the jobs from a queue</li> <li>process them</li> <li>send the result to another destination</li> </ul> <p></p> <p>When an application makes such a promise, a job distribution mechanism is needed. The upside is that if correctly implemented, it allows the application to scale its workload horizontally pretty easily.</p>"},{"location":"learning-python/scaling-python/queue-distribution/#rq","title":"RQ","text":"<p>the RQ Library provides a simple and direct approach to implement a queue in Python program. The letter R and Q are used to designate Redis and Queue.</p> <p><code>Redis</code> is an open-source in-memory database project implementing a networked, in-memory key-value store with optional durability.</p>"},{"location":"learning-python/scaling-python/queue-distribution/#rq-workers","title":"RQ workers","text":"<p>By using RQ, an application can push jobs into a Redis database and have workers executing these jobs asynchronously.</p> <p>Warning</p> <p>You can only run below code once redis server is already run.</p> rq-enqueue.py<pre><code>import time\nfrom rq import Queue\nfrom redis import Redis\n\nq = Queue(connection=Redis())\n\njob = q.enqueue(sum, [42, 43])\n# Wait until the result is ready --\nwhile job.result is None:\n    time.sleep(1)\n\nprint(job.result)\n</code></pre>"},{"location":"learning-python/scaling-python/queue-distribution/#celery","title":"Celery","text":"<p>Celery is another queue management system. In contrast to RQ, it is broker agnostic and can use various software as a broker, such as Redis, RabbitMQ or Amazon SQS.</p>"},{"location":"learning-python/scaling-python/queue-distribution/#celery-backend","title":"Celery backend","text":"<p>Celery also needs a backend for storing the results of the job. It supports a variety of solutions, such as Redis, MongoDB, SQL databases, ElasticSearch, files, etc.</p> <p>Celery implements its own serialization format for its jobs. However, this format is not specific to Python. That means it is possible to implement job creators or consumers in different languages.</p>"},{"location":"learning-python/scaling-python/queue-distribution/#celery-task-execution-worker-and-queues","title":"Celery task execution: Worker and queues","text":"<p>In Celery, tasks are functions that can be called asynchronously. When called, Celery puts them in the broker queue for execution. Remote workers then execute the tasks, putting the task results into the backend.</p> <p>When called, a task returns a <code>celery.result.AsyncResult</code> object.</p> celery-task.py<pre><code>import celery\n\napp = celery.Celery('celery-task',\n                    broker='redis://localhost',\n                    backend='redis://localhost')\n\n@app.task\ndef add(x, y):\n    return x + y\n\nif __name__ == '__main__':\n    result = add.delay(4, 4)\n    print(\"Task state: %s\" % result.state)\n    print(\"Result: %s\" % result.get())\n    print(\"Task state: %s\" % result.state)\n</code></pre> <p>Above example is a simple implementation of a Celery task. The Celery application is created with the main module name as its first argument, and then the URL to access the broker and backends.</p> <p>the <code>app.task</code> function decorator registers the add task so it can be used asynchronously in the application, leveraging Celery workers for execution.</p> <p>If you run the code above, it will be PENDING, until the workers is ready. To initiate the workers, run command</p> <pre><code>celery -A celery-task worker\n</code></pre> <p>Once the worker is ready, the result in earlier code will be shown.</p>"},{"location":"learning-python/scaling-python/queue-distribution/#celery-chaining-tasks","title":"Celery: Chaining Tasks","text":"<p>Celery supports chaining tasks, which allows you to build more complex workflows.</p> <p></p>"},{"location":"learning-python/scaling-python/queue-distribution/#celerychain","title":"<code>celery.chain</code>","text":"celery-chain.py<pre><code>import celery\n\napp = celery.Celery('celery-task',\n                    broker='redis://localhost',\n                    backend='redis://localhost')\n\n@app.task\ndef add(x, y):\n    return x + y\n\nif __name__ == '__main__':\n    result = add.delay(4, 4)\n    print(\"Task state: %s\" % result.state)\n    print(\"Result: %s\" % result.get())\n    print(\"Task state: %s\" % result.state)\n</code></pre> <p>to run code above, you need to start the worker first by running</p> <pre><code>celery -A celery-chain worker\n</code></pre> <p>Building a program with multiple idempotent functions that can be chained together is very natural in functional programming. Again, this kind of design makes it very easy to parallelize job execution and therefore makes it possible to increase the throughput of your program and scale its execution horizontally.</p>"},{"location":"learning-python/scaling-python/queue-distribution/#celery-multiple-queues","title":"Celery: Multiple Queues","text":"<p>By default, Celery uses a single queue named <code>celery</code>. However, it is possible to use multiple queues to spread the distribution of the tasks. This feature makes it possible to have finer control over the distribution of the jobs to execute.</p> <p></p>"},{"location":"learning-python/scaling-python/queue-distribution/#low-priority-job","title":"Low priority job","text":"<p>For example, it's common to have a queue dedicated to low-priority jobs, where only a few workers are available. Notice the new options in the worker command <code>--queues celery, low-priority</code>.</p> celery-task-queue.py<pre><code>import celery\n\napp = celery.Celery('celery-task-queue',\n                    broker='redis://localhost',\n                    backend='redis://localhost')\n\n@app.task\ndef add(x, y):\n    return x + y\n\nif __name__ == '__main__':\n    result = add.apply_async(args=[4, 6], queue='low-priority')\n    print(\"Task state: %s\" % result.state)\n    print(\"Result: %s\" % result.get())\n    print(\"Task state: %s\" % result.state)\n</code></pre> <p>to start the workers with queue low-priority and tag <code>celery</code></p> <pre><code>celery -A celery-task-queue worker --queues celery,low-priority\n</code></pre> <p>Tip</p> <p>You might be familiar with this backend concept as you're using Airflow as much as building the pipelines.</p>"},{"location":"learning-python/unit-testing/","title":"Unit Testing","text":"<p>There are several types of software application testing that are commonly used in the software development lifecycle to ensure the quality and reliability of software applications</p> <p></p> <p>Unit testing is a critical software testing technique used to validate the behavior and functionality of individual units or components in a software application. Automated testing tools are used to ensure that the units are working as intended and meet the specified requirements.</p> <p>Unit testing also promotes better code design and modularity by encouraging developers to write modular and loosely coupled code that is easier to test and maintain. It can also improve code reuse and reduce the overall development time and costs by detecting issues early in the development cycle and reducing the need for manual testing and debugging.</p> <p>Refactoring is the process of improving the design of code without changing its external behavior. Unit tests can help ensure that refactoring does not introduce defects into the application.</p> <p>As in this example, we will be using one of the famous Python library <code>pytest</code></p> <ul> <li>assertion</li> <li>exceptions</li> <li>fixtures</li> <li>decorator</li> <li>parametrizing</li> <li>mocking</li> </ul>"},{"location":"learning-python/unit-testing/#pytest-configuration","title":"Pytest configuration","text":"<p>Pytest configuration is a powerful tool that allows users to customize the behavior of pytest</p>"},{"location":"learning-python/unit-testing/#overview-of-configuration-files","title":"Overview of configuration files","text":"<p>Pytest allows users to specify configuration options using configuration files. Configuration files can be used to set command-line options, define fixtures, and specify other options that control pytest\u2019s behavior. There are two types of configuration files that pytest supports: <code>pytest.ini</code> and <code>conftest.py</code>.</p> <p>The <code>pytest.ini</code> files are INI-style configuration files that can be placed in the root directory of a project or in any directory that contains tests.</p> <p>The <code>conftest.py</code> files are Python files that can be placed in any directory, and they are used to define fixtures and other objects that can be used in tests.</p> <p>Reference: Mastering Unit Testing with Pytest</p>"},{"location":"learning-python/unit-testing/assertion/","title":"Pytest - the <code>assert</code> statement","text":"<p>The <code>assert</code> statement in Python is used for debugging and testing purposes. It allows us to check if a condition is <code>True</code>, and if it's not, it will Raise an <code>Exception</code> called <code>AssertionError</code>. This can be very helpful in writing unit tests to verify if the expected results match the actual results.</p> <pre><code>assert condition, message\n</code></pre>"},{"location":"learning-python/unit-testing/assertion/#check-it-out","title":"Check it out","text":"<ul> <li>Let's see below <code>addition.py</code> and its test case.</li> </ul> addition.py<pre><code>def addition(x, y):\n    return x + y\n</code></pre> test_file.py<pre><code>from addition import addition\n\ndef test_addition_int():\n    assert addition(4, 5) == 9\n    assert addition(12, 2) == 14\n\ndef test_addition_str():\n    assert addition('a', 'b') == 'ab'\n</code></pre> <ul> <li>to test it out, run</li> </ul> <pre><code>docker build -t pytest-assertion docs/learning-python/unit-testing/assertion/ &amp;&amp; docker run pytest-assertion\n</code></pre>"},{"location":"learning-python/unit-testing/assertion/#result","title":"Result","text":"<pre><code>============================= test session starts ==============================\nplatform linux -- Python 3.9.19, pytest-8.2.0, pluggy-1.5.0\nrootdir: /test\ncollected 2 items\n\ntest_file.py ..                                                          [100%]\n\n============================== 2 passed in 0.01s ===============================\n</code></pre>"},{"location":"learning-python/unit-testing/decorator/","title":"Mark Decorator","text":"<p>Pytest marking is a feature that enables categorizing tests and applying specific behaviors or actions to them. It involves tagging tests with markers, which can then be utilized to select, filter, or customize how the tests are executed. Pytest provides several built-in markers, such as <code>@pytest.mark.skip</code> to skip a test and <code>@pytest.mark.parametrize</code> to parametrize a test function with multiple input values</p> <p>Marks are applied to test functions or classes using the <code>@pytest.mark.&lt;MARKER_NAME&gt;</code> decorator.</p>"},{"location":"learning-python/unit-testing/decorator/#expected-failure","title":"Expected failure","text":"<p>This marker marks the test as an expected failure.</p> <pre><code>import pytest\n\n@pytest.mark.xfail\ndef test_example():\n    assert 1 + 1 == 3\n</code></pre>"},{"location":"learning-python/unit-testing/decorator/#conditionally-skipping-the-test","title":"Conditionally skipping the test","text":"<p>This marker skips the test if the condition inside evaluated to <code>True</code>.</p> <pre><code>import sys\nimport pytest\n\n@pytest.mark.skipif(sys.version_info.major == 3 and sys.version_info.minor &lt; 9,\n                    reason=\"requires Python 3.9 or higher\")\ndef test_something():\n    assert True\n</code></pre>"},{"location":"learning-python/unit-testing/decorator/#skipping-the-test","title":"Skipping the test","text":"<p>This marker skips the test.</p> <pre><code>import pytest\n\n@pytest.mark.skip(reason=\"test is not ready yet\")\ndef test_example():\n    assert 1 + 1 == 2\n</code></pre>"},{"location":"learning-python/unit-testing/decorator/#parametrizing-the-test","title":"Parametrizing the test","text":"<p>We can parametrize a test with multiple sets of input values.</p> <pre><code>import pytest\n\n@pytest.mark.parametrize(\"test_input,expected_output\", [(\"3+5\", 8), (\"2+4\", 6), (\"6*9\", 54)])\ndef test_eval(test_input, expected_output):\n    assert eval(test_input) == expected_output\n</code></pre>"},{"location":"learning-python/unit-testing/decorator/#mark-as-a-fixture","title":"Mark as a fixture","text":"<p>Using <code>@pytest.mark.usefixtures</code> decorator can be beneficial when we want to reuse a fixture across multipple tests or ensure that a fixture is always used in a particular test.</p> conftest.py<pre><code>import pytest\n\n@pytest.fixture\ndef my_fixture():\n    return [1,2,3]\n</code></pre> test_file.py<pre><code>import pytest\n\n@pytest.mark.usefixtures(\"my_fixture\")\ndef test_with_fixtures():\n    assert True\n</code></pre>"},{"location":"learning-python/unit-testing/exceptions/","title":"Testing Exceptions","text":"<p>We will delve into testing for exceptions in Python using pytest. As developers, we strive to write resilient code that can handle all possible scenarios. A robust code should be designed to handle exceptions effectively. Exceptions are unexpected events or errors that might occur during the execution of a program, and they can disrupt the normal flow of code execution. By properly handling exceptions, we can prevent crashes and unexpected behaviors and ensure that the code continues to operate gracefully.</p> <p>Let's take a step back and understand what context managers and exceptions in Python are.</p>"},{"location":"learning-python/unit-testing/exceptions/#context-managers","title":"Context managers","text":"<p>Context managers in Python are a way to manage resources like files, network connections, and databases in an efficient manner (pythonic way). They ensure that resources are properly acquired and releases, no matter what. The <code>with</code> statement in Python is commonly used with context managers. It allows us to define a block of code that uses a resource and automatically handles the release of resource.</p> <pre><code># Opening a file using context manager\nwith open(\"file.txt\", \"r\") as file:\n    data = file.read()\n    print(data)\n</code></pre>"},{"location":"learning-python/unit-testing/exceptions/#exceptions","title":"Exceptions","text":"<p>Exceptions in Python are events that occur during the execution of program and disrupt the normal flow of code execution. They are raised when an error or exceptional conditions occur in the program, such as invalid operation, unexpected input, or a runtime error. Exceptions are Python's way of handling errors and providing a mechanism to handle them gracefully. It's very important aspect of writing robust and reliable Python code. Exceptions in Python can be raised explicitly using the <code>raise</code> statement or automatically raised by the Python interpreter when it encounters an error condition.</p> <pre><code>def divide_number(a, b):\n    try:\n        result = a / b\n    except ZeroDivisionError:\n        print(\"Error: Division by zero is not allowed!\")\n    else:\n        return result\n\nprint(divide_numbers(10, 2)) # Output: 5.0\nprint(divide_numbers(10, 0)) # Output: Error: Division by zero is not allowed!\n</code></pre> <p>Python provides a variety of built-in exceptions that cover a wide range of error conditions, such as: <code>TypeError</code>, <code>ValueError</code>, <code>FileNotFoundError</code>, etc</p>"},{"location":"learning-python/unit-testing/exceptions/#check-it-out","title":"Check it out","text":"<ul> <li>Let's see below <code>division.py</code> and its test case.</li> </ul> division.py<pre><code>def divide_numbers(a, b):\n    if b == 0:\n        raise ZeroDivisionError(\"Cannot divide by zero\")\n    return a / b\n</code></pre> test_file.py<pre><code>import pytest\n\nfrom division import divide_numbers\n\ndef test_divide_numbers():\n    with pytest.raises(ZeroDivisionError):\n        divide_numbers(10, 0)\n</code></pre> <ul> <li>to test it out, run</li> </ul> <pre><code>docker build -t pytest-exceptions docs/learning-python/unit-testing/exceptions/ &amp;&amp; docker run pytest-exceptions\n</code></pre>"},{"location":"learning-python/unit-testing/exceptions/#result","title":"Result","text":"<pre><code>============================= test session starts ==============================\nplatform linux -- Python 3.9.19, pytest-8.2.0, pluggy-1.5.0\nrootdir: /test\ncollected 1 item\n\ntest_file.py .                                                           [100%]\n\n============================== 1 passed in 0.01s ===============================\n</code></pre>"},{"location":"learning-python/unit-testing/fixtures/","title":"Fixtures","text":"<p>Fixtures allow us to create reusable code that can be used across multiple tests. A fixture is a function that returns an object that will be used in our tests. They can be used to set up preconditions or data for a test or to clean up after a test. Pytest fixtures are used to make testing code easier and more efficient. They can be used to create test data, set up database connections and more.</p> <p>Fixtures can provide their values to test functions using <code>return</code> or <code>yield</code> statements.</p>"},{"location":"learning-python/unit-testing/fixtures/#configuration-file","title":"Configuration file","text":"<p>In pytest, <code>conftest.py</code> is a special file that allows us to define fixtures, hooks, and plugins that can be shared across multiple test files in a directory or its subdirectories.</p> <p>Fixtures defined in a <code>conftest.py</code> file can be used by any test without importing them. Pytest discovers them automatically.</p> <p>fixtures are defined using <code>@pytest.fixture</code> decorator.</p> test_file.py<pre><code>def test_data(data): # this test will pass\n    assert 'key' in data\n\ndef test_list(lst): # this test will fail and list fixture will be printed\n    assert len(lst) == 5, lst\n</code></pre> conftest.py<pre><code>import pytest\n\n@pytest.fixture\ndef data():\n    return {'key': 'value'}\n\n@pytest.fixture\ndef lst():\n    return [1, 2, 3, 4]\n</code></pre>"},{"location":"learning-python/unit-testing/fixtures/#scope-parameter","title":"Scope parameter","text":"<p>Fixtures can be customized with the help of the <code>scope</code> parameter that determines when the fixture is called and when it is destroyed. The <code>scope</code> parameter in fixtures defines the lifetime of the fixture.</p> <ul> <li><code>function</code>: default scope for a fixture, called once test function is invoked and destroyed after it finished.</li> <li><code>class</code>: called once each test class that uses it and destroyed if it finished all the test methods in the class.</li> <li><code>module</code></li> <li><code>session</code>: called in entire test session and destroyed after test session is completed.</li> </ul>"},{"location":"learning-python/unit-testing/mocking/","title":"Mocking and Patching","text":"<p>Mocking and patching are crucial techniques in software testing that allow the replacement of parts of a system with fake objects called mocks. This technique simulates behavior and data, which enables testing specific parts of a system without relying on other parts. As a result, this technique can help isolate and identify issues in the code.</p> <p>Mocking creates a fake object that mimics the behavior of a real object. The purpose is to isolate the code under test from external dependencies, such as databases or web services.</p> <p>Patching, on the other hand, involves replacing the implementation of a function or method with a mock object.</p>"},{"location":"learning-python/unit-testing/mocking/#patching-using-monkeypatch","title":"Patching using <code>monkeypatch</code>","text":"<p>Pytest provides a <code>monkeypatch</code> fixture that helps safely modify attributes, dictionary items, environment variables, or <code>sys.path</code> for importing in tests.</p> <p>In below example, we define a mock response object with a JSON method that returns the expected response from API request. It will use <code>monkeypatch</code> to replace <code>requests.get</code> with our mock response object so that when <code>get_data</code> is called, it returns our mock response instead of making a real API request.</p> main.py<pre><code>import requests\n\ndef get_json(url):\n    r = requests.get(url)\n    return r # in case of valid API Url, we will return r.json()\n</code></pre> test_main.py<pre><code>import requests\nfrom main import get_json\n\ndef test_get_json(monkeypatch):\n\n    def mock_get(*args, **kwargs):\n        return {\"mock_key\": \"mock_response\"}\n\n    monkeypatch.setattr(requests, \"get\", mock_get)\n\n    result = get_json(\"https://fakeurl\")\n    assert result[\"mock_key\"] == \"mock_response\"\n</code></pre>"},{"location":"learning-python/unit-testing/mocking/#using-mocker","title":"Using <code>mocker</code>","text":"<p>We need to use plugin <code>pytest-mock</code>, it's a pytest plugin that offers an easier-to-use API and integrates seamlessly with pytest fixtures.</p> main.py<pre><code>def divide(a, b):\n    return a / b\n\ndef compute(a, b):\n    result = divide(a, b)\n    return result * 100\n</code></pre> test_main.py<pre><code>import pytest\nfrom main import compute\n\ndef test_compute(mocker):\n    mocker.patch('main.divide', return_value=2)\n    assert compute(10, 5) == 200\n</code></pre>"},{"location":"learning-python/unit-testing/mocking/#magicmock-object","title":"MagicMock object","text":"<p>In addition to <code>mocker.patch()</code>, mocker also provides a <code>mocker.MagicMock</code> object that can be used to create mock objects with custom attributes.</p> <p>Here are some of the methods provided by the <code>MagicMock</code> object:</p> <ul> <li><code>assert_called_once()</code>: This method is used to assert that a mock object was called exactly once.</li> <li><code>assert_called_with(arg1, arg2, ...)</code>: used to assert that a mock object was called with the specified arguments.</li> <li><code>assert_not_called()</code>: used to assert that a mock object was never called.</li> <li><code>assert_any_call(arg1, arg2, ...)</code>: used to assert that a mock object was called with the specified arguments at least once.</li> </ul> <pre><code>import pytest\n\ndef test_my_function_1(mocker):\n    json = mocker.MagicMock()\n    json.loads.assert_not_called()\n    json.loads('{\"key\": \"value\"}')\n    json.loads.assert_called()\n    json.loads.assert_called_once()\n    json.loads.assert_called_with('{\"key\": \"value\"}')\n\ndef test_my_function_2(mocker):\n    json = mocker.MagicMock()\n    json.loads.assert_called_once()\n</code></pre>"},{"location":"learning-python/unit-testing/mocking/#try-yourself-mocking-context-managers","title":"Try yourself - Mocking context managers","text":"<p>Mocking context managers is a common technique in testing for isolating the code being tested from its external dependencies. Context managers are objects that define how a particular block of code should be managed.</p> <ul> <li>we will test <code>my_module.py</code> with its test cases <code>test_my_module.py</code></li> </ul> my_module.py<pre><code>class FileManager:\n    def __init__(self, filename):\n        self.filename = filename\n\n    def __enter__(self):\n        self.file = open(self.filename, 'r')\n        return self.file\n\n    def __exit__(self, exc_type, exc_value, tb):\n        self.file.close()\n\ndef count_lines_in_file(filename):\n    with FileManager(filename) as file:\n        return len(file.readlines())\n</code></pre> test_my_module.py<pre><code>import io\nimport my_module\nimport pytest\n\ndef test_count_lines_in_file(mocker):\n    mock_file_manager = mocker.MagicMock()\n    mock_file = mocker.MagicMock(spec=io.IOBase)\n    mock_file_manager.return_value.__enter__.return_value = mock_file\n\n    mocker.patch('my_module.FileManager', mock_file_manager)\n\n    # Set up the mock file to return 5 lines\n    mock_file.readlines.return_value = ['line\\n'] * 5\n\n    assert my_module.count_lines_in_file('filename.txt') == 5\n</code></pre> <ul> <li>run command</li> </ul> <pre><code>docker build -t pytest-mocking docs/learning-python/unit-testing/mocking/ &amp;&amp; docker run pytest-mocking\n</code></pre>"},{"location":"learning-python/unit-testing/mocking/#result","title":"Result","text":"<pre><code>============================= test session starts ==============================\nplatform linux -- Python 3.9.19, pytest-8.2.0, pluggy-1.5.0\nrootdir: /test\nplugins: mock-3.14.0\ncollected 1 item\n\ntest_my_module.py .                                                      [100%]\n\n============================== 1 passed in 0.01s ===============================\n</code></pre>"},{"location":"learning-python/unit-testing/parametrizing/","title":"Parametrization","text":"<p>Test parametrization is a feature in pytest that allows us to write a single test function and test it with multiple input values. It is a way of writing concise tests that can cover many cases with minimal code.</p> <p>This feature is especially useful when we want to test a function with varying input parameters.</p> <p>The main benefit of this is concise code.</p> main.py<pre><code>def multiply(x, y):\n    return x * y\n</code></pre> test_file.py<pre><code>import pytest\nfrom main import multiply\n\n@pytest.mark.parametrize(\"x, y, result\", [\n    (2, 3, 6),\n    (1, 1, 1),\n    (0, 5, 0),\n    (-2, 3, -6),\n])\ndef test_multiply(x, y, result):\n    assert multiply(x, y) == result\n</code></pre> <p>Given you an example to test Python class in main.py.</p> <ul> <li>run command</li> </ul> <pre><code>docker build -t pytest-parametrizing docs/learning-python/unit-testing/parametrizing/ &amp;&amp; docker run pytest-parametrizing\n</code></pre>"},{"location":"learning-python/unit-testing/parametrizing/#result","title":"Result","text":"<pre><code>============================= test session starts ==============================\nplatform linux -- Python 3.9.19, pytest-8.2.0, pluggy-1.5.0\nrootdir: /test\ncollected 4 items\n\ntest_file.py ....                                                        [100%]\n\n============================== 4 passed in 0.01s ===============================\n</code></pre>"},{"location":"projects/","title":"List of Projects","text":"<p>Hi, you will find list of projects which I had worked on so far:</p> <ul> <li>Running dbt and BigQuery using Docker</li> <li>Automated Data Quality Testing with Great Expectations and BigQuery</li> <li>Deploying and Running Airflow on Kubernetes</li> <li>Building Github Pages using Github Actions</li> <li>Running Apache Flink on Kubernetes</li> <li>Running Spark using Docker Compose</li> <li>Querying Streaming Data in Kafka using ksql</li> <li>Apache Iceberg and PySpark</li> <li>Delta Lake with Apache Spark</li> <li>Apache Hudi and Spark</li> <li>DuckDB Quick Start using Jupyter Notebook</li> </ul> <p>Stay tune, I will keep adding this list.</p>"},{"location":"projects/airflow-k8s/","title":"Deploying and Running Airflow on Kubernetes","text":""},{"location":"projects/airflow-k8s/#quickstart-about-kubernetes","title":"Quickstart about Kubernetes","text":"<p><code>Kubernetes</code> is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.</p> <p>We enter <code>container deployments era</code>. Containers are similar to VMs, but they have relaxed isolation properties to share the Operating System (OS) among the applications. Therefore, containers are considered lightweight. Similar to a VM, a container has its own filesystem, share of CPU, memory, process space, and more. As they are decoupled from the underlying infrastructure, they are portable across clouds and OS distributions.</p> <p>Containers have become popular because they provide extra benefits, such as:</p> <ul> <li>Agile application creation and deployment: increased ease and efficiency of container image creation compared to VM image use.</li> <li>Continuous development, integration, and deployment: provides for reliable and frequent container image build and deployment with quick and efficient rollbacks (due to image immutability).</li> <li>Dev and Ops separation of concerns: create application container images at build/release time rather than deployment time, thereby decoupling applications from infrastructure.</li> <li>Observability: not only surfaces OS-level information and metrics, but also application health and other signals.</li> <li>Loosely coupled, distributed, elastic, liberated micro-services: applications are broken into smaller, independent pieces and can be deployed and managed dynamically \u2013 not a monolithic stack running on one big single-purpose machine.</li> <li>Resource isolation: predictable application performance.</li> <li>Resource utilization: high efficiency and density.</li> </ul>"},{"location":"projects/airflow-k8s/#why-you-need-kubernetes","title":"Why you need Kubernetes","text":"<p>Containers are a good way to bundle and run your applications. In a production environment, you need to manage the containers that run the applications and ensure that there is no downtime.</p> <p>Example</p> <p>if a container goes down, another container needs to start. Wouldn't it be easier if this behavior was handled by a system?</p> <p>What Kubernetes provides:</p> <ul> <li>Service discovery and load balancing</li> </ul> <p>Kubernetes can expose a container using the DNS name or using their own IP address.</p> <ul> <li>Storage orchestration</li> </ul> <p>Kubernetes allows developer to automatically mount any storage systems.</p> <ul> <li>Automated rollouts and rollbacks</li> </ul> <p>Developer can describe the desired state for your deployed containers using Kubernetes, and it can change the actual state to the desired state at a controlled rate.</p> <ul> <li>Automatic bin packing</li> </ul> <p>Developer provides Kubernetes with a cluster of nodes that it can use to run containerized tasks.</p> <ul> <li>Self-healing</li> </ul> <p>Kubernetes restarts containers that fail, replaces containers, kills containers that don't respond to your user-defined health check, and doesn't advertise them to clients until they are ready to serve.\\</p> <ul> <li>Secret and configuration management</li> </ul> <p>Kubernetes lets developers store and manage sensitive information, such as passwords, OAuth tokens, and SSH keys.</p> <ul> <li>Batch execution</li> </ul> <p>Kubernetes can manage your batch and CI workloads, replacing containers that fail, if desired.</p> <ul> <li>Horizontal scaling</li> </ul> <p>Scale your application up and down with a simple command, with a UI, or automatically based on CPU usage.</p> <ul> <li>Designed for extensibility</li> </ul> <p>Add features to your Kubernetes cluster without changing upstream source code.</p> <p>Info</p> <p>Kubernetes is also equipped with <code>kubectl</code> as its CLI command.</p> <p>It provides a tool to communicate with a Kubernetes cluster's control plane, using Kubernetes API.</p>"},{"location":"projects/airflow-k8s/#airflow-kubernetes-executor","title":"Airflow: Kubernetes Executor","text":"<p>The Kubernetes executor runs each task instance in its own pod on a Kubernetes cluster.</p> <p>KubernetesExecutor runs as a process in the Airflow Scheduler. The scheduler itself does not necessarily need to be running on Kubernetes, but does need access to a Kubernetes cluster.</p> <p>KubernetesExecutor requires a non-sqlite database in the backend.</p> <p>When a DAG submits a task, the KubernetesExecutor requests a worker pod from the Kubernetes API. The worker pod then runs the task, reports the result, and terminates.</p> <p></p> <p>This is an example of an Airflow deployment running on a distributed set of Kubernetes nodes and clusters.</p> <p></p> <p>Consistent with the regular Airflow architecture, the Workers need access to the DAG files to execute the tasks within those DAGs and interact with the Metadata repository. Also, configuration information specific to the Kubernetes Executor, such as the worker namespace and image information, needs to be specified in the Airflow Configuration file.</p>"},{"location":"projects/airflow-k8s/#advantages","title":"Advantages","text":"<ul> <li>In contrast to CeleryExecutor, KubernetesExecutor does not require additional components such as Redis, but does require access to Kubernetes cluster.</li> <li>Monitoring Pods can be done with the built-in Kubernetes monitoring.</li> <li>With KubernetesExecutor, each task runs in its own pod. The pod is created when the task is queued, and terminates when the task completes.</li> <li>Resource utilization for each task doesn't need to be worried, because they will spawn each different pods and independent.</li> </ul> <p>Tip</p> <p>In scenarios, such as burstable workloads, this presented a resource utilization advantage over CeleryExecutor, where you needed a fixed number of long-running Celery worker pods, whether or not there were tasks to run.</p>"},{"location":"projects/airflow-k8s/#disadvantages","title":"Disadvantages","text":"<ul> <li>Deploying using Kubernetes is very high learning curve to understand completely Kubernetes and you need to pay attention to every detail if you want to deploy it through Production. It's relatively quite complex.</li> <li>with KubernetesExecutor, you will have more task latency because worker pod is triggered when Airflow trigger the task, it's not ready yet, it will take time to create the Pods.</li> </ul>"},{"location":"projects/airflow-k8s/#deploying-airflow-on-kubernetes","title":"Deploying Airflow on Kubernetes","text":"<p>Warning</p> <p>This setup is only working on MacOS currently (at this point of time), I will try to develop it in other OS in near future</p> <p>you will need to install:</p> <ul> <li> <p>Docker</p> </li> <li> <p>homebrew</p> </li> </ul> <ul> <li>First, you need to clone the repo</li> </ul> <pre><code>git clone git@github.com:karlchris/airflow-k8s.git\n</code></pre> <ul> <li>run below command to install the prerequisites, such as: Docker, KinD, Kubectl</li> </ul> <pre><code>make init\n</code></pre> <p>Output:</p> <pre><code>\u279c  airflow-k8s git:(master) \u2717 make init\nbash scripts/init.sh\nInstalling Kubectl, KinD, Helm, docker and docker compose ...\n==&gt; Downloading https://formulae.brew.sh/api/formula.jws.json\n###################################################################################################################################################################################################################################### 100.0%\n==&gt; Downloading https://formulae.brew.sh/api/cask.jws.json\n###################################################################################################################################################################################################################################### 100.0%\nWarning: kubernetes-cli 1.30.1 is already installed and up-to-date.\nTo reinstall 1.30.1, run:\n  brew reinstall kubernetes-cli\nWarning: kind 0.23.0 is already installed and up-to-date.\nTo reinstall 0.23.0, run:\n  brew reinstall kind\nWarning: helm 3.15.1 is already installed and up-to-date.\nTo reinstall 3.15.1, run:\n  brew reinstall helm\nWarning: Treating docker as a formula. For the cask, use homebrew/cask/docker or specify the `--cask` flag.\nWarning: docker 26.1.3 is already installed and up-to-date.\nTo reinstall 26.1.3, run:\n  brew reinstall docker\nWarning: docker-compose 2.27.1 is already installed and up-to-date.\nTo reinstall 2.27.1, run:\n  brew reinstall docker-compose\nIt's ready, you can proceed to install Airflow\n</code></pre> <ul> <li>create Kubernetes cluster by running below command</li> </ul> <pre><code>make cluster\n</code></pre> <p>Output:</p> <pre><code>\u279c  airflow-k8s git:(master) \u2717 make cluster\nbash scripts/cluster.sh\nyou will create kubernetes cluster and check it ...\nCreating cluster \"airflow-cluster\" ...\n \u2713 Ensuring node image (kindest/node:v1.30.0) \ud83d\uddbc \n \u2713 Preparing nodes \ud83d\udce6 \ud83d\udce6 \ud83d\udce6 \ud83d\udce6  \n \u2713 Writing configuration \ud83d\udcdc \n \u2713 Starting control-plane \ud83d\udd79\ufe0f \n \u2713 Installing CNI \ud83d\udd0c \n \u2713 Installing StorageClass \ud83d\udcbe \n \u2713 Joining worker nodes \ud83d\ude9c \nSet kubectl context to \"kind-airflow-cluster\"\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-airflow-cluster\n\nNot sure what to do next? \ud83d\ude05  Check out https://kind.sigs.k8s.io/docs/user/quick-start/\nCluster info\nKubernetes control plane is running at https://127.0.0.1:50484\nCoreDNS is running at https://127.0.0.1:50484/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\nNodes info\nNAME                            STATUS     ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION    CONTAINER-RUNTIME\nairflow-cluster-control-plane   Ready      control-plane   21s   v1.30.0   172.18.0.5    &lt;none&gt;        Debian GNU/Linux 12 (bookworm)   6.6.26-linuxkit   containerd://1.7.15\nairflow-cluster-worker          NotReady   &lt;none&gt;          1s    v1.30.0   172.18.0.3    &lt;none&gt;        Debian GNU/Linux 12 (bookworm)   6.6.26-linuxkit   containerd://1.7.15\nairflow-cluster-worker2         NotReady   &lt;none&gt;          1s    v1.30.0   172.18.0.4    &lt;none&gt;        Debian GNU/Linux 12 (bookworm)   6.6.26-linuxkit   containerd://1.7.15\nairflow-cluster-worker3         NotReady   &lt;none&gt;          1s    v1.30.0   172.18.0.2    &lt;none&gt;        Debian GNU/Linux 12 (bookworm)   6.6.26-linuxkit   containerd://1.7.15\n</code></pre> <ul> <li>Create a Kubernetes namespace, run below command</li> </ul> <pre><code>make ns\n</code></pre> <p>Output:</p> <pre><code>\u279c  airflow-k8s git:(master) \u2717 make ns     \nbash scripts/namespace.sh\nCreating kubernetes namespace Airflow ...\nnamespace/airflow created\nNAME                 STATUS   AGE\nairflow              Active   0s\ndefault              Active   57m\nkube-node-lease      Active   57m\nkube-public          Active   57m\nkube-system          Active   57m\nlocal-path-storage   Active   57m\n</code></pre> <ul> <li>Fetch airflow chart from Helm repository</li> </ul> <pre><code>make fetch\n</code></pre> <p>Output:</p> <pre><code>\u279c  airflow-k8s git:(master) \u2717 make fetch\nFetching airflow from Helm chart\n\"apache-airflow\" already exists with the same configuration, skipping\nHang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"apache-airflow\" chart repository\nUpdate Complete. \u2388Happy Helming!\u2388\nNAME                    CHART VERSION   APP VERSION     DESCRIPTION                                       \napache-airflow/airflow  1.13.1          2.8.3           The official Helm chart to deploy Apache Airflo...\n</code></pre> <ul> <li>Install airflow Helm chart on Kubernetes cluster, this will take around 5 minutes.</li> </ul> <pre><code>make install\n</code></pre> <p>to check if all the Airflow pods are already running</p> <pre><code>kubectl get pods -n airflow\n</code></pre> <p>Output:</p> <pre><code>NAME                                    READY   STATUS                            RESTARTS   AGE\nairflow-k8-postgresql-0                 1/1     Running                           0          99s\nairflow-k8-redis-0                      1/1     Running                           0          99s\nairflow-k8-scheduler-d85d89786-6dtml    2/2     Running                           0          99s\nairflow-k8-statsd-54b54565c9-xqjgk      1/1     Running                           0          99s\nairflow-k8-triggerer-0                  2/2     Running                           0          99s\nairflow-k8-webserver-688d4cdb74-6949c   1/1     Running                           0          99s\nairflow-k8-worker-0                     2/2     Running                           0          99s\n</code></pre> <p>since you have forwarded the port to <code>localhost</code>, you will be able to see the Airflow UI through <code>http://localhost:8080</code></p> <p></p> <ul> <li>By default, airflow will still run <code>CeleryExecutor</code>, you can change to <code>KubernetesExecutor</code> to run natively on Kubernetes and get more benefits of it.</li> </ul> <pre><code># Run this to get and generate values.yaml file\nhelm show values apache-airflow/airflow &gt; values.yaml\n</code></pre> <p>Open it and change the <code>executor</code> setting</p> <pre><code># Airflow executor\n# One of: LocalExecutor, LocalKubernetesExecutor, CeleryExecutor, KubernetesExecutor, CeleryKubernetesExecutor\nexecutor: \"KubernetesExecutor\"\n</code></pre> <p>Then, you need to check your helm install list</p> <pre><code>\u279c helm list -n airflow\n\nNAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION\nairflow-k8      airflow         1               2024-06-05 11:29:33.499928 +0400 +04    deployed        airflow-1.13.1  2.8.3   \n\n# Run this to apply the updated values.yaml to deploy Airflow\n\u279c helm upgrade --install airflow apache-airflow/airflow -n airflow -f values.yaml --debug\n\n\u279c helm list -n airflow\n\nNAME            NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION\nairflow-k8      airflow         2               2024-06-05 11:32:33.499928 +0400 +04    deployed        airflow-1.13.1  2.8.3   \n</code></pre>"},{"location":"projects/airflow-k8s/#check-if-the-pods-are-running","title":"Check if the pods are running","text":"<pre><code>\u279c  airflow-k8s git:(master) \u2717 kubectl get pods -n airflow\n\nNAME                                    READY   STATUS    RESTARTS   AGE\nairflow-k8-postgresql-0                 1/1     Running   0          19m\nairflow-k8-scheduler-664779678f-wvpbz   2/2     Running   0          2m\nairflow-k8-statsd-54b54565c9-xqjgk      1/1     Running   0          19m\nairflow-k8-triggerer-0                  2/2     Running   0          118s\nairflow-k8-webserver-9b9b95948-gkfkf    1/1     Running   0          2m\n</code></pre>"},{"location":"projects/airflow-k8s/#install-dependencies-on-airflow","title":"Install dependencies on Airflow","text":"<p>In common use cases, we usually want to run <code>spark</code> code with help of Airflow as scheduler.</p> <p>To install dependencies, we can do this as follow:</p> <ul> <li>create a Dockerfile</li> </ul> <pre><code>FROM apache/airflow:2.8.3\nRUN pip install apache-airflow-providers-apache-spark==4.8.1\nCOPY ./dags/ \\${AIRFLOW_HOME}/dags/\n</code></pre> <ul> <li>build the custom docker image and load image to KinD</li> </ul> <pre><code>make load\n</code></pre> <p>Output:</p> <pre><code>Image: \"airflow-base:1.0.0\" with ID \"sha256:5b938a838c583fb2ec534fb347eb6ceae32d76d3241661de5568803aa7e96de3\" not yet present on node \"airflow-cluster-worker\", loading...\nImage: \"airflow-base:1.0.0\" with ID \"sha256:5b938a838c583fb2ec534fb347eb6ceae32d76d3241661de5568803aa7e96de3\" not yet present on node \"airflow-cluster-worker3\", loading...\nImage: \"airflow-base:1.0.0\" with ID \"sha256:5b938a838c583fb2ec534fb347eb6ceae32d76d3241661de5568803aa7e96de3\" not yet present on node \"airflow-cluster-control-plane\", loading...\nImage: \"airflow-base:1.0.0\" with ID \"sha256:5b938a838c583fb2ec534fb347eb6ceae32d76d3241661de5568803aa7e96de3\" not yet present on node \"airflow-cluster-worker2\", loading...\n</code></pre> <ul> <li>update <code>values.yaml</code></li> </ul> <pre><code># Default airflow repository -- overridden by all the specific images below\ndefaultAirflowRepository: airflow-base\n\n# Default airflow tag to deploy\ndefaultAirflowTag: \"1.0.0\"\n</code></pre> <ul> <li>upgrade the helm chart</li> </ul> <pre><code>make upgrade\n</code></pre> <ul> <li>check the airflow provider list</li> </ul> <pre><code>kubectl exec &lt;webserver_pod_id&gt; -n airflow --airflow providers list\n</code></pre> <p>Output:</p> <pre><code>\u279c  airflow-k8s git:(master) \u2717 kubectl exec airflow-k8-webserver-678f64d65b-p55rt -n airflow -- airflow providers list\nDefaulted container \"webserver\" out of: webserver, wait-for-airflow-migrations (init)\npackage_name                             | description                                                                                  | version\n=========================================+==============================================================================================+========\napache-airflow-providers-apache-spark    | Apache Spark https://spark.apache.org/                                                       | 4.8.1  \n</code></pre>"},{"location":"projects/airflow-k8s/#adding-webserver-key","title":"Adding Webserver key","text":"<p>You should set a static webserver secret key when deploying with this chart as it will help ensure your Airflow components only restart when necessary.</p> <ul> <li>run this command to create kubernetes secrets</li> </ul> <pre><code>kubectl apply -f secrets.yaml\n</code></pre> secrets.yaml<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  namespace: airflow\n  name: my-webserver-secret\ntype: Opaque\nstringData:\n  webserverSecretKey: \"$(python3 -c 'import secrets; print(secrets.token_hex(16))')\"\n</code></pre> <ul> <li>update in <code>values.yaml</code></li> </ul> <pre><code># Flask secret key for Airflow Webserver: `[webserver] secret_key` in airflow.cfg\nwebserverSecretKey: my-webserver-secret\nwebserverSecretKeySecretName: ~\n</code></pre> <ul> <li>run this command to apply changes to Helm</li> </ul> <pre><code>make upgrade\n</code></pre>"},{"location":"projects/airflow-k8s/#deploying-dag-to-airflow","title":"Deploying DAG to Airflow","text":"<ul> <li>Developers can put the <code>dag</code> files in <code>dags/</code> directory.</li> <li>Then, run below command to rebuild the image and load to <code>KinD</code></li> </ul> <pre><code>make load\n</code></pre> <ul> <li>apply changes to helm chart</li> </ul> <pre><code>make upgrade\n</code></pre> <ul> <li>you need to keep doing this once you added new DAG files.</li> </ul> <p>Tip</p> <p>There are several approach to manage DAGs files.</p> <p>You can follow it in official documentation Apache Airflow - Manage DAGs files</p>"},{"location":"projects/airflow-k8s/#running-dag","title":"Running DAG","text":"<p>There is new and interesting topic from Airflow, using TaskFlow API</p> tutorial_taskflow_api_etl.py<pre><code>import json\nimport pendulum\n\nfrom airflow.decorators import dag, task\n\n\n@dag(\n    schedule_interval=None,\n    start_date=pendulum.datetime(2021, 1, 1, tz=\"UTC\"),\n    catchup=False,\n    tags=['taskflow-example'],\n)\ndef tutorial_taskflow_api_etl():\n    \"\"\"\n    ### TaskFlow API Tutorial Documentation\n    This is a simple ETL data pipeline example which demonstrates the use of\n    the TaskFlow API using three simple tasks for Extract, Transform, and Load.\n    Documentation that goes along with the Airflow TaskFlow API tutorial is\n    located\n    [here](https://airflow.apache.org/docs/apache-airflow/stable/tutorial_taskflow_api.html)\n    \"\"\"\n\n    @task()\n    def extract():\n        \"\"\"\n        #### Extract task\n        A simple Extract task to get data ready for the rest of the data\n        pipeline. In this case, getting data is simulated by reading from a\n        hardcoded JSON string.\n        \"\"\"\n        data_string = '{\"1001\": 301.27, \"1002\": 433.21, \"1003\": 502.22}'\n\n        order_data_dict = json.loads(data_string)\n        return order_data_dict\n\n    @task(multiple_outputs=True)\n    def transform(order_data_dict: dict):\n        \"\"\"\n        #### Transform task\n        A simple Transform task which takes in the collection of order data and\n        computes the total order value.\n        \"\"\"\n        total_order_value = 0\n\n        for value in order_data_dict.values():\n            total_order_value += value\n\n        return {\"total_order_value\": total_order_value}\n\n    @task()\n    def load(total_order_value: float):\n        \"\"\"\n        #### Load task\n        A simple Load task which takes in the result of the Transform task and\n        instead of saving it to end user review, just prints it out.\n        \"\"\"\n\n        print(f\"Total order value is: {total_order_value:.2f}\")\n\n    order_data = extract()\n    order_summary = transform(order_data)\n    load(order_summary[\"total_order_value\"])\n\ntutorial_etl_dag = tutorial_taskflow_api_etl()\n</code></pre> <p>It will be rendered as below through Airflow UI</p> <p></p> <ul> <li>click <code>Trigger DAG</code> (Play button in top right pane) to execute it right away</li> </ul> <p>Some key takeaways from TaskFlow API:</p> <ul> <li>you just need to use <code>@dag</code> to instantiate a DAG.</li> <li>only use decorator <code>@task</code> to create airflow task. And, you can interchange the values among tasks, just as easy as Python variables.</li> </ul> <p>Tip</p> <p>more about the complete documentation, you can check on Tutorial on TaskFlow API</p>"},{"location":"projects/airflow-k8s/#enabling-logs","title":"Enabling logs","text":"<p>With the <code>KubernetesExecutor</code>, Airflow creates a new Pod each time a task runs. Once the task is completed, Airflow deletes the Pod and so the logs.</p> <p>Therefore, you need a way to store your logs somewhere so that you can still access them. For local development, the easiest way is to configure a HostPath PV.</p> <ul> <li>create <code>pv.yaml</code> for the PersistentVolume</li> </ul> pv.yaml<pre><code>kind: PersistentVolume\napiVersion: v1\nmetadata:\n  name: testlog-volume\nspec:\n  accessModes:\n    - ReadWriteMany\n  capacity:\n    storage: 2Gi\n  hostPath:\n    path: /opt/airflow/logs/\n  storageClassName: standard\n</code></pre> <ul> <li>create <code>pvc.yaml</code> for the PersistentVolumeClaim</li> </ul> pvc.yaml<pre><code>kind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: testlog-volume\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 2Gi\n  storageClassName: standard\n</code></pre> <ul> <li>run this command to apply both files</li> </ul> <pre><code>kubectl apply -f pv.yaml &amp;&amp; \\\nkubectl apply -f pvc.yaml\n</code></pre> <ul> <li>update <code>values.yaml</code></li> </ul> <pre><code>logs:\n  persistence:\n    # Enable persistent volume for storing logs\n    enabled: true\n    # Volume size for logs\n    size: 100Gi\n    # Annotations for the logs PVC\n    annotations: {}\n    # If using a custom storageClass, pass name here\n    storageClassName:\n    ## the name of an existing PVC to use\n    existingClaim: testlog-volume\n</code></pre> <ul> <li>run command to upgrade helm chart</li> </ul> <pre><code>make upgrade\n</code></pre>"},{"location":"projects/airflow-k8s/#cleaning-up-resources","title":"Cleaning up resources","text":"<p>Make sure to clean up used resources, it's quite taking resource in local.</p> <pre><code>make clean\n</code></pre>"},{"location":"projects/airflow-k8s/#references","title":"References","text":"<ul> <li> <p>Airflow - Kubernetes Executor</p> </li> <li> <p>KinD</p> </li> <li> <p>Helm Chart</p> </li> <li> <p>Kubectl: Kubernetes CLI</p> </li> <li> <p>Airflow on Kubernetes by Marc Lamberti</p> </li> </ul>"},{"location":"projects/dbt-bq/","title":"Running dbt and BigQuery using Docker","text":"<p>must have</p> <p>Before starting this project, you need to install some prerequisites in your laptop:</p> <ul> <li>Docker</li> <li>gcloud CLI</li> <li>GCP console</li> </ul>"},{"location":"projects/dbt-bq/#interact-with-google-cloud-platform","title":"Interact with Google Cloud Platform","text":"<ul> <li> <p>Create your project, follow this guidances.</p> </li> <li> <p>Enter your GCP console and go to BigQuery.   You will find view as below, your [project name] is within the red box.</p> </li> </ul> <p></p> <ul> <li> <p>In your GCP project, create dataset name <code>data_eng</code>.</p> </li> <li> <p>Go to your local terminal and run command below, it will redirect you to browser and login to your google account and create the credentials file to your laptop.</p> </li> </ul> <p>Credentials saved to file: [~/.config/gcloud/application_default_credentials.json]</p> <pre><code>gcloud auth application-default login\n</code></pre> <ul> <li>Done, your laptop is connected to GCP and you can connect it through container as well.</li> </ul>"},{"location":"projects/dbt-bq/#build-dbt-docker-image","title":"Build dbt docker image","text":"<ul> <li>clone the repo first, run this command</li> </ul> <pre><code>git clone git@github.com:karlchris/dbt-bigquery.git\n</code></pre> <ul> <li>update your dbt <code>profiles.yml</code> with your GCP [project name]</li> </ul> <pre><code>data-eng:\n  target: dev\n\n  outputs:\n    dev:\n      type: bigquery\n      method: oauth\n      project: [project name]\n      dataset: \"data_eng\"\n      timeout_seconds: 300\n      location: US\n      priority: interactive\n      retries: 0\n      threads: 8\n</code></pre> <ul> <li>make sure you are in <code>data-engineering directory</code>, then run this command</li> </ul> <pre><code>make build-dbt\n</code></pre> <pre><code>\u279c  data-engineering git:(dq1) \u2717 make build-dbt\nBuilding dbt image\n[+] Building 1.1s (9/9) FINISHED                                 docker:desktop-linux\n =&gt; [internal] load build definition from Dockerfile                             0.0s\n =&gt; =&gt; transferring dockerfile: 264B                                             0.0s\n =&gt; [internal] load metadata for docker.io/fishtownanalytics/dbt:1.0.0           1.0s\n =&gt; [internal] load .dockerignore                                                0.0s\n =&gt; =&gt; transferring context: 2B                                                  0.0s\n =&gt; [1/4] FROM docker.io/fishtownanalytics/dbt:1.0.0@sha256:4c9462867d2db6869a0  0.0s\n =&gt; [internal] load build context                                                0.0s\n =&gt; =&gt; transferring context: 1.76kB                                              0.0s\n =&gt; CACHED [2/4] WORKDIR /dbt                                                    0.0s\n =&gt; [3/4] COPY . /dbt/                                                           0.0s\n =&gt; [4/4] COPY profiles /root/.dbt                                               0.0s\n =&gt; exporting to image                                                           0.0s\n =&gt; =&gt; exporting layers                                                          0.0s\n =&gt; =&gt; writing image sha256:03631678cdc560b844f608519ac23cb15d37cab9099feb52075  0.0s\n =&gt; =&gt; naming to docker.io/library/dbt                                           0.0s\n</code></pre> <ul> <li>if you see above image, then your dbt image is already built.</li> </ul>"},{"location":"projects/dbt-bq/#run-dbt-image-container-interactively","title":"Run dbt image container interactively","text":"<ul> <li>run this command to execute dbt image and enter in <code>/bin/bash</code> terminal,</li> </ul> <pre><code>make run-dbt\n</code></pre> <p>Output:</p> <pre><code>\u279c  data-engineering git:(dq1) \u2717 make run-dbt\nRunning dbt in container\nWARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\nroot@d94bac5d04ab:/dbt#\n</code></pre> <ul> <li>run this command to execute <code>dbt run</code> in FULL REFRESH mode.</li> </ul> <pre><code>dbt run --full-refresh\n</code></pre> <p>Output:</p> <pre><code>root@d94bac5d04ab:/dbt# dbt run --full-refresh\n17:25:54  Running with dbt=1.0.0\n17:25:54  Partial parse save file not found. Starting full parse.\n17:25:54  [WARNING]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.\nThere are 1 unused configuration paths:\n- models.data-eng.example\n\n17:25:54  Found 5 models, 2 tests, 0 snapshots, 0 analyses, 189 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics\n17:25:54\n17:25:56  Concurrency: 8 threads (target='dev')\n17:25:56\n17:25:56  1 of 4 START table model data_eng.table_sales................................... [RUN]\n17:26:00  1 of 4 OK created table model data_eng.table_sales.............................. [CREATE TABLE (3.0 rows, 0 processed) in 3.58s]\n17:26:00  2 of 4 START incremental model data_eng.incremental_report...................... [RUN]\n17:26:00  3 of 4 START table model data_eng.table_report.................................. [RUN]\n17:26:00  4 of 4 START view model data_eng.view_apple_sales............................... [RUN]\n17:26:02  4 of 4 OK created view model data_eng.view_apple_sales.......................... [OK in 1.84s]\n17:26:03  2 of 4 OK created incremental model data_eng.incremental_report................. [CREATE TABLE (1.0 rows, 93.0 Bytes processed) in 3.26s]\n17:26:04  3 of 4 OK created table model data_eng.table_report............................. [CREATE TABLE (1.0 rows, 93.0 Bytes processed) in 3.78s]\n17:26:04\n17:26:04  Finished running 2 table models, 1 view model, 1 incremental model in 9.47s.\n17:26:04\n17:26:04  Completed successfully\n17:26:04\n17:26:04  Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4\n</code></pre> <ul> <li> <p>your models have been run based on the dependencies and SQL models.</p> </li> <li> <p>you can check in the BigQuery UI for the created tables.</p> </li> </ul> <p></p>"},{"location":"projects/dbt-bq/#check-dbt-docs-and-dag","title":"Check dbt docs and DAG","text":"<ul> <li>Inside your container, in your terminal, run this command to compile dbt models, etc</li> </ul> <pre><code>dbt compile --target dev\n</code></pre> <p>Output:</p> <pre><code>root@d94bac5d04ab:/dbt# dbt compile --target dev\n17:27:26  Running with dbt=1.0.0\n17:27:26  Unable to do partial parsing because config vars, config profile, or config target have changed\n17:27:27  [WARNING]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.\nThere are 1 unused configuration paths:\n- models.data-eng.example\n\n17:27:27  Found 5 models, 2 tests, 0 snapshots, 0 analyses, 189 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics\n17:27:27\n17:27:28  Concurrency: 8 threads (target='dev')\n17:27:28\n17:27:28  Done.\n</code></pre> <ul> <li>run this command to generate docs.</li> </ul> <pre><code>dbt docs generate --target dev\n</code></pre> <p>Output:</p> <pre><code>root@d94bac5d04ab:/dbt# dbt docs generate --target dev\n17:27:48  Running with dbt=1.0.0\n17:27:48  [WARNING]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.\nThere are 1 unused configuration paths:\n- models.data-eng.example\n\n17:27:48  Found 5 models, 2 tests, 0 snapshots, 0 analyses, 189 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics\n17:27:48\n17:27:49  Concurrency: 8 threads (target='dev')\n17:27:49\n17:27:49  Done.\n17:27:49  Building catalog\n17:27:53  Catalog written to /dbt/target/catalog.json\n</code></pre> <ul> <li>run this command to serve the docs into UI.</li> </ul> <pre><code>dbt docs serve --target dev\n</code></pre> <p>Output:</p> <pre><code>root@d94bac5d04ab:/dbt# dbt docs serve --target dev\n17:28:13  Running with dbt=1.0.0\n17:28:13  Serving docs at 0.0.0.0:8080\n17:28:13  To access from your browser, navigate to:  http://localhost:8080\n17:28:13\n17:28:13\n17:28:13  Press Ctrl+C to exit.\n</code></pre> <ul> <li>the UI is accessible in port <code>8080</code> inside the container, but in my Makefile, I redirect it to port <code>8082</code>, then you can access it in http://localhost:8082 through your browser.</li> </ul> <pre><code>run-dbt:\n @echo \"Running dbt in container\"\n @docker run \\\n  -e DESTINATION=${USER} \\\n  -e DBT_ENV=dev \\\n  --rm \\\n  -v ${GCP_AUTH}:/creds -it \\\n  --entrypoint /bin/bash \\\n  -p 8082:8080 \\\n  dbt\n</code></pre> <ul> <li>you can see the DAG lineage through the URL as below</li> </ul> <p></p> <p>and you can also see each of dbt models informations, such as: details, descriptions, column and code.</p> <p></p>"},{"location":"projects/dbt-bq/#testing-data-quality","title":"Testing data quality","text":"<ul> <li>to perform data quality check on dbt, written in <code>schema.yml</code></li> </ul> <pre><code>version: 2\n\nmodels:\n  - name: table_sales\n    columns:\n      - name: quantity\n        tests:\n          - not_null\n      - name: product\n        tests:\n          - accepted_values:\n              values: [\"apple\", \"pear\", \"banana\"]\n</code></pre> <p>run this</p> <pre><code>dbt test\n</code></pre> <p>Output:</p> <pre><code>root@167804cf135d:/dbt# dbt test\n05:11:38  Running with dbt=1.0.0\n05:11:38  Partial parse save file not found. Starting full parse.\n05:11:39  [WARNING]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.\nThere are 1 unused configuration paths:\n- models.data-eng.example\n\n05:11:39  Found 5 models, 2 tests, 0 snapshots, 0 analyses, 189 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics\n05:11:39\n05:11:40  Concurrency: 8 threads (target='dev')\n05:11:40\n05:11:40  1 of 2 START test accepted_values_table_sales_product__apple__pear__banana...... [RUN]\n05:11:40  2 of 2 START test not_null_table_sales_quantity................................. [RUN]\n05:11:42  2 of 2 PASS not_null_table_sales_quantity....................................... [PASS in 1.75s]\n05:11:42  1 of 2 PASS accepted_values_table_sales_product__apple__pear__banana............ [PASS in 1.96s]\n05:11:42\n05:11:42  Finished running 2 tests in 3.21s.\n05:11:42\n05:11:42  Completed successfully\n05:11:42\n05:11:42  Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2\n</code></pre>"},{"location":"projects/delta-spark/","title":"Delta Lake with Apache Spark","text":"<p>After years of data management, data warehouses reigned supreme with their structured storage and optimized querying.</p> <p>However, these warehouses struggled when confronted with the deluge of unstructured and semi-structured data, revealing their limitations. This void led to the emergence of data lakes, offering a promising solution by accommodating diverse data types without immediate structure.</p> <p>Yet, the initial euphoria surrounding data lakes gave way to challenges of maintaining data integrity, ensuring consistency, and handling updates and deletions effectively.</p> <p>Success</p> <p>Enter Delta Lake, a technological evolution that seeks to address the shortcomings of traditional data warehouses and data lakes alike.</p>"},{"location":"projects/delta-spark/#what-is-delta-lake","title":"What is Delta Lake","text":"<p>Delta Lake is an open-source table format for data storage. Delta Lake improves data storage by supporting ACID transactions, high-performance query optimizations, schema enforcement and evolution, data versioning and many other features.</p> <p>A Delta table consists of 2 main components:</p> <ul> <li>Parquet files that contain the data, and</li> <li>a transaction log that stores metadata about the transactions</li> </ul>"},{"location":"projects/delta-spark/#lakehouse-architecture","title":"Lakehouse Architecture","text":"<p>A lakehouse is a new, open architecture that combines the best elements of data lakes and data warehouses.</p> <p>Lakehouses are enabled by a new system design: implementing similar data structures and data management features to those in a data warehouse directly on top of low cost cloud storage in open formats.</p> <p></p>"},{"location":"projects/delta-spark/#lakehouse-features","title":"Lakehouse Features","text":"<ul> <li>Transaction support: many data pipelines will often be reading and writing data concurrently.</li> <li>Schema enforcement and governance: The Lakehouse should have a way to support the schema enforcement and evolution, supporting DW schema such as star/snowflake-schemas.</li> <li>BI support: Lakehouses enable using BI tools directly on the source data.</li> <li>Storage is decoupled from compute: In practice, this means storage and compute use separate clusters, thus these systems are able to scale to many more concurrent users and larger data sizes.</li> <li>Openness: The storage format they use are open and standardized, such as Parquet, and they provide an API so a variety of tools/engines can efficiently access the data.</li> <li>Support for diverse data types ranging from unstructured and structured data</li> <li>Support for diverse workloads: including data science, machine learning, and SQL and analytics.</li> <li>End-to-end streaming: Real-time reports are the norm in many enterprises. Support for streaming eliminates the need for separate systems dedicated to serving real-time data applications.</li> </ul>"},{"location":"projects/delta-spark/#contents-of-delta-table","title":"Contents of Delta table","text":"<p>The transaction log enables Delta Lake to optimize the queries, ensure reliable reads and writes, and to store a record of all data transformations.</p> <p>Delta Lake supports both ETL and ELT workloads.</p>"},{"location":"projects/delta-spark/#benefits-for-etl","title":"Benefits for ETL","text":"<p>Delta Lake is great for ETL because of its performance and reliability.</p> <p>Here are some features that will improve ETL workloads:</p>"},{"location":"projects/delta-spark/#query-optimization","title":"Query Optimization","text":"<p>Delta Lake makes data transformations faster by:</p> <ul> <li>storing file paths and metadata in the transaction log</li> <li>executing partial reads via file-skipping</li> <li>co-locating similar data to allow for better file skipping</li> </ul> <p>Tips</p> <p>Transaction logs</p> <p>Delta Lake stores all file paths, metadata and data transformation operations in a dedicated transaction log. This makes file listing faster and enables partial data reads.</p> <p>File-skipping</p> <p>Delta Lake stores metadata at the file-level in a single transaction log. This way query engines can figure out which data can be skipped using a single network request. Entire files can be skipped this way.</p> <p>Co-locating similar data</p> <p>Delta Lake can store similar data close together to improve your query performance, using Z-Ordering or Liquid Clustering.</p>"},{"location":"projects/delta-spark/#reliability","title":"Reliability","text":"<p>Delta Lake makes your ETL workloads more reliable by enforcing ACID transactions. Transactions prevent your data from being corrupted or lost.</p> <p>Example</p> <p>Data storage formats without transaction support (like CSV or Parquet) can easily be corrupted. </p> <p>For example, if you're writing a large amount of data and your cluster dies then you'll have several partially written files in your table. These partial files will cause downstream read operations to crash.</p> <p>Delta Lake transactions give you 4 important guarantess:</p> <ul> <li>No more failed partial writes: Every write operation either completes entirely or else it fails entirely and no data gets changed.</li> <li>No more corrupted tables: If a transaction is going to break any of the pre-defined constraints, the entire transaction is rejected and will not complete.</li> <li>No more conflicting data versions: Concurrent processes happen in isolation from each other and cannot access each other's intermediate states.</li> <li>No more unintended data loss: All data changes are guaranteed to never be lost, even in the events of system failures or power outages.</li> </ul>"},{"location":"projects/delta-spark/#schema-enformement-and-evolution","title":"Schema Enformement and Evolution","text":"<p>To prevent accidental data corruption, Delta Lake provides schema enforcement.</p> <p>You cannot write new data to a Delta table if it doesn't match the existing table's schema. It will error out with an <code>AnalysisException</code>.</p> <p>For example, let's create a Delta table with a simple schema:</p> <pre><code>df = spark.createDataFrame([(\"bob\", 47), (\"li\", 23), (\"leonard\", 51)]).toDF(\n    \"first_name\", \"age\"\n)\n\ndf.write.format(\"delta\").save(\"data/toy_data\")\n</code></pre> <p>Now, let's try to write data with a different schema to this same Delta table:</p> <pre><code>df = spark.createDataFrame([(\"frank\", 68, \"usa\"), (\"jordana\", 26, \"brasil\")]).toDF(\n    \"first_name\", \"age\", \"country\"\n)\n\ndf.write.format(\"delta\").mode(\"append\").save(\"data/toy_data\")\n</code></pre> <p>Here's the complete error:</p> <pre><code>AnalysisException: [_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table\n\nTable schema:\nroot\n-- first_name: string (nullable = true)\n-- age: long (nullable = true)\n\nData schema:\nroot\n-- first_name: string (nullable = true)\n-- age: long (nullable = true)\n-- country: string (nullable = true)\n</code></pre> <p>Warning</p> <p>Delta Lake does not allow you to append data with mismatched schema by default.</p>"},{"location":"projects/delta-spark/#schema-evolution","title":"Schema Evolution","text":"<p>Of course, ETL workloads evolve over time. Input data may change or your downstream analysis might need a new column. When you need more flexibility in your schema, Delta Lake also supports Schema Evolution.</p> <p>To update the schema of your Delta table, you can write data with the <code>mergeSchema</code> option. Let's try this for the example that we just saw above:</p> <pre><code>df.write.option(\"mergeSchema\", \"true\").mode(\"append\").format(\"delta\").save(\n    \"data/toy_data\"\n)\n</code></pre> <p>Here are the contents of your Delta table after the write:</p> <pre><code>spark.read.format(\"delta\").load(\"data/toy_data\").show()\n\n+----------+---+-------+\n|first_name|age|country|\n+----------+---+-------+\n|   jordana| 26| brasil| # new\n|     frank| 68|    usa| # new\n|   leonard| 51|   null|\n|       bob| 47|   null|\n|        li| 23|   null|\n+----------+---+-------+\n</code></pre> <p>The Delta table now has three columns. It previously only had two columns. Rows that don't have any data for the new column will be marked as null when new columns are added.</p>"},{"location":"projects/delta-spark/#time-travel","title":"Time Travel","text":"<p>Nobody is perfect. And no ETL pipeline is perfect, either.</p> <p>When mistakes happen, you want to be able to roll back your data to a previous version before the mistake. Doing this manually is painful and takes a lot of time. Delta Lake makes this easy by supporting time travel functionality.</p> <p>Because all of the transactions are stored in the transaction log, Delta Lake can always travel back to earlier states of your table.</p>"},{"location":"projects/delta-spark/#scalability","title":"Scalability","text":"<p>ETL workloads often scale up as more data becomes available. Delta Lake supports both small and very large data workloads.</p> <p>Delta Lake makes it easier and faster to process large workloads by:</p>"},{"location":"projects/delta-spark/#partitioning","title":"Partitioning","text":"<p>File partitioning makes it faster to work with data at scale.</p> <p>In the Query Optimization example we saw above, we used a smart partitioning strategy to make our query run faster. When you partition a table on a certain column, Delta Lake stores all records with the same column value in the same file. This way it can skip entire files when certain column values are not needed.</p> <p>Partitioning is especially efficient with parallel query engines. In this case, each process (or \u201cworker\u201d) can read its own partitions in parallel. This speeds up your queries and lets you process more data in less time.</p>"},{"location":"projects/delta-spark/#data-clustering","title":"Data Clustering","text":"<p>Delta Lake lets you store similar data close together via Liquid Clustering, Z-ordering and Hive-style partitioning. Liquid Clustering is the newest and most performant technique of the three.</p> <p>Your ETL workload will likely benefit from clustering if:</p> <ul> <li>You often need to filter by high cardinality columns.</li> <li>Your data has significant skew in data distribution.</li> <li>Your tables grow quickly and require maintenance and tuning effort.</li> <li>Your data access patterns change over time.</li> </ul>"},{"location":"projects/delta-spark/#query-engine-support","title":"Query Engine Support","text":"<p>Delta Lake makes it easy to work with lots of different query engines.</p> <p>You might start working locally with a single-node processing library like polars:</p> <pre><code># load data\ndf = pl.DataFrame(data)\n\n# perform some data operations\n...\n\n# write to delta table\ndf.write_delta(\"data/delta_table\")\n</code></pre> <p>When your data volume increases, you can switch to a distributed query engine like Spark:</p> <pre><code># load delta table created with polars\ndf = spark.read.format(\"delta\").load(\"data/delta_table\")\n\n# join to much larger dataset\nbig_df = df.join(new_df, \u2026)\n\n# run big data operations\n\u2026\n\n# write to delta table\nbig_df.write.format(\"delta\").mode(\"append\").option(\"overwriteSchema\", \"True\").save(\"data/delta_table\")\n</code></pre> <p>Info</p> <p>Delta Lake has great interoperability with many query engines.</p>"},{"location":"projects/delta-spark/#optimizing-delta-lake","title":"Optimizing Delta Lake","text":"<p>How to optimize your Delta Lake table to reduce the number of small files.</p> <p>Small files can be a problem because they slow down your query reads. Listing, opening and closing many small files incurs expensive overhead. This is called \"the Small File Problem\". You can reduce the Small File Problem overhead by combining the data into bigger, more efficient files.</p> <p>The code below runs a query on a Delta table with 2 million rows. The Delta table is partitioned on the <code>education</code> column and has 1440 files per partition:</p> <pre><code>%%time\ndf = spark.read.format(\"delta\").load(\"test/delta_table_1440\")\nres = df.where(df.education == \"10th\").collect()\n\nCPU times: user 175 ms, sys: 20.1 ms, total: 195 ms\nWall time: 16.1 s\n</code></pre> <p>Now compare this to the same query on the same 2M-rows of data stored in a Delta table with only 1 optimized file per partition:</p> <pre><code>%%time\ndf = spark.read.format(\"delta\").load(\"test/delta_table_1\")\nres = df.where(df.education == \"10th\").collect()\n\nCPU times: user 156 ms, sys: 16 ms, total: 172 ms\nWall time: 4.62 s\n</code></pre> <p>Storing data in an optimized number of files will improve your out-of-the-box read performance.</p> <p>There are 3 ways you can optimize your Delta Lake table:</p>"},{"location":"projects/delta-spark/#offline-optimize","title":"Offline Optimize","text":"<p>Suppose you have an ETL pipeline that ingests some data every day. The data is streamed into a partitioned Delta table that gets updated every minute.</p> <p>This means you will end up with 1440 files per partition at the end of every day.</p> <pre><code>&gt; # get n files per partition\n&gt; !ls test/delta_table/education\\=10th/*.parquet | wc -l\n1440\n</code></pre> <p>Running a query on a Delta table with many small files is not efficient, as we saw above.</p> <p>You can manually run the Delta OPTIMIZE command to optimize the number of files. This is done by compacting all the inefficient small files into larger files that are more efficient to read. The default file size written in each partition after this operation is 1 GB.</p> <p>You can perform a compaction manually using:</p> <pre><code>from delta.tables import *\ndeltaTable = DeltaTable.forPath(spark, \"test/delta_table\")\ndeltaTable.optimize().executeCompaction()\n</code></pre> <p>Downstream queries on this optimized Delta table will now be faster.</p>"},{"location":"projects/delta-spark/#optimized-write","title":"Optimized Write","text":"<p>You don\u2019t have to manually run the OPTIMIZE command. You can also configure optimizations to run automatically on your Delta Table.</p> <p></p> <p>Optimized Write combines all the small writes to the same partition into a single write command before executing. This is great when multiple processes are writing to the same partitioned Delta table, i.e. a distributed write operation.</p> <p>Optimized Write rebalances the data using a data shuffle before writing the files to the table. This way you will reduce the number of small files.</p> <p>You can enable Optimized Write by setting the optimizeWrite option in your Delta Lake writer:</p> <pre><code>df.write.format(\"delta\").option(\"optimizeWrite\", \"True\").save(\"path/to/delta\")\n</code></pre> <p>You can also enable the Optimized Write functionality:</p> <ul> <li>for your whole Delta table by setting the <code>delta.autoOptimize.optimizeWrite</code> table property.</li> <li>for your whole Spark SQL session by setting the <code>spark.databricks.delta.optimizeWrite.enabled</code> SQL configuration</li> </ul> <p>Warning</p> <p>Optimized Writes take a bit longer to execute because of the data shuffle that is performed before the data gets written. That\u2019s why the feature is not enabled by default.</p>"},{"location":"projects/delta-spark/#auto-compaction","title":"Auto Compaction","text":"<p>Optimized Write is great for distributed write situations with many different processes writing to the same Delta table.</p> <p>But sometimes that is not enough to solve the Small File Problem, for example when you are writing frequent small updates to a table. In this case, the files will still be small, even after an Optimized Write.</p> <p>Auto Compaction solves this problem by automatically running a small <code>optimize</code> command after every write operation. Data from files under a certain threshold size is automatically combined into a larger file. This way, your downstream queries can benefit from a more optimal file size.</p> <p>You can enable Auto Compaction for your Delta table or your entire Spark session:</p> <ul> <li>Table property: <code>delta.autoOptimize.autoCompact</code></li> <li>SparkSession setting: <code>spark.databricks.delta.autoCompact.enabled</code></li> </ul> <p>Auto Compaction is only triggered for partitions or tables that have at least a certain number of small files. The minimum number of files required to trigger auto compaction can be configured with <code>spark.databricks.delta.autoCompact</code>.minNumFiles.</p>"},{"location":"projects/delta-spark/#optimize-vacuum","title":"Optimize: Vacuum","text":"<p>Because Auto Compaction optimizes your Delta Table after the write operations, you may still have many small files on disk.</p> <pre><code>&gt; # get n files per partition\n&gt; !ls delta/census_table_compact/education\\=10th/*.parquet | wc -l\n\n1441\n</code></pre> <p>In this case we have 1440 files (one per partition) and 1 final file that contains all the data.</p> <p>Delta Lake has iteratively combined all the small writes into a larger file. It has also recorded in the transaction log the path to the latest file. Downstream data reads will look at the transaction log and access only the last, largest file.</p> <p>But as you can see the older, smaller files are still on disk. This doesn\u2019t affect your read performance because Delta Lake knows that you only need to access the latest file. But you might still want to remove these files, for example to save on storage costs.</p> <p>You can remove these older files with a <code>VACUUM</code> command. The parameter is the number of preceding hours you want to preserve.</p> <pre><code>deltaTable.vacuum(0)\n</code></pre> <p>The <code>VACUUM</code> command removes old files that are no longer actively referenced in the transaction log. By default, <code>VACUUM</code> only affects files older than the default retention duration which is 7 days.</p> <p>You can override this default setting using:</p> <pre><code>spark.sql(\"SET spark.databricks.delta.retentionDurationCheck.enabled=false\")\n</code></pre>"},{"location":"projects/delta-spark/#implementation","title":"Implementation","text":"<p>Here we go to start the implementation of how delta table is being executed and let's get a quick look on delta CDC operations.</p> <p>First of all, you need to clone this repo</p> <pre><code>git clone git@github.com:karlchris/spark-docker.git\n</code></pre>"},{"location":"projects/delta-spark/#installing-delta-package","title":"Installing Delta Package","text":"<p>You need to install some delta jars into Dockerfile</p> <pre><code># Download delta jars\nRUN curl https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.4.0/delta-core_2.12-2.4.0.jar -Lo /opt/spark/jars/delta-core_2.12-2.4.0.jar\nRUN curl https://repo1.maven.org/maven2/io/delta/delta-spark_2.12/3.2.0/delta-spark_2.12-3.2.0.jar -Lo /opt/spark/jars/delta-spark_2.12-3.2.0.jar\nRUN curl https://repo1.maven.org/maven2/io/delta/delta-storage/3.2.0/delta-storage-3.2.0.jar -Lo /opt/spark/jars/delta-storage-3.2.0.jar\n</code></pre> <p>In the python script itself, you need to set the configurations.</p> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark import SparkConf\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\nfrom delta import *\nfrom delta.tables import *\nfrom pyspark.sql.functions import expr, lit, col\n\njars = 'io.delta:delta-spark_2.12:3.2.0,io.delta:delta-core_2.12:2.4.0,io.delta:delta-storage:3.2.0'\n\n# Setup config\nconf = SparkConf().setAppName(\"deltaDemo\") \\\n    .set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .set('spark.jars.packages', jars)\n\n# Create spark session\nspark = SparkSession.builder.config(conf=conf).getOrCreate()\nspark.sparkContext.setLogLevel(\"ERROR\")\n</code></pre> <ul> <li>You will need to build the docker compose, by running below command:</li> </ul> <pre><code>make up\n</code></pre> <p>Output:</p> <pre><code>(base) \u279c  spark-docker git:(main) \u2717 make up \ndocker compose up -d --build\n[+] Building 2.7s (21/21) FINISHED                                                                                                                                                                                                                                                                 docker:desktop-linux\n =&gt; [spark-master internal] load build definition from Dockerfile                                                                                                                                                                                                                                                  0.0s\n =&gt; =&gt; transferring dockerfile: 2.38kB                                                                                                                                                                                                                                                                             0.0s\n =&gt; [spark-master internal] load metadata for docker.io/library/python:3.10-bullseye                                                                                                                                                                                                                               2.5s\n =&gt; [spark-master auth] library/python:pull token for registry-1.docker.io                                                                                                                                                                                                                                         0.0s\n =&gt; [spark-master internal] load .dockerignore                                                                                                                                                                                                                                                                     0.0s\n =&gt; =&gt; transferring context: 2B                                                                                                                                                                                                                                                                                    0.0s\n =&gt; [spark-master spark-base 1/5] FROM docker.io/library/python:3.10-bullseye@sha256:a894d9de36c807c43b543bce5e52702b8b22de985ca087f3478a4a4b18c86d37                                                                                                                                                              0.0s\n =&gt; =&gt; resolve docker.io/library/python:3.10-bullseye@sha256:a894d9de36c807c43b543bce5e52702b8b22de985ca087f3478a4a4b18c86d37                                                                                                                                                                                      0.0s\n =&gt; [spark-master internal] load build context                                                                                                                                                                                                                                                                     0.0s\n =&gt; =&gt; transferring context: 1.00kB                                                                                                                                                                                                                                                                                0.0s\n =&gt; CACHED [spark-master spark-base 2/5] RUN apt-get update &amp;&amp;     apt-get install -y --no-install-recommends       sudo       curl       vim       unzip       rsync       openjdk-11-jdk       build-essential       software-properties-common       ssh &amp;&amp;     apt-get clean &amp;&amp;     rm -rf /var/lib/apt/lists  0.0s\n =&gt; CACHED [spark-master spark-base 3/5] RUN mkdir -p /opt/hadoop &amp;&amp; mkdir -p /opt/spark                                                                                                                                                                                                                           0.0s\n =&gt; CACHED [spark-master spark-base 4/5] WORKDIR /opt/spark                                                                                                                                                                                                                                                        0.0s\n =&gt; CACHED [spark-master spark-base 5/5] RUN mkdir -p /opt/spark     &amp;&amp; curl https://dlcdn.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz -o spark-3.5.1-bin-hadoop3.tgz     &amp;&amp; tar xvzf spark-3.5.1-bin-hadoop3.tgz --directory /opt/spark --strip-components 1     &amp;&amp; rm -rf spark-3.5.1-bin-hadoop3.  0.0s\n =&gt; CACHED [spark-master pyspark 1/4] COPY requirements.txt .                                                                                                                                                                                                                                                      0.0s\n =&gt; CACHED [spark-master pyspark 2/4] RUN pip3 install -r requirements.txt                                                                                                                                                                                                                                         0.0s\n =&gt; CACHED [spark-master pyspark 3/4] COPY conf/spark-defaults.conf /opt/spark/conf                                                                                                                                                                                                                                0.0s\n =&gt; CACHED [spark-master pyspark 4/4] RUN chmod u+x /opt/spark/sbin/* &amp;&amp;     chmod u+x /opt/spark/bin/*                                                                                                                                                                                                            0.0s\n =&gt; CACHED [spark-master stage-2 1/6] RUN curl https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.4_2.12/1.4.3/iceberg-spark-runtime-3.4_2.12-1.4.3.jar -Lo /opt/spark/jars/iceberg-spark-runtime-3.4_2.12-1.4.3.jar                                                                        0.0s\n =&gt; CACHED [spark-master stage-2 2/6] RUN curl https://repo1.maven.org/maven2/io/delta/delta-core_2.12/2.4.0/delta-core_2.12-2.4.0.jar -Lo /opt/spark/jars/delta-core_2.12-2.4.0.jar                                                                                                                               0.0s\n =&gt; CACHED [spark-master stage-2 3/6] RUN curl https://repo1.maven.org/maven2/io/delta/delta-spark_2.12/3.2.0/delta-spark_2.12-3.2.0.jar -Lo /opt/spark/jars/delta-spark_2.12-3.2.0.jar                                                                                                                            0.0s\n =&gt; CACHED [spark-master stage-2 4/6] RUN curl https://repo1.maven.org/maven2/io/delta/delta-storage/3.2.0/delta-storage-3.2.0.jar -Lo /opt/spark/jars/delta-storage-3.2.0.jar                                                                                                                                     0.0s\n =&gt; [spark-master stage-2 5/6] COPY entrypoint.sh .                                                                                                                                                                                                                                                                0.0s\n =&gt; [spark-master stage-2 6/6] RUN chmod u+x /opt/spark/entrypoint.sh                                                                                                                                                                                                                                              0.1s\n =&gt; [spark-master] exporting to image                                                                                                                                                                                                                                                                              0.0s\n =&gt; =&gt; exporting layers                                                                                                                                                                                                                                                                                            0.0s\n =&gt; =&gt; writing image sha256:e9e8d30b8debaa1194efcb691c192c7bf21c7ac7ee32bad255d0a91440fee562                                                                                                                                                                                                                       0.0s\n =&gt; =&gt; naming to docker.io/library/spark-image                                                                                                                                                                                                                                                                     0.0s\n[+] Running 3/3\n \u2714 Container spark-master   Started                                                                                                                                                                                                                                                                               20.5s \n \u2714 Container spark-history  Started                                                                                                                                                                                                                                                                               10.5s \n \u2714 Container spark-worker   Started    \n</code></pre> <ul> <li>Then, you need to enter the container interactive terminal, by running below command:</li> </ul> <pre><code>make dev\n</code></pre> <p>Output</p> <pre><code>(base) \u279c  spark-docker git:(main) \u2717 make dev\ndocker exec -it spark-master bash\nroot@6af4bb0bbdcf:/opt/spark# \n</code></pre> <ul> <li>to execute the <code>delta-demo.py</code> script, you can run this command:</li> </ul> <pre><code>./bin/spark-submit scripts/delta-demo.py\n</code></pre>"},{"location":"projects/delta-spark/#create-delta-table","title":"Create delta table","text":"<pre><code># Create a spark dataframe and write as a delta table\nprint(\"Starting Delta table creation\")\n\ndata = [(\"Robert\", \"Baratheon\", \"Baratheon\", \"Storms End\", 48),\n        (\"Eddard\", \"Stark\", \"Stark\", \"Winterfell\", 46),\n        (\"Jamie\", \"Lannister\", \"Lannister\", \"Casterly Rock\", 29)\n        ]\nschema = StructType([\n    StructField(\"firstname\", StringType(), True),\n    StructField(\"lastname\", StringType(), True),\n    StructField(\"house\", StringType(), True),\n    StructField(\"location\", StringType(), True),\n    StructField(\"age\", IntegerType(), True)\n])\n\ndf = spark.createDataFrame(data=data, schema=schema)\ndf.write.mode(saveMode=\"overwrite\").format(\"delta\").save(\"warehouse/delta-table\")\n</code></pre> <p>First, we define a spark DataFrame. In this case, we have the schema and input its data.</p> <pre><code>Starting Delta table creation\n+---------+---------+---------+-------------+---+\n|firstname| lastname|    house|     location|age|\n+---------+---------+---------+-------------+---+\n|   Robert|Baratheon|Baratheon|   Storms End| 48|\n|   Eddard|    Stark|    Stark|   Winterfell| 46|\n|    Jamie|Lannister|Lannister|Casterly Rock| 29|\n+---------+---------+---------+-------------+---+\n</code></pre>"},{"location":"projects/delta-spark/#read-delta-table","title":"Read delta table","text":"<p>Reading is as easy as again just specifying the <code>.format(\"delta\")</code> in the spark read API</p> <pre><code># read from delta\ndf = spark.read.format(\"delta\").load(\"warehouse/delta-table\")\ndf.show()\n\n# print delta table schema\ndf.printSchema()\n</code></pre> <p>Output:</p> <pre><code>root\n |-- firstname: string (nullable = true)\n |-- lastname: string (nullable = true)\n |-- house: string (nullable = true)\n |-- location: string (nullable = true)\n |-- age: integer (nullable = true)\n</code></pre>"},{"location":"projects/delta-spark/#update-delta-table","title":"Update delta table","text":"<p>There are some approach to update data in delta table, such following:</p>"},{"location":"projects/delta-spark/#overwrite-whole-table","title":"Overwrite whole table","text":"<p>In case you want to simply overwrite the delta table, you can simply provide <code>.mode(saveMode=\"overwrite\")</code> command.</p> <pre><code># Update data\nprint(\"Updating Delta table...!\")\ndata = [(\"Robert\", \"Baratheon\", \"Baratheon\", \"Storms End\", 49),\n        (\"Eddard\", \"Stark\", \"Stark\", \"Winterfell\", 47),\n        (\"Jamie\", \"Lannister\", \"Lannister\", \"Casterly Rock\", 30)\n        ]\nschema = StructType([\n    StructField(\"firstname\", StringType(), True),\n    StructField(\"lastname\", StringType(), True),\n    StructField(\"house\", StringType(), True),\n    StructField(\"location\", StringType(), True),\n    StructField(\"age\", IntegerType(), True)\n])\nsample_dataframe = spark.createDataFrame(data=data, schema=schema)\nsample_dataframe.write.mode(saveMode=\"overwrite\").format(\"delta\").save(\"warehouse/delta-table\")\n</code></pre> <p>Output</p> <pre><code>Updating Delta table...!\nShowing the data after being updated\n+---------+---------+---------+-------------+---+\n|firstname| lastname|    house|     location|age|\n+---------+---------+---------+-------------+---+\n|   Robert|Baratheon|Baratheon|   Storms End| 49|\n|   Eddard|    Stark|    Stark|   Winterfell| 47|\n|    Jamie|Lannister|Lannister|Casterly Rock| 30|\n+---------+---------+---------+-------------+---+\n</code></pre>"},{"location":"projects/delta-spark/#conditional-update","title":"Conditional Update","text":"<p>If you want to update a record or few records based on conditions, we can simply use the <code>.update</code> method</p> <pre><code># Update data in Delta\nprint(\"Conditional Update...!\")\n\n# delta table path\ndeltaTable = DeltaTable.forPath(spark, \"warehouse/delta-table\")\ndeltaTable.toDF().show()\n\ndeltaTable.update(\n    condition=expr(\"firstname == 'Jamie'\"),\n    set={\"firstname\": lit(\"Jamie\"), \"lastname\": lit(\"Lannister\"), \"house\": lit(\"Lannister\"),\n        \"location\": lit(\"Kings Landing\"), \"age\": lit(37)})\n\ndeltaTable.toDF().show()\n</code></pre> <p>Output</p> <pre><code>Conditional Update...!\n+---------+---------+---------+-------------+---+\n|firstname| lastname|    house|     location|age|\n+---------+---------+---------+-------------+---+\n|    Jamie|Lannister|Lannister|Casterly Rock| 30|\n|   Robert|Baratheon|Baratheon|   Storms End| 49|\n|   Eddard|    Stark|    Stark|   Winterfell| 47|\n+---------+---------+---------+-------------+---+\n\n+---------+---------+---------+-------------+---+\n|firstname| lastname|    house|     location|age|\n+---------+---------+---------+-------------+---+\n|    Jamie|Lannister|Lannister|Kings Landing| 37|\n|   Robert|Baratheon|Baratheon|   Storms End| 49|\n|   Eddard|    Stark|    Stark|   Winterfell| 47|\n+---------+---------+---------+-------------+---+\n</code></pre>"},{"location":"projects/delta-spark/#upsert-delta-table","title":"Upsert delta table","text":"<p>Upsert is simply a combination of 2 operations (update and insert). In order to upsert records, we will need to run the code below:</p> <pre><code># Upsert Data\nprint(\"Upserting Data...!\")\n# delta table path\ndeltaTable = DeltaTable.forPath(spark, \"warehouse/delta-table\")\ndeltaTable.toDF().show()\n\n# define new data\ndata = [(\"Gendry\", \"Baratheon\", \"Baratheon\", \"Kings Landing\", 19),\n        (\"Jon\", \"Snow\", \"Stark\", \"Winterfell\", 21),\n        (\"Jamie\", \"Lannister\", \"Lannister\", \"Casterly Rock\", 36)\n        ]\nschema = StructType([\n    StructField(\"firstname\", StringType(), True),\n    StructField(\"lastname\", StringType(), True),\n    StructField(\"house\", StringType(), True),\n    StructField(\"location\", StringType(), True),\n    StructField(\"age\", IntegerType(), True)\n])\n\nnewData = spark.createDataFrame(data=data, schema=schema)\n\ndeltaTable.alias(\"oldData\") \\\n    .merge(\n    newData.alias(\"newData\"),\n    \"oldData.firstname = newData.firstname\") \\\n    .whenMatchedUpdate(\n    set={\"firstname\": col(\"newData.firstname\"), \"lastname\": col(\"newData.lastname\"), \"house\": col(\"newData.house\"),\n        \"location\": col(\"newData.location\"), \"age\": col(\"newData.age\")}) \\\n    .whenNotMatchedInsert(\n    values={\"firstname\": col(\"newData.firstname\"), \"lastname\": col(\"newData.lastname\"), \"house\": col(\"newData.house\"),\n            \"location\": col(\"newData.location\"), \"age\": col(\"newData.age\")}) \\\n    .execute()\n\ndeltaTable.toDF().show()\n</code></pre> <p>First, we define a new dataframe which has updates to \"jamie\" again with his \"age\" and then we have 2 new records for \"Jon Snow\" and \"Gendry Baratheon\".</p> <p>The magic function that we use for upsert is <code>merge</code>.</p> <p>In this case, we assign <code>alias</code> to the old and new dataframes and set the rules of what to do if a record matches with the existing data record.</p> <p>The condition we're looking for is <code>oldData.firstName = newData.firstName</code>. And, if it matches, we update everything to the new values</p> <pre><code>.whenMatchedUpdate(\n    set={\"firstname\": col(\"newData.firstname\"), \"lastname\": col(\"newData.lastname\"), \"house\": col(\"newData.house\"),\n           \"location\": col(\"newData.location\"), \"age\": col(\"newData.age\")})\n</code></pre> <p>If it doesn't match, it will execute and insert</p> <pre><code>.whenNotMatchedInsert(\n    values={\"firstname\": col(\"newData.firstname\"), \"lastname\": col(\"newData.lastname\"), \"house\": col(\"newData.house\"),\n            \"location\": col(\"newData.location\"), \"age\": col(\"newData.age\")})\n</code></pre> <p>if we take a look at before and after of our operation on the dataframe, we can clearly see that the records have been upserted correctly.</p> <pre><code>Upserting Data...!\n+---------+---------+---------+-------------+---+\n|firstname| lastname|    house|     location|age|\n+---------+---------+---------+-------------+---+\n|    Jamie|Lannister|Lannister|Kings Landing| 37|\n|   Robert|Baratheon|Baratheon|   Storms End| 49|\n|   Eddard|    Stark|    Stark|   Winterfell| 47|\n+---------+---------+---------+-------------+---+\n\n+---------+---------+---------+-------------+---+\n|firstname| lastname|    house|     location|age|\n+---------+---------+---------+-------------+---+\n|   Gendry|Baratheon|Baratheon|Kings Landing| 19|\n|    Jamie|Lannister|Lannister|Casterly Rock| 36|\n|      Jon|     Snow|    Stark|   Winterfell| 21|\n|   Robert|Baratheon|Baratheon|   Storms End| 49|\n|   Eddard|    Stark|    Stark|   Winterfell| 47|\n+---------+---------+---------+-------------+---+\n</code></pre>"},{"location":"projects/delta-spark/#delete-delta-table","title":"Delete delta table","text":"<p>We can also delete a particular record based on filters just like we did for update</p> <pre><code># Delete Data\nprint(\"Deleting data...!\")\n\n# delta table path\ndeltaTable = DeltaTable.forPath(spark, \"warehouse/delta-table\")\ndeltaTable.toDF().show()\n\ndeltaTable.delete(condition=expr(\"firstname == 'Gendry'\"))\n\ndeltaTable.toDF().show()\n</code></pre> <p>In this case, we deleted the record for \"Gendry\" and it's reflected in the dataframe.</p> <pre><code>Deleting data...!\n+---------+---------+---------+-------------+---+\n|firstname| lastname|    house|     location|age|\n+---------+---------+---------+-------------+---+\n|   Gendry|Baratheon|Baratheon|Kings Landing| 19|\n|    Jamie|Lannister|Lannister|Casterly Rock| 36|\n|      Jon|     Snow|    Stark|   Winterfell| 21|\n|   Robert|Baratheon|Baratheon|   Storms End| 49|\n|   Eddard|    Stark|    Stark|   Winterfell| 47|\n+---------+---------+---------+-------------+---+\n\n+---------+---------+---------+-------------+---+\n|firstname| lastname|    house|     location|age|\n+---------+---------+---------+-------------+---+\n|   Robert|Baratheon|Baratheon|   Storms End| 49|\n|   Eddard|    Stark|    Stark|   Winterfell| 47|\n|    Jamie|Lannister|Lannister|Casterly Rock| 36|\n|      Jon|     Snow|    Stark|   Winterfell| 21|\n+---------+---------+---------+-------------+---+\n</code></pre>"},{"location":"projects/delta-spark/#time-travel_1","title":"Time Travel","text":"<p>Delta lake also allows you to read differnt historic versions of the data. the version history is stored in the <code>_delta_log</code> folder. we can inspect it to exactly know the kind of operation that happened on that point in time.</p> <p></p> <p>In order to read the data, you can specify versions and read like a normal dataframe.</p> <pre><code># Reading Older version of Data\nprint(\"Read old data...!\")\n\nprint(\"Dataframe Version 0\")\ndf_ver0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"warehouse/delta-table\")\ndf_ver0.show()\n\nprint(\"Dataframe Version 1\")\ndf_ver1 = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"warehouse/delta-table\")\ndf_ver1.show()\n</code></pre> <p>Output</p> <pre><code>Read old data...!\nDataframe Version 0\n+---------+---------+---------+-------------+---+\n|firstname| lastname|    house|     location|age|\n+---------+---------+---------+-------------+---+\n|   Robert|Baratheon|Baratheon|   Storms End| 48|\n|   Eddard|    Stark|    Stark|   Winterfell| 46|\n|    Jamie|Lannister|Lannister|Casterly Rock| 29|\n+---------+---------+---------+-------------+---+\n\nDataframe Version 1\n+---------+---------+---------+-------------+---+\n|firstname| lastname|    house|     location|age|\n+---------+---------+---------+-------------+---+\n|    Jamie|Lannister|Lannister|Casterly Rock| 30|\n|   Robert|Baratheon|Baratheon|   Storms End| 49|\n|   Eddard|    Stark|    Stark|   Winterfell| 47|\n+---------+---------+---------+-------------+---+\n</code></pre>"},{"location":"projects/delta-spark/#change-data-feed-cdc","title":"Change Data Feed (CDC)","text":"<p>Change Data Feed is customized CDC on top of Delta Lake, created by Databricks</p> <p>Change data feed allows Databricks to track row-level changes between versions of a Delta table. When enabled on a Delta table, the runtime records change events for all the data written into the table. This includes the row data along with metadata indicating whether the specified row was inserted, deleted, or updated.</p>"},{"location":"projects/delta-spark/#incrementally-process-change-data","title":"Incrementally process change data","text":"<p>Databricks recommends using change data feed in combination with Structured Streaming to incrementally process changes from Delta tables. You must use Structured Streaming for Databricks to automatically track versions for your table\u2019s change data feed.</p>"},{"location":"projects/delta-spark/#quick-code","title":"Quick Code","text":"<p>You can take a look the quick code</p> scripts/delta-cdc.py<pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import expr\nfrom delta.tables import DeltaTable\nimport shutil\n\n\npath = \"/tmp/delta-change-data-feed/student\"\notherPath = \"/tmp/delta-change-data-feed/student_source\"\n\n# Enable SQL commands and Update/Delete/Merge for the current spark session.\n# we need to set the following configs\nspark = SparkSession.builder \\\n    .appName(\"Change Data Feed\") \\\n    .master(\"local[*]\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n    .getOrCreate()\n\n\ndef cleanup():\n    shutil.rmtree(path, ignore_errors=True)\n    shutil.rmtree(otherPath, ignore_errors=True)\n    spark.sql(\"DROP TABLE IF EXISTS student\")\n    spark.sql(\"DROP TABLE IF EXISTS student_source\")\n\n\ndef read_cdc_by_table_name(starting_version):\n    return spark.read.format(\"delta\") \\\n        .option(\"readChangeFeed\", \"true\") \\\n        .option(\"startingVersion\", str(starting_version)) \\\n        .table(\"student\") \\\n        .orderBy(\"_change_type\", \"id\")\n\n\ndef stream_cdc_by_table_name(starting_version):\n    return spark.readStream.format(\"delta\") \\\n        .option(\"readChangeFeed\", \"true\") \\\n        .option(\"startingVersion\", str(starting_version)) \\\n        .table(\"student\") \\\n        .writeStream \\\n        .format(\"console\") \\\n        .option(\"numRows\", 1000) \\\n        .start()\n\n\ncleanup()\n\ntry:\n    # =============== Create student table ===============\n    spark.sql('''CREATE TABLE student (id INT, name STRING, age INT)\n                USING DELTA\n                PARTITIONED BY (age)\n                TBLPROPERTIES (delta.enableChangeDataFeed = true)\n                LOCATION '{0}'\n            '''.format(path))\n\n    spark.range(0, 10) \\\n        .selectExpr(\n            \"CAST(id as INT) as id\",\n            \"CAST(id as STRING) as name\",\n            \"CAST(id % 4 + 18 as INT) as age\") \\\n        .write.format(\"delta\").mode(\"append\").save(path)  # v1\n\n    # =============== Show table data + changes ===============\n\n    print(\"(v1) Initial Table\")\n    spark.read.format(\"delta\").load(path).orderBy(\"id\").show()\n\n    print(\"(v1) CDC changes\")\n    read_cdc_by_table_name(1).show()\n\n    table = DeltaTable.forPath(spark, path)\n\n    # =============== Perform UPDATE ===============\n\n    print(\"(v2) Updated id -&gt; id + 1\")\n    table.update(set={\"id\": expr(\"id + 1\")})  # v2\n    read_cdc_by_table_name(2).show()\n\n    # =============== Perform DELETE ===============\n\n    print(\"(v3) Deleted where id &gt;= 7\")\n    table.delete(condition=expr(\"id &gt;= 7\"))  # v3\n    read_cdc_by_table_name(3).show()\n\n    # =============== Perform partition DELETE ===============\n\n    print(\"(v4) Deleted where age = 18\")\n    table.delete(condition=expr(\"age = 18\"))  # v4, partition delete\n    read_cdc_by_table_name(4).show()\n\n    # =============== Create source table for MERGE ===============\n\n    spark.sql('''CREATE TABLE student_source (id INT, name STRING, age INT)\n                USING DELTA\n                LOCATION '{0}'\n            '''.format(otherPath))\n    spark.range(0, 3) \\\n        .selectExpr(\n            \"CAST(id as INT) as id\",\n            \"CAST(id as STRING) as name\",\n            \"CAST(id % 4 + 18 as INT) as age\") \\\n        .write.format(\"delta\").mode(\"append\").saveAsTable(\"student_source\")\n    source = spark.sql(\"SELECT * FROM student_source\")\n\n    # =============== Perform MERGE ===============\n\n    table.alias(\"target\") \\\n        .merge(\n            source.alias(\"source\"),\n            \"target.id = source.id\")\\\n        .whenMatchedUpdate(set={\"id\": \"source.id\", \"age\": \"source.age + 10\"}) \\\n        .whenNotMatchedInsertAll() \\\n        .execute() # v5\n    print(\"(v5) Merged with a source table\")\n    read_cdc_by_table_name(5).show()\n\n    # =============== Stream changes ===============\n\n    print(\"Streaming by table name\")\n    cdfStream = stream_cdc_by_table_name(0)\n    cdfStream.awaitTermination(10)\n    cdfStream.stop()\n\nfinally:\n    cleanup()\n    spark.stop()\n</code></pre> <p>It will do some steps:</p> <ul> <li>create student table</li> <li>show table data and all the changes</li> <li>perform any <code>UPDATE</code></li> <li>perform <code>DELETE</code></li> <li>perform partition <code>DELETE</code></li> <li>create source table for <code>MERGE</code></li> <li>perform <code>MERGE</code></li> <li>Stream changes</li> </ul> <p>to run the code, simply run below command</p> <pre><code>./bin/spark-submit scripts/delta-cdc.py\n</code></pre> <p>Output:</p> <pre><code>-------------------------------------------\nBatch: 0\n-------------------------------------------\n24/07/02 07:59:49 INFO CodeGenerator: Code generated in 1.676625 ms\n+---+----+---+----------------+---------------+--------------------+\n| id|name|age|    _change_type|_commit_version|   _commit_timestamp|\n+---+----+---+----------------+---------------+--------------------+\n|  6|   6| 20| update_preimage|              2|2024-07-02 07:59:...|\n|  7|   6| 20|update_postimage|              2|2024-07-02 07:59:...|\n|  2|   2| 20| update_preimage|              2|2024-07-02 07:59:...|\n|  3|   2| 20|update_postimage|              2|2024-07-02 07:59:...|\n|  8|   8| 18| update_preimage|              2|2024-07-02 07:59:...|\n|  9|   8| 18|update_postimage|              2|2024-07-02 07:59:...|\n|  4|   4| 18| update_preimage|              2|2024-07-02 07:59:...|\n|  5|   4| 18|update_postimage|              2|2024-07-02 07:59:...|\n|  1|   1| 19| update_preimage|              2|2024-07-02 07:59:...|\n|  2|   1| 19|update_postimage|              2|2024-07-02 07:59:...|\n|  5|   5| 19| update_preimage|              2|2024-07-02 07:59:...|\n|  6|   5| 19|update_postimage|              2|2024-07-02 07:59:...|\n|  9|   9| 19| update_preimage|              2|2024-07-02 07:59:...|\n| 10|   9| 19|update_postimage|              2|2024-07-02 07:59:...|\n|  7|   7| 21| update_preimage|              2|2024-07-02 07:59:...|\n|  8|   7| 21|update_postimage|              2|2024-07-02 07:59:...|\n|  3|   3| 21| update_preimage|              2|2024-07-02 07:59:...|\n|  4|   3| 21|update_postimage|              2|2024-07-02 07:59:...|\n|  0|   0| 18| update_preimage|              2|2024-07-02 07:59:...|\n|  1|   0| 18|update_postimage|              2|2024-07-02 07:59:...|\n|  2|   1| 30|update_postimage|              5|2024-07-02 07:59:...|\n|  1|   1| 19|          insert|              5|2024-07-02 07:59:...|\n|  2|   1| 19| update_preimage|              5|2024-07-02 07:59:...|\n| 10|   9| 19|          delete|              3|2024-07-02 07:59:...|\n|  7|   6| 20|          delete|              3|2024-07-02 07:59:...|\n|  0|   0| 18|          insert|              5|2024-07-02 07:59:...|\n|  8|   7| 21|          delete|              3|2024-07-02 07:59:...|\n|  9|   8| 18|          delete|              3|2024-07-02 07:59:...|\n|  0|   0| 18|          insert|              1|2024-07-02 07:59:...|\n|  8|   8| 18|          insert|              1|2024-07-02 07:59:...|\n|  4|   4| 18|          insert|              1|2024-07-02 07:59:...|\n|  3|   3| 21|          insert|              1|2024-07-02 07:59:...|\n|  7|   7| 21|          insert|              1|2024-07-02 07:59:...|\n|  1|   1| 19|          insert|              1|2024-07-02 07:59:...|\n|  5|   5| 19|          insert|              1|2024-07-02 07:59:...|\n|  9|   9| 19|          insert|              1|2024-07-02 07:59:...|\n|  6|   6| 20|          insert|              1|2024-07-02 07:59:...|\n|  2|   2| 20|          insert|              1|2024-07-02 07:59:...|\n|  1|   0| 18|          delete|              4|2024-07-02 07:59:...|\n|  5|   4| 18|          delete|              4|2024-07-02 07:59:...|\n+---+----+---+----------------+---------------+--------------------+\n</code></pre> <p>The latest status</p> <pre><code>24/07/02 07:59:49 INFO MicroBatchExecution: Streaming query made progress: {\n  \"id\" : \"b8eae84d-c996-4b17-aeca-84fbbe5afe7e\",\n  \"runId\" : \"987c4c3e-76c6-48be-b755-3b888297c183\",\n  \"name\" : null,\n  \"timestamp\" : \"2024-07-02T07:59:49.387Z\",\n  \"batchId\" : 0,\n  \"numInputRows\" : 40,\n  \"inputRowsPerSecond\" : 0.0,\n  \"processedRowsPerSecond\" : 90.29345372460496,\n  \"durationMs\" : {\n    \"addBatch\" : 182,\n    \"commitOffsets\" : 12,\n    \"getBatch\" : 55,\n    \"latestOffset\" : 115,\n    \"queryPlanning\" : 44,\n    \"triggerExecution\" : 443,\n    \"walCommit\" : 24\n  },\n  \"stateOperators\" : [ ],\n  \"sources\" : [ {\n    \"description\" : \"DeltaSource[file:/tmp/delta-change-data-feed/student]\",\n    \"startOffset\" : null,\n    \"endOffset\" : {\n      \"sourceVersion\" : 1,\n      \"reservoirId\" : \"77a3c27a-452c-47db-92bb-59b155a3fea2\",\n      \"reservoirVersion\" : 5,\n      \"index\" : 2,\n      \"isStartingVersion\" : false\n    },\n    \"latestOffset\" : null,\n    \"numInputRows\" : 40,\n    \"inputRowsPerSecond\" : 0.0,\n    \"processedRowsPerSecond\" : 90.29345372460496\n  } ],\n  \"sink\" : {\n    \"description\" : \"org.apache.spark.sql.execution.streaming.ConsoleTable$@643c3322\",\n    \"numOutputRows\" : 40\n  }\n}\n</code></pre> <p>You can also view the stages in the UI</p> <p></p> <p>In addition, there is provided streaming query statistics, showing metrics for CDC operations</p> <p></p>"},{"location":"projects/delta-spark/#references","title":"References","text":"<ul> <li>Delta Lake Official Documentation</li> <li>What is a Lakehouse?</li> <li>Delta Lake for ETL</li> <li>Delta Lake Optimize</li> <li>Delta Lake Introduction</li> <li>Use Delta Lake change data feed on Databricks</li> </ul>"},{"location":"projects/duckdb-quickstart/","title":"DuckDB Quick Start using Jupyter Notebook","text":""},{"location":"projects/duckdb-quickstart/#what-is-duckdb","title":"What is DuckDB?","text":"<p>DuckDB is a fast in-memory process analytical database. In other words, DuckDB is the SQLite for analytics use cases. It has no dependencies, is extremely easy to set up, and is optimized to perform queries on data.</p>"},{"location":"projects/duckdb-quickstart/#use-cases","title":"Use Cases","text":"<p>As a data analyst or a data scientiest, a typical workflow is to load data from CSV files or an S3 bucket, perform preprocessing steps, and run the analysis. To achieve these things, we can pull up a Jupyter Notebook, import Numpy and Pandas, and then execute the queries.</p> <p>Interestingly, if you look at your operations, you usually perform database operations such as joins, aggregates, filters, etc. But, instead of using a relational database management system (RDBMS), you use Pandas and Numpy.</p> <p>The question is, WHY? Setting up a database and loading data in it can be a painful slow and frustrating experience. If it were easy, you would have everything to gain by using an RDBMS.</p> <p>Say \"WELCOME\" to DuckDB! </p> <p>Success</p> <p>DuckDB is the easiest and fastest way to analyze data with a DB.</p>"},{"location":"projects/duckdb-quickstart/#why-is-using-duckdb","title":"Why is using DuckDB?","text":"<p>Few benefits of using DuckDB:</p>"},{"location":"projects/duckdb-quickstart/#local","title":"Local","text":"<ul> <li>DuckDB is an in-process single-file database with no external dependencies. What does that mean? Unlike Postgres, there is no client/server to set up or external dependencies to install.</li> <li>In addition, the data transfer to/from the client is slower than it should be, especially for local installations.</li> <li>On the other hand, DuckDB is embedded in the host application process and manages a database stored in a single file.</li> <li>There is no client/server, and it is straightforward to install.</li> </ul>"},{"location":"projects/duckdb-quickstart/#performances","title":"Performances","text":"<ul> <li>DuckDB is highly optimized for analytical query workloads (OLAP). Because it is columnar-oriented DB (along with other optimizations), complex-long-running queries to aggregate, join or read data become blazingly fast!</li> <li>It\u2019s easy to claim that ~90% of big data workloads can be handled on a single machine without any setup or computation pains with DuckDB.</li> </ul> <p>Warning</p> <p>That\u2019s not the case with Pandas. Pandas cannot leverage CPU cores to parallelize computations, making them slow to complete. It operates entirely in memory leading to out-of-memory errors if the dataset is too big.</p>"},{"location":"projects/duckdb-quickstart/#sql","title":"SQL","text":"<ul> <li>Whether you like it or not, SQL is more alive than ever. DuckDB supports a fairly advanced SQL command set, such as window functions and aggregates.</li> <li>It provides transactional guarantees (ACID properties) and uses the Postgres API. Forget about the ugly and slow Panda\u2019s manipulations.</li> <li>You can replace all of them with elegant SQL (it offers some additions on top of SQL to make it more friendly, as shown here).</li> <li>Finally, DuckDB has a Python Client and shines when mixed with Arrow and Parquet.</li> <li>DuckDB is an actively developed open-source, embeddable, in-process, and column-oriented SQL OLAP RDBMS.</li> </ul> <p>Info</p> <p>Perfect for data practitioners who want to perform local analytical workloads easily and quickly.</p>"},{"location":"projects/duckdb-quickstart/#limitations","title":"Limitations","text":"<ul> <li>DuckDB is designed to run on a single machine</li> </ul> <p>That means your data has to fit with that single machine; otherwise, it doesn\u2019t work. In reality, you can use powerful computers that will be enough for 99% of your workload. That shouldn\u2019t be an issue unless you process terabytes of data daily.</p> <ul> <li>DuckDB is not a multi-tenant database</li> </ul> <p>Having different people with different teams, developing models, and sharing data on the database will be very challenging. However, when you integrate the DB with other tools such as Airflow, S3, Parquet, and dbt, you can get a robust data ecosystem with which teams can efficiently work.</p> <ul> <li>DuckDB is not supposed a transactional database and should not be used that way</li> </ul>"},{"location":"projects/duckdb-quickstart/#duckdb-using-jupyter","title":"DuckDB using Jupyter","text":"<p>Time to play with DuckDB.</p> <p>First, you need to clone the repo first.</p> <pre><code>git clone git@github.com:karlchris/duckdb-quickstart.git\n</code></pre>"},{"location":"projects/duckdb-quickstart/#quick-start-installation","title":"Quick Start Installation","text":"<ul> <li>Add the <code>Dockerfile</code> to run everything inside Docker container.</li> </ul> <pre><code>FROM python:3.12-slim-bullseye\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nEXPOSE 8888\n\n# Run Jupyter Notebook when the container launches\nCMD [\"jupyter\", \"notebook\", \"--ip=0.0.0.0\", \"--port=8888\", \"--no-browser\", \"--allow-root\", \"--NotebookApp.token=''\"]\n</code></pre> <ul> <li>put relevant libraries in <code>requirements.txt</code></li> </ul> requirements.txt<pre><code>duckdb==1.0.0\npandas==2.2.2\njupyter==1.0.0\n</code></pre> <ul> <li>run <code>make build</code> to build the docker image.</li> </ul> <pre><code>(base) \u279c  duckdb-quickstart git:(main) make build\ndocker build -t duckdb-quickstart .\n[+] Building 40.8s (11/11) FINISHED                                                                                                                                                                                                                                                                docker:desktop-linux\n =&gt; [internal] load build definition from Dockerfile                                                                                                                                                                                                                                                               0.0s\n =&gt; =&gt; transferring dockerfile: 337B                                                                                                                                                                                                                                                                               0.0s\n =&gt; [internal] load metadata for docker.io/library/python:3.12-slim-bullseye                                                                                                                                                                                                                                       3.6s\n =&gt; [auth] library/python:pull token for registry-1.docker.io                                                                                                                                                                                                                                                      0.0s\n =&gt; [internal] load .dockerignore                                                                                                                                                                                                                                                                                  0.0s\n =&gt; =&gt; transferring context: 2B                                                                                                                                                                                                                                                                                    0.0s\n =&gt; [1/5] FROM docker.io/library/python:3.12-slim-bullseye@sha256:7e9400f6d73dcb6fd4ea9a2f6e5334da18631f8facb007711648d924a38cfdc6                                                                                                                                                                                 3.2s\n =&gt; =&gt; resolve docker.io/library/python:3.12-slim-bullseye@sha256:7e9400f6d73dcb6fd4ea9a2f6e5334da18631f8facb007711648d924a38cfdc6                                                                                                                                                                                 0.0s\n =&gt; =&gt; sha256:7e9400f6d73dcb6fd4ea9a2f6e5334da18631f8facb007711648d924a38cfdc6 9.12kB / 9.12kB                                                                                                                                                                                                                     0.0s\n =&gt; =&gt; sha256:a3db278a8d77cbdf5e65489c7088997af2402fa9fdc1ac964a48a4d9d11b81d2 1.95kB / 1.95kB                                                                                                                                                                                                                     0.0s\n =&gt; =&gt; sha256:4324ba87c71ab1bae726e1b4cf41a7431890b13de8649090720eb61faf9f4a5e 6.69kB / 6.69kB                                                                                                                                                                                                                     0.0s\n =&gt; =&gt; sha256:d13ad33c16eef01d20d5563bfb2ec4f25c0d12699b40cdab418e47b88d2f02e2 30.07MB / 30.07MB                                                                                                                                                                                                                   1.9s\n =&gt; =&gt; sha256:2197229b63cd075b53e5db849039a3900a119a8b056fbc92b7c606dbf3ecc530 1.06MB / 1.06MB                                                                                                                                                                                                                     1.6s\n =&gt; =&gt; sha256:95308c8804cad9fdf83d34811f850e032b4b36b2df9b53869b4f5a864e2219ab 11.29MB / 11.29MB                                                                                                                                                                                                                   2.5s\n =&gt; =&gt; sha256:90092edca53bb728d1d06f3d69125fc61fc5ea000d537a582a50391757705a45 232B / 232B                                                                                                                                                                                                                         2.0s\n =&gt; =&gt; extracting sha256:d13ad33c16eef01d20d5563bfb2ec4f25c0d12699b40cdab418e47b88d2f02e2                                                                                                                                                                                                                          0.8s\n =&gt; =&gt; sha256:5c5dbeaf51892c2b8c912e91ffe69046e3181b752e395fc638288a034c58e579 2.87MB / 2.87MB                                                                                                                                                                                                                     2.7s\n =&gt; =&gt; extracting sha256:2197229b63cd075b53e5db849039a3900a119a8b056fbc92b7c606dbf3ecc530                                                                                                                                                                                                                          0.0s\n =&gt; =&gt; extracting sha256:95308c8804cad9fdf83d34811f850e032b4b36b2df9b53869b4f5a864e2219ab                                                                                                                                                                                                                          0.2s\n =&gt; =&gt; extracting sha256:90092edca53bb728d1d06f3d69125fc61fc5ea000d537a582a50391757705a45                                                                                                                                                                                                                          0.0s\n =&gt; =&gt; extracting sha256:5c5dbeaf51892c2b8c912e91ffe69046e3181b752e395fc638288a034c58e579                                                                                                                                                                                                                          0.1s\n =&gt; [internal] load build context                                                                                                                                                                                                                                                                                  0.9s\n =&gt; =&gt; transferring context: 278.21MB                                                                                                                                                                                                                                                                              0.9s\n =&gt; [2/5] WORKDIR /app                                                                                                                                                                                                                                                                                             0.2s\n =&gt; [3/5] COPY requirements.txt .                                                                                                                                                                                                                                                                                  0.0s\n =&gt; [4/5] RUN pip install -r requirements.txt                                                                                                                                                                                                                                                                     32.6s\n =&gt; [5/5] COPY . .                                                                                                                                                                                                                                                                                                 0.2s \n =&gt; exporting to image                                                                                                                                                                                                                                                                                             1.0s \n =&gt; =&gt; exporting layers                                                                                                                                                                                                                                                                                            1.0s \n =&gt; =&gt; writing image sha256:9ca3eaade8602a5b5e100894d9d2b7bb2a641b1b280e9bdb0c861573148ab58d                                                                                                                                                                                                                       0.0s \n =&gt; =&gt; naming to docker.io/library/duckdb-quickstart                                                                                                                                                                                                                                                               0.0s \n\nView build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/mg3pyrw20olihy8xvkg7f2kz6\n\nWhat's next:\n    View a summary of image vulnerabilities and recommendations \u2192 docker scout quickview\n</code></pre> <ul> <li>run <code>make run</code> to start container.</li> </ul> <pre><code>(base) \u279c  duckdb-quickstart git:(main) make run  \ndocker run --rm -d -p 8888:8888 --name duckdb-quickstart -v /Users/karlchristian/repos/duckdb-quickstart:/app duckdb-quickstart\n17abb0eb31ce83dffffcf279ec5431fc377169e426dd17a1a12fc735fd9ed985\n</code></pre> <p>Danger</p> <p>Please, run <code>make stop</code> once you finish with this project.</p>"},{"location":"projects/duckdb-quickstart/#create-duckdb-connection","title":"Create DuckDB Connection","text":"<ul> <li>open <code>http://localhost:8888/tree?</code> as the Jupyter Notebook UI.</li> </ul> <ul> <li>open <code>duckdb-playground.ipynb</code> to run the main code.</li> </ul> <pre><code>import duckdb\n\n\nconn = duckdb.connect()\n</code></pre>"},{"location":"projects/duckdb-quickstart/#query-data-with-duckdb-and-python","title":"Query Data With DuckDB and Python","text":"<pre><code>df = conn.sql(\"\"\"\n    SELECT *\n    FROM 'data/yellow_trip_data/*.parquet'\n    LIMIT 10\n\"\"\").df()\n\ndf.head()\n</code></pre> <p>Output: </p> <p>Being able to query data is good, but retrieving data from a query is even better! DuckDB provides multiple methods that you can use to efficiently retrieve data, such as:</p> <ul> <li><code>fetchnumpy()</code> fetches the data as a dictionary of Numpy arrays.</li> <li><code>df()</code> brings the data as a Pandas Dataframe.</li> <li><code>arrow()</code> fetches the data as an Arrow table.</li> <li><code>pl()</code> pulls the data as a Polars Dataframe.</li> </ul> <p>Warning</p> <p>Ultimately, choosing the best method that suits your needs is up to you. As a best practice, don't use <code>fetch_one</code> or <code>fetch_all</code> as they create a Python object for every value returned by your query.</p> <p>If you have a lot of data, you can have a memory error or poor performance.</p>"},{"location":"projects/duckdb-quickstart/#create-a-table-from-dataframe","title":"Create a Table from DataFrame","text":"<ul> <li>you need to register the dataframe as a virtual table so you can query the data using the usual SQL features.</li> </ul> <pre><code>conn.register(\"df_view\", df)\nprint(conn.execute(\"DESCRIBE df_view\").df())\n</code></pre> <p>Output: </p> <ul> <li>execute <code>DESCRIBE</code> to check table information, including its table schema.</li> </ul> <pre><code>conn.execute(\"DESCRIBE df_view\").df()\n</code></pre> <p>Output: </p>"},{"location":"projects/duckdb-quickstart/#conclusion","title":"Conclusion","text":"<p>DuckDB saves you time by being easy to use and efficient at querying any data.  Instead of going through a classic RDBMS to load and query your data, simply use DuckDB. </p> <p>Danger</p> <p>Remember that for most of your use cases, you may not need a massive setup with 100 CPUs and 100Gb of memory.</p>"},{"location":"projects/duckdb-quickstart/#references","title":"References","text":"<ul> <li>DuckDB Official Documentation</li> <li>DuckDB: Getting started for Begineers</li> </ul>"},{"location":"projects/flink-k8s/","title":"Running Apache Flink on Kubernetes","text":""},{"location":"projects/flink-k8s/#apache-flink","title":"Apache Flink","text":"<p>Flink is a distributed system and requires effective allocation and management of compute resources in order to execute streaming applications.</p> <p>It integrates with all common cluster resource managers such as Hadoop YARN and Kubernetes, but can also be set up to run as a standalone cluster or even as a library.</p> <p>The Flink runtime consists of 2 types of processes:</p>"},{"location":"projects/flink-k8s/#client","title":"Client","text":"<p>The client is not part of the runtime and program execution, but is used to prepare and send a dataflow to the JobManager.</p> <p>After that, client can disconnect (detached mode), or stay connected to receive progress reports (attached mode).</p> <p>The client runs either as part of the Java/Scala program that triggers the execution, or in the command line process <code>./bin/flink run</code></p>"},{"location":"projects/flink-k8s/#jobmanager","title":"JobManager","text":"<p>The <code>JobManager</code> has a number of responsibilities related to coordinating the distributed execution of Flink Applications:</p> <ul> <li>it decides when to schedule the next task (or set of tasks)</li> <li>reacts to finished tasks or execution failures</li> <li>coordinates checkpoints</li> <li>coordinates recovery on failures</li> </ul> <p>this process consists of 3 components:</p> <ul> <li>ResourceManager: responsible for resource de-/allocation and provisioning in a Flink cluster -- it manages task slots, which are the unit of the resource scheduling in a Flink cluster.</li> <li>Dispatcher: REST interface to submit Flink applications for executions and starts a new JobMaster for each submitted job.</li> <li>JobMaster: responsible for managing the execution of a single JobGraph. Multiple jobs can run simultaneously in a Flink cluster, each having its own JobMaster.</li> </ul> <p>There is always at least one JobManager. A high-availability setup might have multiple JobManagers, one of which is always the leader, and the others are standby.</p>"},{"location":"projects/flink-k8s/#taskmanagers","title":"TaskManagers","text":"<p>The <code>TaskManagers</code> (also called workers) execute the tasks of a dataflow, and buffer and exchange the data streams.</p> <p>There must always be at least one TaskManager. The smallest unit of resource scheduling in a TaskManager is a task <code>slot</code>. The number of task slots in a TaskManager indicates the number of concurrent processing tasks.</p> <p>Note</p> <p>multiple operators may execute in a task slot</p>"},{"location":"projects/flink-k8s/#flink-application-execution","title":"Flink Application Execution","text":"<p>A Flink Application is any user program that spawns one or multiple Flink jobs from its <code>main()</code> method. The execution of these jobs can happen in a local JVM (<code>LocalEnvironment</code>) or on a remote setup of clusters with multiple machines (<code>RemoteEnvironment</code>).</p> <p>For each program, the <code>ExecutionEnvironment</code> provide methods to control the job execution.</p> <p>The jobs of a Flink Application can either be submitted to a long-running Flink Session CLuster or a Flink Application Cluster.</p> <p>Info</p> <p>the difference between those options is mainly related to the cluster's lifecycle and to resource isolation guarantees.</p>"},{"location":"projects/flink-k8s/#flink-kubernetes-operator","title":"Flink Kubernetes Operator","text":"<p>Flink Kubernetes Operator acts as a control plane to manage the complete deployment lifecycle of Apache Flink applications.</p> <p>Although Flink\u2019s native Kubernetes integration already allows you to directly deploy Flink applications on a running Kubernetes(k8s) cluster, custom resources and the operator pattern have also become central to a Kubernetes native deployment experience.</p> <p>Flink Kubernetes Operator aims to capture the responsibilities of a human operator who is managing Flink deployments.</p> <p>Human operators have deep knowledge of how Flink deployments ought to behave, how to start clusters, how to deploy jobs, how to upgrade them and how to react if there are problems.</p> <p>The main goal of the operator is the automation of these activities, which cannot be achieved through the Flink native integration alone.</p>"},{"location":"projects/flink-k8s/#control-loop","title":"Control Loop","text":"<p>Users can interact with the operator using the Kubernetes command-line tool, <code>kubectl</code>. The operator continuously tracks cluster events relating to the <code>FlinkDeployment</code> custom resources. When the operator receives a new resource update, it will take action to adjust the Kubernetes cluster to the desired state as part of its reconciliation loop.</p> <p>The steps are:</p> <ul> <li>User submits a <code>FlinkDeployment</code> custom resource using <code>kubectl</code></li> <li>Operator observes the current status of the Flink resource (if previously deployed)</li> <li>Operator validates the submitted resource change</li> <li>Operator reconciles any required changes and executes upgrades</li> </ul> <p>The CR can be (re)applied on the cluster any time. The Operator makes continuous adjustments to imitate the desired state until the current state becomes the desired state. All lifecycle management operations are realized using this very simple principle in the Operator.</p> <p>Info</p> <p>The Operator is built with the Java Operator SDK and uses the Native Kubernetes Integration for launching Flink deployments and submitting jobs under the hood.</p>"},{"location":"projects/flink-k8s/#steps-to-deploy-flink-kubernetes-operator","title":"Steps to deploy Flink Kubernetes Operator","text":"<p>First, clone this repo Flink on K8s, by running this command</p> <pre><code>git clone git@github.com:karlchris/flink-k8s.git\n</code></pre>"},{"location":"projects/flink-k8s/#1-create-flink-kubernetes-cluster","title":"1. Create Flink Kubernetes Cluster","text":"<p>run below command to create Kubernetes cluster</p> <pre><code>kind create cluster --name flink --config kind-cluster.yaml\nkubectl cluster-info\nkubectl get nodes -o wide\n</code></pre> <p>or just run</p> <pre><code>make cluster\n</code></pre> <p>Output</p> <pre><code>\u279c  flink-k8s make cluster\nCreating kubernetes cluster and check it ...\nCreating cluster \"flink\" ...\n \u2713 Ensuring node image (kindest/node:v1.30.0) \ud83d\uddbc \n \u2713 Preparing nodes \ud83d\udce6 \ud83d\udce6  \n \u2713 Writing configuration \ud83d\udcdc \n \u2713 Starting control-plane \ud83d\udd79\ufe0f \n \u2713 Installing CNI \ud83d\udd0c \n \u2713 Installing StorageClass \ud83d\udcbe \n \u2713 Joining worker nodes \ud83d\ude9c \nSet kubectl context to \"kind-flink\"\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-flink\n\nHave a nice day! \ud83d\udc4b\nKubernetes control plane is running at https://127.0.0.1:58557\nCoreDNS is running at https://127.0.0.1:58557/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\nNAME                  STATUS     ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE                         KERNEL-VERSION    CONTAINER-RUNTIME\nflink-control-plane   Ready      control-plane   21s   v1.30.0   172.24.0.3    &lt;none&gt;        Debian GNU/Linux 12 (bookworm)   6.6.26-linuxkit   containerd://1.7.15\nflink-worker          NotReady   &lt;none&gt;          1s    v1.30.0   172.24.0.2    &lt;none&gt;        Debian GNU/Linux 12 (bookworm)   6.6.26-linuxkit   containerd://1.7.15\n</code></pre>"},{"location":"projects/flink-k8s/#2-deploy-kubernetes-namespace","title":"2. Deploy Kubernetes namespace","text":"<p>run this command to create and deploy kubernetes namespace <code>flink</code></p> <pre><code>kubectl create namespace flink\nkubectl get namespaces\nkubectl config set-context --current --namespace=flink\n</code></pre> <p>or just run</p> <pre><code>make ns\n</code></pre> <p>Output</p> <pre><code>\u279c  flink-k8s make ns     \nnamespace/flink created\nNAME                 STATUS   AGE\ndefault              Active   84s\nflink                Active   0s\nkube-node-lease      Active   84s\nkube-public          Active   84s\nkube-system          Active   84s\nlocal-path-storage   Active   81s\nContext \"kind-flink\" modified.\n</code></pre>"},{"location":"projects/flink-k8s/#3-install-helm-chart-for-flink-and-prometheus","title":"3. Install Helm chart for Flink and Prometheus","text":"<p>run below command to fetch and install helm charts</p> <pre><code>echo \"Fetching flink from Helm chart\"\nhelm repo add flink-operator-repo https://downloads.apache.org/flink/flink-kubernetes-operator-1.7.0/\nhelm repo update\nhelm search repo flink\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm install prometheus prometheus-community/kube-prometheus-stack\n\necho \"Installing certs ...\"\nkubectl create -f https://github.com/jetstack/cert-manager/releases/download/v1.8.2/cert-manager.yaml\n\necho \"Waiting for 1 min ...\"\nsleep 60\n\necho \"Installing flink ...\"\nhelm install flink flink-operator-repo/flink-kubernetes-operator --namespace flink --debug\n</code></pre> <p>or run</p> <pre><code>make install\n</code></pre> <p>Output</p> <pre><code>\u279c  flink-k8s make install\nFetching flink from Helm chart\n\"flink-operator-repo\" already exists with the same configuration, skipping\nHang tight while we grab the latest from your chart repositories...\n...Successfully got an update from the \"apache-airflow\" chart repository\n...Successfully got an update from the \"flink-operator-repo\" chart repository\n...Successfully got an update from the \"prometheus-community\" chart repository\nUpdate Complete. \u2388Happy Helming!\u2388\nNAME                                            CHART VERSION   APP VERSION     DESCRIPTION                                       \nflink-operator-repo/flink-kubernetes-operator   1.7.0           1.7.0           A Helm chart for the Apache Flink Kubernetes Op...\n\"prometheus-community\" already exists with the same configuration, skipping\nNAME: prometheus\nLAST DEPLOYED: Mon Jun 10 13:52:21 2024\nNAMESPACE: flink\nSTATUS: deployed\nREVISION: 1\nNOTES:\nkube-prometheus-stack has been installed. Check its status by running:\n  kubectl --namespace flink get pods -l \"release=prometheus\"\n\nVisit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create &amp; configure Alertmanager and Prometheus instances using the Operator.\nInstalling certs ...\nnamespace/cert-manager created\ncustomresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created\ncustomresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created\ncustomresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io created\ncustomresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io created\ncustomresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io created\ncustomresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io created\nserviceaccount/cert-manager-cainjector created\nserviceaccount/cert-manager created\nserviceaccount/cert-manager-webhook created\nconfigmap/cert-manager-webhook created\nclusterrole.rbac.authorization.k8s.io/cert-manager-cainjector created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-issuers created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificates created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-orders created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-challenges created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created\nclusterrole.rbac.authorization.k8s.io/cert-manager-view created\nclusterrole.rbac.authorization.k8s.io/cert-manager-edit created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created\nclusterrole.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests created\nclusterrole.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-issuers created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-clusterissuers created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificates created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-orders created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-challenges created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-ingress-shim created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-approve:cert-manager-io created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-controller-certificatesigningrequests created\nclusterrolebinding.rbac.authorization.k8s.io/cert-manager-webhook:subjectaccessreviews created\nrole.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created\nrole.rbac.authorization.k8s.io/cert-manager:leaderelection created\nrole.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created\nrolebinding.rbac.authorization.k8s.io/cert-manager-cainjector:leaderelection created\nrolebinding.rbac.authorization.k8s.io/cert-manager:leaderelection created\nrolebinding.rbac.authorization.k8s.io/cert-manager-webhook:dynamic-serving created\nservice/cert-manager created\nservice/cert-manager-webhook created\ndeployment.apps/cert-manager-cainjector created\ndeployment.apps/cert-manager created\ndeployment.apps/cert-manager-webhook created\nmutatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created\nvalidatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created\nWaiting for 1 min ...\nInstalling flink ...\ninstall.go:222: [debug] Original chart version: \"\"\ninstall.go:239: [debug] CHART PATH: /Users/karlchristian/Library/Caches/helm/repository/flink-kubernetes-operator-1.7.0-helm.tgz\n</code></pre>"},{"location":"projects/flink-k8s/#4-build-docker-image-and-load-it-into-flink-cluster","title":"4. Build docker image and load it into Flink Cluster","text":"<p>run below commands to build the docker image and load it to Kubernetes cluster</p> <pre><code>DOCKER_BUILDKIT=1 docker build -t pyflink:dev .\nkind load docker-image pyflink:dev --name flink\n</code></pre> <p>or just run</p> <pre><code>make build\n</code></pre> <p>Output</p> <pre><code>\u279c  flink-k8s make build  \n[+] Building 309.5s (13/13) FINISHED                                                                                                                                                                                                     docker:desktop-linux\n =&gt; [internal] load build definition from Dockerfile                                                                                                                                                                                                     0.0s\n =&gt; =&gt; transferring dockerfile: 1.45kB                                                                                                                                                                                                                   0.0s\n =&gt; [internal] load metadata for docker.io/library/flink:1.17.1                                                                                                                                                                                          3.5s\n =&gt; [auth] library/flink:pull token for registry-1.docker.io                                                                                                                                                                                             0.0s\n =&gt; [internal] load .dockerignore                                                                                                                                                                                                                        0.0s\n =&gt; =&gt; transferring context: 2B                                                                                                                                                                                                                          0.0s\n =&gt; [1/7] FROM docker.io/library/flink:1.17.1@sha256:2d2733e37ead5b1df1a81110db653382ba7906fb3ba4e998e0f325d8c752cd7b                                                                                                                                   56.2s\n =&gt; =&gt; resolve docker.io/library/flink:1.17.1@sha256:2d2733e37ead5b1df1a81110db653382ba7906fb3ba4e998e0f325d8c752cd7b                                                                                                                                    0.0s\n =&gt; =&gt; sha256:895d322e8e5957c04af3ab7b3431f2a562182d34167c6e159e02044150a66967 28.39MB / 28.39MB                                                                                                                                                         4.7s\n =&gt; =&gt; sha256:4035bfa3ea28e4a939fa769d2862227b94b4b5b961f65ac5df2ea3df7a0c51e4 12.84MB / 12.84MB                                                                                                                                                         8.0s\n =&gt; =&gt; sha256:44426d62154b920edc46b4f9c3d95ead5cff773239ebb0640ed78a3eabd75d3d 45.40MB / 45.40MB                                                                                                                                                        10.1s\n =&gt; =&gt; sha256:2d2733e37ead5b1df1a81110db653382ba7906fb3ba4e998e0f325d8c752cd7b 549B / 549B                                                                                                                                                               0.0s\n =&gt; =&gt; sha256:ccd51a0f3b8f4c019155583df273c6562bfaac9abe52704a48e67d967f8e5388 2.62kB / 2.62kB                                                                                                                                                           0.0s\n =&gt; =&gt; sha256:1501184341512f2c553a429222fed9ffdf62b1e815356676ed2767b8033358bf 11.93kB / 11.93kB                                                                                                                                                         0.0s\n =&gt; =&gt; sha256:a5b0a8b8196635efd1dce5dd763022df6a147662cd3aa1dcff312442551fce3d 159B / 159B                                                                                                                                                               5.0s\n =&gt; =&gt; extracting sha256:895d322e8e5957c04af3ab7b3431f2a562182d34167c6e159e02044150a66967                                                                                                                                                                0.5s\n =&gt; =&gt; sha256:deeea1065ab43308ab77465a15be2c3e434488174fb1da412ee0e1f868c0ea93 732B / 732B                                                                                                                                                               5.4s\n =&gt; =&gt; sha256:6f996a6320ba196b53221fe2bc2ce7ce09f922cbc421e0123815593ac38c025e 4.51MB / 4.51MB                                                                                                                                                           6.8s\n =&gt; =&gt; sha256:8e2a6e0a48e478e486e6c060d2b736fb3687a0bab95bb276acceff8e9f218734 835.39kB / 835.39kB                                                                                                                                                       7.4s\n =&gt; =&gt; sha256:9d20355c28ef09a7afe8ebd2a08cadaae9ffc2d8d9039d7e7a52edbbebc07575 4.64kB / 4.64kB                                                                                                                                                           8.0s\n =&gt; =&gt; extracting sha256:4035bfa3ea28e4a939fa769d2862227b94b4b5b961f65ac5df2ea3df7a0c51e4                                                                                                                                                                0.5s\n =&gt; =&gt; sha256:00a06505ac4a71a0a3a3fa5d6b4c8a61621c35508a0bfdce82d878dcfd1d9c19 148B / 148B                                                                                                                                                               8.4s\n =&gt; =&gt; sha256:a8077a97f1384ae731c36f4f260e8acd9f7f50521b9ccacec61a903165fa5a6a 469.60MB / 469.60MB                                                                                                                                                      53.4s\n =&gt; =&gt; sha256:33336083219c139aedbf20429bd9b3f24b495ee36f145b0e8d253f675ea27a7c 2.11kB / 2.11kB                                                                                                                                                           8.9s\n =&gt; =&gt; extracting sha256:44426d62154b920edc46b4f9c3d95ead5cff773239ebb0640ed78a3eabd75d3d                                                                                                                                                                0.9s\n =&gt; =&gt; extracting sha256:a5b0a8b8196635efd1dce5dd763022df6a147662cd3aa1dcff312442551fce3d                                                                                                                                                                0.0s\n =&gt; =&gt; extracting sha256:deeea1065ab43308ab77465a15be2c3e434488174fb1da412ee0e1f868c0ea93                                                                                                                                                                0.0s\n =&gt; =&gt; extracting sha256:6f996a6320ba196b53221fe2bc2ce7ce09f922cbc421e0123815593ac38c025e                                                                                                                                                                0.1s\n =&gt; =&gt; extracting sha256:8e2a6e0a48e478e486e6c060d2b736fb3687a0bab95bb276acceff8e9f218734                                                                                                                                                                0.0s\n =&gt; =&gt; extracting sha256:9d20355c28ef09a7afe8ebd2a08cadaae9ffc2d8d9039d7e7a52edbbebc07575                                                                                                                                                                0.0s\n =&gt; =&gt; extracting sha256:00a06505ac4a71a0a3a3fa5d6b4c8a61621c35508a0bfdce82d878dcfd1d9c19                                                                                                                                                                0.0s\n =&gt; =&gt; extracting sha256:a8077a97f1384ae731c36f4f260e8acd9f7f50521b9ccacec61a903165fa5a6a                                                                                                                                                                2.7s\n =&gt; =&gt; extracting sha256:33336083219c139aedbf20429bd9b3f24b495ee36f145b0e8d253f675ea27a7c                                                                                                                                                                0.0s\n =&gt; [internal] load build context                                                                                                                                                                                                                        0.0s\n =&gt; =&gt; transferring context: 1.82kB                                                                                                                                                                                                                      0.0s\n =&gt; [2/7] RUN apt-get update -y &amp;&amp;   apt-get install -y --no-install-recommends     build-essential=12.9ubuntu3     openjdk-11-jdk-headless     libbz2-dev     libffi-dev     libssl-dev     zlib1g-dev     lzma     liblzma-dev   &amp;&amp;   wget -q \"http  147.2s\n =&gt; [3/7] RUN pip3 install --no-cache-dir apache-flink==1.17.1 &amp;&amp;   pip3 cache purge                                                                                                                                                                   100.9s \n =&gt; [4/7] WORKDIR /opt/flink                                                                                                                                                                                                                             0.0s \n =&gt; [5/7] RUN mkdir /opt/flink/usrlib &amp;&amp;     mkdir /opt/flink/usrlib/output                                                                                                                                                                              0.1s \n =&gt; [6/7] COPY src /opt/flink/usrlib/src/                                                                                                                                                                                                                0.0s \n =&gt; [7/7] COPY input /opt/flink/usrlib/input/                                                                                                                                                                                                            0.0s \n =&gt; exporting to image                                                                                                                                                                                                                                   1.5s \n =&gt; =&gt; exporting layers                                                                                                                                                                                                                                  1.5s \n =&gt; =&gt; writing image sha256:75a2e6a108d935d8c707c9dbd358d9483995c5308a1f0f371901f1a7640ac36c                                                                                                                                                             0.0s\n =&gt; =&gt; naming to docker.io/library/pyflink:dev                                                                                                                                                                                                           0.0s\n\nView build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/mp9pld6uzxlzlvohlenyzcp31\nImage: \"pyflink:dev\" with ID \"sha256:75a2e6a108d935d8c707c9dbd358d9483995c5308a1f0f371901f1a7640ac36c\" not yet present on node \"flink-control-plane\", loading...\nImage: \"pyflink:dev\" with ID \"sha256:75a2e6a108d935d8c707c9dbd358d9483995c5308a1f0f371901f1a7640ac36c\" not yet present on node \"flink-worker\", loading...\n</code></pre>"},{"location":"projects/flink-k8s/#5-execute-the-job","title":"5. Execute the Job","text":"<p>There's 1 simple job to show word count based on the <code>FileSource</code>, it's located in <code>src/word_count.py</code></p> <p>There's related YAML file for this job</p> jobs/word_count.yaml<pre><code>apiVersion: flink.apache.org/v1beta1\nkind: FlinkDeployment\nmetadata:\n  name: word-count\nspec:\n  image: pyflink:dev\n  imagePullPolicy: Never\n  flinkVersion: v1_17\n  flinkConfiguration:\n    taskmanager.numberOfTaskSlots: \"2\"\n  serviceAccount: flink\n  jobManager:\n    resource:\n      memory: \"2048m\"\n      cpu: 1\n  taskManager:\n    resource:\n      memory: \"2048m\"\n      cpu: 1\n  job:\n    entryClass: \"org.apache.flink.client.python.PythonDriver\"\n    args: [\"-pyclientexec\", \"/usr/local/bin/python3\", \"-py\", \"/opt/flink/usrlib/src/word_count.py\"]\n    parallelism: 1\n    upgradeMode: stateless\n</code></pre> <p>run with this command</p> <pre><code>kubectl apply -f jobs/word_count.yaml -n flink\n</code></pre> <p>It will create <code>Pods</code> and <code>Service</code> related to the submitted jobs. Once you apply some new jobs, it will create new pods with resources.</p> <p>You can check the existing pods by running</p> <pre><code>kubectl get pods\n</code></pre> <p>to check the running job on Flink UI, you can run this command</p> <pre><code>kubectl port-forward svc/word-count-rest 8081\n</code></pre> <p>you can access the UI on <code>http://localhost:8081</code></p> <p></p> <p>You can also check the Word Count DAG and process details on UI</p> <p></p> <p>You can also go to <code>Job Manager Log</code> to see the full logs</p> <p></p> <p>There's <code>Metrics</code> related to this job</p> <p></p> <p>and its <code>Configuration</code></p> <p></p>"},{"location":"projects/flink-k8s/#monitoring","title":"Monitoring","text":"<p>You can use <code>Prometheus</code> to view infrastructure loads in the dashboard.</p> <p>The Prometheus Operator among other options provides an elegant, declarative way to specify how group of pods should be monitored using custom resources.</p> <p>To install the Prometheus operator via Helm run:</p> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm install prometheus prometheus-community/kube-prometheus-stack\n</code></pre> <p>you can run below commands to create prometheus pods and forward it to localhost port</p> pod-monitor.yaml<pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PodMonitor\nmetadata:\n  name: flink-kubernetes-operator\n  labels:\n    release: prometheus\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: flink-kubernetes-operator\n  podMetricsEndpoints:\n      - port: metrics\n</code></pre> <pre><code>kubectl apply -f pod-monitor.yaml -n flink\nkubectl port-forward deployment/prometheus-grafana 3000\n</code></pre> <p>you can view it through <code>http://localhost:3000/explore</code></p> <p>you can do all the above commands or just simply run below</p> <pre><code>make monitor\n</code></pre> <p>the credentials for the dashboard can be retrieved by:</p> <pre><code>kubectl get secret prometheus-grafana -o jsonpath=\"{.data.admin-user}\" | base64 --decode ; echo\nkubectl get secret prometheus-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n</code></pre> <p>or just simply run</p> <pre><code>make grafana-creds\n</code></pre> <p></p> <p>By dashboard above, you can see the CPU utilization for this Kubernetes cluster and jobs. This is a great view to be able to optimize the infrastructure better.</p>"},{"location":"projects/flink-k8s/#references","title":"References","text":"<ul> <li>Flink Kubernetes Operator</li> <li>Apache Flink</li> </ul>"},{"location":"projects/gh-pages-gh-actions/","title":"Building Github Pages using Github Actions","text":""},{"location":"projects/gh-pages-gh-actions/#what-is-cicd","title":"What is CI/CD?","text":"<p>Automation is a core principle for achieving DevOps success and CI/CD is a critical component. CI/CD comprises of continuous integration and continuous delivery or continuous deployment. Put together, they form a \u201cCI/CD pipeline\u201d\u2014a series of automated workflows that help DevOps teams cut down on manual tasks:</p> <ul> <li>Continuous Integration (CI): Automatically builds, tests, and integrates code changes within a shared repository</li> <li>Continuous Delivery (CD): automatically delivers code changes to production-ready environments for approval</li> <li>Continuous Deployment (CD): automatically deploys code changes to customers directly</li> </ul> <p></p> <p>Continuous deployment is the ultimate example of DevOps automation. That doesn\u2019t mean it\u2019s the only way to do CI/CD, or the \u201cright\u201d way. Since continuous deployment relies on rigorous testing tools and a mature testing culture, most software teams start with continuous delivery and integrate more automated testing over time.</p>"},{"location":"projects/gh-pages-gh-actions/#why-cicd","title":"Why CI/CD?","text":"<ul> <li>Deployment velocity: Ongoing feedback allows developers to commit smaller changes more often, versus waiting for one release.</li> <li>Stability and reliability: Automated, continuous testing ensures that codebases remain stable and release-ready at any time.</li> <li>Business growth: Freed up from manual tasks, organizations can focus resources on innovation, customer satisfaction, and paying down technical debt.</li> </ul>"},{"location":"projects/gh-pages-gh-actions/#github-world","title":"Github world","text":"<p>This is everything you must know quickly about Github, Github Actions &amp; Github Pages.</p> <p>GitHub is a developer platform that allows developers to create, store, manage and share their code.</p> <p></p>"},{"location":"projects/gh-pages-gh-actions/#github-actions","title":"Github Actions","text":"<p>Using GitHub Actions, it\u2019s easier than ever to bring CI/CD directly into your workflow right from your repository.</p> <p></p> <p>Benefits of using Github Actions as CI/CD automation:</p> <ul> <li>CI/CD pipeline setup is relatively simple</li> </ul> <p>GitHub Actions is made by and for developers, so you don\u2019t need dedicated resources to set up and maintain your pipeline. There\u2019s no need to manually configure and set up CI/CD.</p> <ul> <li>Respond to any webhook on GitHub</li> </ul> <p>Since GitHub Actions is fully integrated with GitHub, you can set any webhook as an event trigger for an automation or CI/CD pipeline. This includes things like pull requests, issues, and comments, but it also includes webhooks from any app you have integrated into your GitHub repository.</p> <ul> <li>Community-powered, reusable workflows</li> </ul> <p>You can share your workflows publicly with the wider GitHub community or access pre-built CI/CD workflows in the GitHub Marketplace (there are more than 11,000 available actions!).</p> <ul> <li>Support for any platform, any language, and any cloud</li> </ul> <p>GitHub Actions is platform agnostic, language agnostic, and cloud agnostic.</p>"},{"location":"projects/gh-pages-gh-actions/#github-pages","title":"Github Pages","text":"<p>GitHub Pages are public webpages hosted and published through GitHub. You can use GitHub Pages to showcase some open source projects, host a blog or share any portfolio.</p> <p>Note</p> <p>Usually, github pages will be published in <code>&lt;your github handle&gt;.github.io</code> domain, unless you provide custom domain.</p>"},{"location":"projects/gh-pages-gh-actions/#ci-test","title":"CI Test","text":"<p>CI is used to to test the code changes, it will lint, check and validate whatever we put as the test checks and ensure if this code changes is deployed, it will be really safe.</p> <p>Example</p> <p>In this project, the website will only contains markdown files, then the test check will lint all the markdown and ensure nothing is violating the rules.</p> <p>the used library here is markdownlint</p> <ul> <li>First, we need to wrap the test execution inside Docker, so the idea is everytime it attempts to test, it will build the image, run the container and perform testing.</li> </ul> Dockerfile-test<pre><code>FROM markdownlint/markdownlint:0.13.0\n\nWORKDIR /docs\nCOPY docs .\nCOPY .mdlrc .\n\nENTRYPOINT [\"mdl\", \"/docs\"]\n</code></pre> <ul> <li>You will aware about <code>.mdlrc</code> file, it's config file specific to markdownlint, you can put all the rules, ignore rules, etc in that config file.</li> </ul> .mdlrc<pre><code>rules \"~MD013\", \"~MD046\", \"~MD036\", \"~MD026\", \"~MD009\", \"~MD024\", \"~MD022\", \"~MD033\", \"~MD012\", \"~MD023\", \"~MD025\", \"~MD034\"\n</code></pre> <ul> <li>build <code>ci.yaml</code> github actions workflow file</li> </ul> .github/workflows/ci.yaml<pre><code>name: ci\non: pull_request\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: execute test\n        run: make test\n</code></pre> <p>CI will be triggered every time GH Pull Request is opened or reopened and perform the testings.</p> <p></p> <ul> <li>you can also try to perform the testing in local first, before proceeding to perform CI on github, just run this command.</li> </ul> <pre><code>make test\n</code></pre> <pre><code>test:\n    @docker build . -f Dockerfile-test \\\n        -t ${IMG}:test\n    @docker run ${IMG}:test\n</code></pre> <p>Output</p> <pre><code>\u279c  data-engineering git:(cicd) make test\n[+] Building 2.9s (10/10) FINISHED                                                                                                                                   docker:desktop-linux\n =&gt; [internal] load build definition from Dockerfile-test                                                                                                                            0.0s\n =&gt; =&gt; transferring dockerfile: 150B                                                                                                                                                 0.0s\n =&gt; [internal] load metadata for docker.io/markdownlint/markdownlint:0.13.0                                                                                                          2.8s\n =&gt; [auth] markdownlint/markdownlint:pull token for registry-1.docker.io                                                                                                             0.0s\n =&gt; [internal] load .dockerignore                                                                                                                                                    0.0s\n =&gt; =&gt; transferring context: 2B                                                                                                                                                      0.0s\n =&gt; [1/4] FROM docker.io/markdownlint/markdownlint:0.13.0@sha256:25c930bb03b1ea50a2e6e377e4ab6f1fe89a760f85e56d189cc007caa4301b0b                                                    0.0s\n =&gt; [internal] load build context                                                                                                                                                    0.0s\n =&gt; =&gt; transferring context: 26.95kB                                                                                                                                                 0.0s\n =&gt; CACHED [2/4] WORKDIR /docs                                                                                                                                                       0.0s\n =&gt; [3/4] COPY docs .                                                                                                                                                                0.0s\n =&gt; [4/4] COPY .mdlrc .                                                                                                                                                              0.0s\n =&gt; exporting to image                                                                                                                                                               0.0s\n =&gt; =&gt; exporting layers                                                                                                                                                              0.0s\n =&gt; =&gt; writing image sha256:4d7129163c4685f05e0c55eb21fc2c628ab08379a9de905393050f7d7698ae6a                                                                                         0.0s\n =&gt; =&gt; naming to docker.io/library/de-kchrs:test                                                                                                                                     0.0s\n\nView build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/4nvr40dc58fstxv72l1mtsse6\n</code></pre> <p>Tip</p> <p>CI should run really fast and keep it light, under 10s or maximum couple of minutes.</p> <p>Why? Just imagine if you will have a lot of PRs in your repo, they will run CI at the same time or almost same time.  If 1 CI on 1 PR will run around 30 minutes or 1 hour, you will know how long it will take to perform testing on multiple PRs.</p> <p>Also, it will take a lot of resources to run heavy testing, if the production code interact with REST API with considerate I/O, we need to eliminate this blockers in CI by mocking it.</p>"},{"location":"projects/gh-pages-gh-actions/#development-in-local","title":"Development in local","text":"<p>You can try/test to build the code and host it to your localhost.</p> <ul> <li>First, you will need a <code>Dockerfile</code></li> </ul> Dockerfile<pre><code>ARG MKDM_VERSION=latest\n\nFROM squidfunk/mkdocs-material:${MKDM_VERSION}\n\nCOPY . .\n\n# Start development server by default\nENTRYPOINT [\"mkdocs\"]\nCMD [\"serve\", \"--dev-addr=0.0.0.0:8000\"]\n</code></pre> <ul> <li>run below command to build the image</li> </ul> <pre><code>make build-page\n</code></pre> <pre><code># Build docker image for local development\nbuild-page:\n    @echo \"Building page image\"\n    @docker build . -f Dockerfile \\\n        -t ${IMG}:dev \\\n        --build-arg MKDM_VERSION=${MKDM_VERSION}\n</code></pre> <p>Output</p> <pre><code>\u279c  data-engineering git:(cicd) \u2717 make build-page\nBuilding page image\n[+] Building 3.1s (8/8) FINISHED                                                                                                                                     docker:desktop-linux\n =&gt; [internal] load build definition from Dockerfile                                                                                                                                 0.0s\n =&gt; =&gt; transferring dockerfile: 260B                                                                                                                                                 0.0s\n =&gt; [internal] load metadata for docker.io/squidfunk/mkdocs-material:9.5.24                                                                                                          2.9s\n =&gt; [auth] squidfunk/mkdocs-material:pull token for registry-1.docker.io                                                                                                             0.0s\n =&gt; [internal] load .dockerignore                                                                                                                                                    0.0s\n =&gt; =&gt; transferring context: 2B                                                                                                                                                      0.0s\n =&gt; [internal] load build context                                                                                                                                                    0.1s\n =&gt; =&gt; transferring context: 12.87MB                                                                                                                                                 0.1s\n =&gt; CACHED [1/2] FROM docker.io/squidfunk/mkdocs-material:9.5.24@sha256:5358893a04dc6ed0e267ef1c0c06abc5d6b00d13dd0fee703c978ef98d56fd53                                             0.0s\n =&gt; [2/2] COPY . .                                                                                                                                                                   0.1s\n =&gt; exporting to image                                                                                                                                                               0.0s\n =&gt; =&gt; exporting layers                                                                                                                                                              0.0s\n =&gt; =&gt; writing image sha256:88fcc7e84428e26425f1c373f98cdfe37b6cc39a85768b47dad3536e86179aa1                                                                                         0.0s\n =&gt; =&gt; naming to docker.io/library/de-kchrs:dev                                                                                                                                      0.0s\n\nView build details: docker-desktop://dashboard/build/desktop-linux/desktop-linux/viwdpwiy4tlj9xrlmygm0lgrp\n</code></pre> <ul> <li>run command to run the image inside container and host it to <code>localhost</code></li> </ul> <pre><code>make dev\n</code></pre> <pre><code># Run dev server in docker\ndev:\n    @echo \"Starting dev server in a docker container\"\n    @docker run \\\n        --rm -d \\\n        --name ${IMG} \\\n        -p 127.0.0.1:${PORT}:8000 \\\n        -v ${PWD}:/docs \\\n        ${IMG}:dev\n</code></pre> <ul> <li>you can preview the pages in <code>localhost:8000</code></li> </ul>"},{"location":"projects/gh-pages-gh-actions/#deployment","title":"Deployment","text":"<p>CD is triggered when CI is success, then it will proceed to deployment, push to production.</p> <p>In this project, there will be 2 steps in deployment.</p> <ol> <li>It will ships all the code and push to branch <code>gh-pages</code>.</li> <li>It will handle for the page deployment and publish it to domain <code>karlchris.github.io/data-engineering</code></li> </ol>"},{"location":"projects/gh-pages-gh-actions/#1-push-to-gh-pages","title":"1. Push to <code>gh-pages</code>","text":"<ul> <li>create <code>cd.yaml</code></li> </ul> .github/workflows/cd.yaml<pre><code>name: cd\non:\n  push:\n    branches:\n      - master\npermissions:\n  contents: write\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - name: Configure Git Credentials\n        run: |\n          git config user.name github-actions[bot]\n          git config user.email 41898282+github-actions[bot]@users.noreply.github.com\n      - uses: actions/setup-python@v5\n        with:\n          python-version: 3.x\n      - run: echo \"cache_id=$(date --utc '+%V')\" &gt;&gt; $GITHUB_ENV \n      - uses: actions/cache@v4\n        with:\n          key: mkdocs-material-${{ env.cache_id }}\n          path: .cache\n          restore-keys: |\n            mkdocs-material-\n      - run: pip install mkdocs-material==9.5.24\n      - run: mkdocs gh-deploy --force\n</code></pre> <ul> <li>push this file to <code>master</code> branch.</li> <li>once it's pushed, you can view the execution as follow</li> </ul>"},{"location":"projects/gh-pages-gh-actions/#2-publish-to-domain","title":"2. Publish to domain","text":"<p>Basically, you don't need to configure yaml or code for this. Github Actions bot will handle it for you.</p> <p></p>"},{"location":"projects/gh-pages-gh-actions/#references","title":"References","text":"<ul> <li>CI/CD Explained</li> <li>Build CI/CD Pipeline with Github Actions</li> <li>Quickstart for Github Pages</li> </ul>"},{"location":"projects/great-expectations-bq/","title":"Automated Data Quality with Great Expectations and BigQuery","text":"<p>must have</p> <p>Before starting this project, you need to install some prerequisites in your laptop:</p> <ul> <li>Docker</li> <li>gcloud CLI</li> <li>GCP console</li> </ul> <p>If you haven't create any project in Google Cloud Platform, please follow this</p>"},{"location":"projects/great-expectations-bq/#great-expectations","title":"Great Expectations","text":"<p>Great Expectations(GX) is a open source tool to have automated data quality testing, it can integrate with a lot of other tools/platforms.</p>"},{"location":"projects/great-expectations-bq/#advantages","title":"Advantages","text":"<ul> <li>Get started quickly</li> </ul> <p>use GX OSS with tools you're familiar such as Python and Jupyter notebook</p> <ul> <li>Create a shared point of view</li> </ul> <p>using GX OSS gives everyone a shared toolset and starting point while still allowing each team their own flexibility and independence.</p> <ul> <li>Communicate better</li> </ul> <p>Data Docs make it easy to work with stakeholders by automatically rendering business-ready visualizations and test results expressed in plain language.</p> <ul> <li>Take action</li> </ul> <p>Your data systems never sleep, so your data quality processes can\u2019t either. Take proactive action 24/7 with GX OSS: it integrates with your orchestrator to enable automation that ensures your data never goes unchecked.</p> <ul> <li>It works with the tools we know and love</li> </ul> <p>Apache Airflow, Amazon S3, databricks, Google Cloud Platform, Jupyter Notebook, PostgreSQL, many more.</p>"},{"location":"projects/great-expectations-bq/#the-power-of-expectations","title":"The power of <code>Expectations</code>","text":"<p>It's using <code>Expectation</code> as how they are calling their test cases.</p> <ul> <li>An Expectation is a verifiable assertion about your data.</li> </ul> <p>Creating an Expectation offers you unparalleled flexibility and control compared with other ways of defining data quality tests.</p> <ul> <li>This intuitive approach is accessible to technical and nontechnical teams.</li> </ul> <p>Expectations automatically generate updated documentation on top of individual validation results, making sure everyone has visibility into your test suites and validation results.</p> <ul> <li>Expectations offer deeper insights than schema-focused validation</li> </ul> <p>more resilient to changing business and technical requirements than low-configuration options like anomaly detection.</p> <ul> <li>Expectations are reusable and can be auto-generated</li> </ul> <p>making it easy to deploy them across large amounts of data</p>"},{"location":"projects/great-expectations-bq/#build-great-expectations-docker-image","title":"Build Great Expectations docker image","text":"<ul> <li>clone the repo</li> </ul> <pre><code>git clone git@github.com:karlchris/great-expectations-bq.git\n</code></pre> <ul> <li>make sure you are in <code>data-engineering dir</code>, then run this command</li> </ul> <pre><code>make build-gx\n</code></pre> <p>Output:</p> <pre><code>\u279c  data-engineering git:(great-expectations) make build-gx\nBuilding great expectations image\n[+] Building 2.1s (10/10) FINISHED                               docker:desktop-linux\n =&gt; [internal] load build definition from Dockerfile                             0.0s\n =&gt; =&gt; transferring dockerfile: 315B                                             0.0s\n =&gt; [internal] load metadata for docker.io/library/python:3.9                    1.9s\n =&gt; [auth] library/python:pull token for registry-1.docker.io                    0.0s\n =&gt; [internal] load .dockerignore                                                0.0s\n =&gt; =&gt; transferring context: 2B                                                  0.0s\n =&gt; [1/4] FROM docker.io/library/python:3.9@sha256:1446afd121c574b13077f4137443  0.0s\n =&gt; [internal] load build context                                                0.0s\n =&gt; =&gt; transferring context: 4.19MB                                              0.0s\n =&gt; CACHED [2/4] WORKDIR /app                                                    0.0s\n =&gt; CACHED [3/4] RUN pip install great-expectations==0.18.15 &amp;&amp;     pip install  0.0s\n =&gt; [4/4] COPY . /app                                                            0.1s\n =&gt; exporting to image                                                           0.0s\n =&gt; =&gt; exporting layers                                                          0.0s\n =&gt; =&gt; writing image sha256:03928fb4d00f8d5a26079b81d01e4df586e116ee1e3f05fda18  0.0s\n =&gt; =&gt; naming to docker.io/library/gx                                            0.0s\n</code></pre> <ul> <li>If you see above output, then your great expectations image is alrady built.</li> </ul>"},{"location":"projects/great-expectations-bq/#run-great-expectations-container-interactively","title":"Run Great Expectations container interactively","text":"<ul> <li>run this command</li> </ul> <pre><code>make run-gx\n</code></pre> <p>Output:</p> <pre><code>\u279c  data-engineering git:(great-expectations) \u2717 make run-gx\nRunning great expectations in container\nroot@c6ee28c3a1f6:/app#\n</code></pre> <ul> <li>run this command to initiate the <code>DataContext</code>, connecting to <code>Datasource</code> and running some tests (tests are called as <code>Expectations</code>)</li> </ul> <pre><code>from ruamel import yaml\n\nimport great_expectations as ge\nfrom great_expectations.core.batch import RuntimeBatchRequest\n\nGCP_PROJECT_NAME = \"data-engineering-424915\"\nBIGQUERY_DATASET = \"data_eng\"\n\n\n# Instantiate project DataContext\ncontext = ge.get_context()\n\n# Configure Datasource\ndatasource_config = {\n    \"name\": \"my_bigquery_datasource\",\n    \"class_name\": \"Datasource\",\n    \"execution_engine\": {\n        \"class_name\": \"SqlAlchemyExecutionEngine\",\n        \"connection_string\": f\"bigquery://{GCP_PROJECT_NAME}/{BIGQUERY_DATASET}\",\n    },\n    \"data_connectors\": {\n        \"default_runtime_data_connector_name\": {\n            \"class_name\": \"RuntimeDataConnector\",\n            \"batch_identifiers\": [\"default_identifier_name\"],\n        },\n        \"default_inferred_data_connector_name\": {\n            \"class_name\": \"InferredAssetSqlDataConnector\",\n            \"include_schema_name\": True,\n        },\n    },\n}\ncontext.test_yaml_config(yaml.dump(datasource_config))\n\n# Save Datasource configuration to DataContext\ncontext.add_datasource(**datasource_config)\n\n# Test new Datasource\nbatch_request = RuntimeBatchRequest(\n    datasource_name=\"my_bigquery_datasource\",\n    data_connector_name=\"default_runtime_data_connector_name\",\n    data_asset_name=\"table_sales\",  # this can be anything that identifies this data\n    runtime_parameters={\"query\": \"SELECT * from data_eng.table_sales LIMIT 10\"},\n    batch_identifiers={\"default_identifier_name\": \"default_identifier\"},\n)\n\ncontext.add_or_update_expectation_suite(\n    expectation_suite_name=\"test_suite\"\n)\nvalidator = context.get_validator(\n    batch_request=batch_request, expectation_suite_name=\"test_suite\"\n)\nprint(validator.head())\n\n# Data Quality checks\nvalidator.expect_column_values_to_not_be_null(\"quantity\")\n\nvalidator.expect_column_values_to_not_be_null(\"product\")\nvalidator.expect_column_values_to_be_in_set(\n    \"product\",\n    [\"apple\", \"pear\", \"banana\",],\n)\n\nvalidator.expect_column_values_to_not_be_null(\"price\")\nvalidator.expect_column_values_to_be_between(\n    \"price\", min_value=0, max_value=None,\n)\nvalidator.save_expectation_suite(discard_failed_expectations=False)\n\n# Run checkpoint\ncheckpoint = context.add_or_update_checkpoint(\n    name=\"my_quickstart_checkpoint\",\n    validator=validator,\n)\ncheckpoint_result = checkpoint.run()\ncontext.view_validation_result(checkpoint_result)\n</code></pre> <p>you need to run python script above using</p> <pre><code>python run.py\n</code></pre> <p>Output:</p> <pre><code>root@9e909c33e977:/app# python run.py\nAttempting to instantiate class from config...\n        Instantiating as a Datasource, since class_name is Datasource\n        Successfully instantiated Datasource\n\n\nExecutionEngine class name: SqlAlchemyExecutionEngine\nData Connectors:\n        default_inferred_data_connector_name : InferredAssetSqlDataConnector\n\n        Available data_asset_names (3 of 10):\n                data_eng.gx_temp_29379392 (1 of 1): [{}]\n                data_eng.gx_temp_31f38543 (1 of 1): [{}]\n                data_eng.gx_temp_78a977d7 (1 of 1): [{}]\n\n        Unmatched data_references (0 of 0):[]\n\n        default_runtime_data_connector_name:RuntimeDataConnector\n\n        Available data_asset_names (0 of 0):\n                Note : RuntimeDataConnector will not have data_asset_names until they are passed in through RuntimeBatchRequest\n\n        Unmatched data_references (0 of 0): []\n\nCalculating Metrics: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  1.61it/s]\n  product  quantity  price                      date\n0   apple        10    1.0 2023-01-01 00:00:00+00:00\n1  banana         7    3.0 2023-01-01 00:00:00+00:00\n2    pear         5    2.0 2023-01-01 00:00:00+00:00\nCalculating Metrics: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:02&lt;00:00,  3.30it/s]\nCalculating Metrics: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:02&lt;00:00,  2.69it/s]\nCalculating Metrics: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:03&lt;00:00,  3.63it/s]\nCalculating Metrics: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:02&lt;00:00,  2.80it/s]\nCalculating Metrics: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:02&lt;00:00,  4.63it/s]\nCalculating Metrics: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 34/34 [00:05&lt;00:00,  6.17it/s]\n</code></pre> <ul> <li>all the <code>Expectations</code> have been built and tested, the result will be stored in <code>gx</code> dir.</li> </ul>"},{"location":"projects/great-expectations-bq/#data-docs-in-ui","title":"Data docs in UI","text":"<ul> <li>run this command</li> </ul> <pre><code>great_expectations docs build\n</code></pre> <p>Output:</p> <pre><code>root@12a2f87f7cb7:/app# great_expectations docs build\n\nThe following Data Docs sites will be built:\n\n - local_site: file:///app/gx/uncommitted/data_docs/local_site/index.html\n\nWould you like to proceed? [Y/n]: Y\n\nBuilding Data Docs...\n\nDone building Data Docs\n</code></pre> <ul> <li>the UI will be saved as HTML file in your local path <code>file:///app/gx/uncommitted/data_docs/local_site/index.html</code></li> </ul> <p>List of expectations suite</p> <p></p> <p>Test results</p> <p></p> <ul> <li>Done, you have wrapped data quality testing using Great Expectations to run within Docker.</li> </ul> <p>Tip</p> <p>Next step, you can schedule this to run in any server or cloud providers.</p> <p>Since it's already in Docker, it's easier to ship it in anywhere.</p> <p>Reference: Great Expectations: Connecting to BigQuery</p>"},{"location":"projects/kafka-ksql/","title":"Querying Streaming Data in Kafka using ksql","text":"<p>Let's discuss about how data streaming ingestion looks like and how we can query those data using ksql.</p>"},{"location":"projects/kafka-ksql/#what-is-kafka","title":"What is Kafka","text":"<p>Apache Kafka is an open-source, distributed, event streaming platform capable of handling large volumes of real-time data. You use Kafka to build real-time streaming applications.</p>"},{"location":"projects/kafka-ksql/#kafka-components","title":"Kafka Components","text":"<ul> <li>Kafka Cluster</li> </ul> <p>A Kafka cluster is a system that comprises of different brokers, topics, and their respective partitions. Data is written to the topic within the cluster and read by the cluster itself.</p> <ul> <li>Producers</li> </ul> <p>A producer sends or writes data/messages to the topic within the cluster. In order to store a huge amount of data, different producers within an application send data to the Kafka cluster.</p> <ul> <li>Consumers</li> </ul> <p>A consumer is the one that reads or consumes messages from the Kafka cluster. There can be several consumers consuming different types of data form the cluster. The beauty of Kafka is that each consumer knows from where it needs to consume the data.</p> <ul> <li>Brokers</li> </ul> <p>A Kafka server is known as a broker. A broker is a bridge between producers and consumers. If a producer wishes to write data to the cluster, it is sent to the Kafka server. All brokers lie within a Kafka cluster itself. Also, there can be multiple brokers.</p> <ul> <li>Topics</li> </ul> <p>It is a common name or a heading given to represent a similar type of data. In Apache Kafka, there can be multiple topics in a cluster. Each topic specifies different types of messages.</p> <ul> <li>Partitions</li> </ul> <p>The data or message is divided into small subparts, known as partitions. Each partition carries data within it having an offset value.</p>"},{"location":"projects/kafka-ksql/#the-capabilities","title":"The Capabilities","text":"<ul> <li>At the core of Kafka is the Kafka broker. A broker stores data in a durable way from clients in one or more topics that can be consumed by one or more clients. Kafka also provides several command-line tools that enable you to start and stop Kafka, create topics and more.</li> <li>Kafka provides security features such as data encryption between producers and consumers and brokers using SSL / TLS.</li> <li> <p>Support of the following Java APIs</p> <ul> <li>The Producer API that enables an application to send messages to Kafka.</li> <li>The Consumer API that enables an application to subscribe to one or more topics and process the stream of records produced to them.</li> <li>Kafka Connect, a component that you can use to stream data between Kafka and other data systems in a scalable and reliable way. It makes it simple to configure connectors to move data into and out of Kafka.</li> <li>The Streams API that enables applications to act as a stream processor, consuming an input stream from one or more topics and producing an output stream to one or more output topics, effectively transforming the input streams to output streams.</li> <li>The Admin API that provides the capability to create, inspect, delete, and manage topics, brokers, ACLs, and other Kafka objects.</li> </ul> </li> </ul>"},{"location":"projects/kafka-ksql/#challenges","title":"Challenges","text":"<ul> <li>Must have high-throughput to support high volume event streams such as real-time log aggregation.</li> <li>Requires the ability to gracefully deal with large data backlogs in order to support periodic data loads from offline systems.</li> <li>Must handle low-latency delivery for more traditional messaging use-cases.</li> </ul> <p>Info</p> <p>The goal for Kafka is to support partitioned, distributed, real-time processing feeds to create new, derived feeds. This motivated Kafka\u2019s partitioning and consumer model.</p> <p>Finally in cases where the stream is fed into other data systems for serving, it was important that the system would guarantee fault-tolerance in case of machine failures.</p>"},{"location":"projects/kafka-ksql/#confluent-and-kafka","title":"Confluent and Kafka","text":"<p>Confluent is a commercial, global corporation that specializes in providing businesses with real-time access to data. Confluent was founded by the creators of Kafka, and its product line includes proprietary products based on open-source Kafka.</p> <p></p> <p>At the core of Confluent Platform is Kafka, the most popular open source distributed streaming platform. Kafka enables you to:</p> <ul> <li>Publish and subscribe to streams of records</li> <li>Store streams of records in a fault tolerant way</li> <li>Process streams of records</li> </ul> <p>Each Confluent Platform release includes the latest release of Kafka and additional tools and services that make it easier to build and manage an event streaming platform. Confluent Platform provides community and commercially licensed features such as Schema Registry, Cluster Linking, a REST Proxy, 100+ pre-built Kafka connectors, and ksqlDB.</p>"},{"location":"projects/kafka-ksql/#use-cases","title":"Use Cases","text":"<p>Consider an application that uses Kafka topics as a backend to store and retrieve posts, likes, and comments from a popular social media site. The application incorporates producers and consumers that subscribe to those Kafka topics.</p> <ul> <li>When a user of the application publishes a post, likes something, or comments, the Kafka producer code in the application sends that data to the associated topic.</li> <li>When the user navigates to a particular page in the application, a Kafka consumer reads from the associated backend topic and the application renders data on the user\u2019s device.</li> </ul>"},{"location":"projects/kafka-ksql/#kafka-streams","title":"Kafka Streams","text":"<p>Kafka Streams is a client library for building applications and microservices, where the input and output data are stored in an Apache Kafka\u00ae cluster. It combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka\u2019s server-side cluster technology.</p>"},{"location":"projects/kafka-ksql/#ksqldb-for-querying","title":"ksqlDB for Querying","text":"<p>ksqlDB is the streaming SQL engine for Kafka that you can use to perform stream processing tasks using SQL statements.</p> <p></p> <p>Some of its features:</p> <ul> <li>Create real-time value by processing data in motion rather than data at rest.</li> <li>Simplify your stream processing architecture.</li> <li>Start building real-time applications with simple SQL syntax, as below example.</li> </ul> <pre><code>CREATE TABLE activePromotions AS\n    SELECT rideId,\n            qualityPromotion(distanceToDst) AS promotion\n    FROM locations\n    GROUP BY rideId\n    EMIT CHANGES\n</code></pre>"},{"location":"projects/kafka-ksql/#quick-start","title":"Quick Start","text":"<p>Use this quick start to get up and running locally with Confluent Platform and its main components using Docker containers.</p> <p>In this quick start, you will create Apache Kafka topics, use Kafka Connect to generate mock data to those topics, and create ksqlDB streaming queries on those topics. You then go to Confluent Control Center to monitor and analyze the event streaming queries. When you finish, you\u2019ll have a real-time app that consumes and processes data streams by using familiar SQL statements.</p> <p>must have</p> <p>Before starting this project, you need to install some prerequisites in your laptop:</p> <ul> <li>Docker</li> </ul> <p>First, you need to clone the repo.</p> <pre><code>git clone git@github.com:karlchris/kafka-docker.git\n</code></pre> <p>then, run below command to initialize the apps and its resources.</p> <pre><code>make up\n</code></pre> <p>This will launch several containers.</p> <pre><code>(base) \u279c  kafka-docker git:(main) dpsa\nCONTAINER ID   IMAGE                                             COMMAND                  CREATED             STATUS             PORTS                                            NAMES\ncb4f8fb1d3c5   confluentinc/ksqldb-examples:7.6.1                \"bash /root/datagen_\u2026\"   About an hour ago   Up About an hour                                                    ksql-datagen\nc132b535cfed   confluentinc/cp-ksqldb-cli:7.6.1                  \"/bin/sh\"                About an hour ago   Up About an hour                                                    ksqldb-cli\n515344724de1   confluentinc/cp-enterprise-control-center:7.6.1   \"/etc/confluent/dock\u2026\"   About an hour ago   Up About an hour   0.0.0.0:9021-&gt;9021/tcp                           control-center\n1f3a2ce862b1   confluentinc/cp-ksqldb-server:7.6.1               \"/etc/confluent/dock\u2026\"   About an hour ago   Up About an hour   0.0.0.0:8088-&gt;8088/tcp                           ksqldb-server\nd0d7b2abe0e4   cnfldemos/cp-server-connect-datagen:0.6.4-7.6.0   \"/etc/confluent/dock\u2026\"   About an hour ago   Up About an hour   0.0.0.0:8083-&gt;8083/tcp, 9092/tcp                 connect\n915a1c8552e3   confluentinc/cp-kafka-rest:7.6.1                  \"/etc/confluent/dock\u2026\"   About an hour ago   Up About an hour   0.0.0.0:8082-&gt;8082/tcp                           rest-proxy\n8df2fc0e9c7a   confluentinc/cp-schema-registry:7.6.1             \"/etc/confluent/dock\u2026\"   About an hour ago   Up About an hour   0.0.0.0:8081-&gt;8081/tcp                           schema-registry\n640b67c01686   provectuslabs/kafka-ui:latest                     \"/bin/sh -c 'java --\u2026\"   About an hour ago   Up About an hour   0.0.0.0:8080-&gt;8080/tcp                           kafka-ui\nbee8cb1574bc   confluentinc/cp-kafka:7.6.1                       \"/etc/confluent/dock\u2026\"   About an hour ago   Up About an hour   0.0.0.0:9092-&gt;9092/tcp, 0.0.0.0:9101-&gt;9101/tcp   broker\n</code></pre>"},{"location":"projects/kafka-ksql/#1-create-kafka-topics","title":"1. Create Kafka Topics","text":"<ul> <li>Open <code>http://localhost:9021</code></li> <li>Click controlcenter.cluster</li> </ul> <p>Cluster overview</p> <p></p> <ul> <li>In navigation menu, click Topics. Then, Create topic <code>pageviews</code> and <code>users</code>, rest click Create with defaults</li> </ul> <p></p>"},{"location":"projects/kafka-ksql/#2-generate-mock-data","title":"2. Generate Mock Data","text":"<ul> <li>In navigation menu click Connect.</li> <li>Click <code>connect-default</code> cluster in the Connect cluster list.</li> <li>Click Add connector.</li> <li>Select <code>DatagenConnector</code> tile.</li> <li>In the Name field, enter <code>datagen-pageviews</code> as the name of connector.</li> <li>Enter the following configuration values in the following sections:</li> </ul> <p>Common section:</p> <p><code>Key converter class</code>: <code>org.apache.kafka.connect.storage.StringConverter</code></p> <p>General section:</p> <p><code>kafka.topic</code>: Choose <code>pageviews</code> from the dropdown menu <code>max.interval</code>: <code>100</code> <code>quickstart</code>: <code>pageviews</code></p> <ul> <li>Click Next to review the configurations, then click Launch.</li> </ul> datagen-pageviews<pre><code>{\n  \"name\": \"datagen-pageviews\",\n  \"config\": {\n    \"name\": \"datagen-pageviews\",\n    \"connector.class\": \"io.confluent.kafka.connect.datagen.DatagenConnector\",\n    \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n    \"kafka.topic\": \"pageviews\",\n    \"max.interval\": \"100\",\n    \"quickstart\": \"pageviews\"\n  }\n}\n</code></pre> <ul> <li>Repeat above steps to create <code>datagen-users</code> for users mock data with below configurations.</li> </ul> datagen-users<pre><code>{\n  \"name\": \"datagen-users\",\n  \"config\": {\n    \"name\": \"datagen-users\",\n    \"connector.class\": \"io.confluent.kafka.connect.datagen.DatagenConnector\",\n    \"key.converter\": \"org.apache.kafka.connect.storage.StringConverter\",\n    \"kafka.topic\": \"users\",\n    \"max.interval\": \"1000\",\n    \"quickstart\": \"users\"\n  }\n}\n</code></pre> <p></p>"},{"location":"projects/kafka-ksql/#3-create-a-stream-and-table-using-sql-statements","title":"3. Create a Stream and Table using SQL statements","text":"<p>Note</p> <p>A stream is a an immutable, append-only collection that represents a series of historical facts, or events.</p> <p>A table is a mutable collection that models change over time. It uses row keys to display the most recent data for each key.</p> <ul> <li>In the navigation menu, click ksqlDB.</li> <li>Click <code>ksqldb1</code> cluster to open ksqldb1 page.</li> <li>Run this SQL statements</li> </ul> create_pageviews_stream.sql<pre><code>CREATE STREAM pageviews_stream\n  WITH (KAFKA_TOPIC='pageviews', VALUE_FORMAT='AVRO');\n</code></pre> <p>Output</p> <pre><code>{\n  \"@type\": \"currentStatus\",\n  \"statementText\": \"CREATE STREAM PAGEVIEWS_STREAM (VIEWTIME BIGINT, USERID STRING, PAGEID STRING) WITH (CLEANUP_POLICY='delete', KAFKA_TOPIC='pageviews', KEY_FORMAT='KAFKA', VALUE_FORMAT='AVRO');\",\n  \"commandId\": \"stream/`PAGEVIEWS_STREAM`/create\",\n  \"commandStatus\": {\n    \"status\": \"SUCCESS\",\n    \"message\": \"Stream created\",\n    \"queryId\": null\n  },\n  \"commandSequenceNumber\": 2,\n  \"warnings\": [\n\n  ]\n}\n</code></pre> view_pageviews_stream.sql<pre><code>SELECT * FROM pageviews_stream EMIT CHANGES;\n</code></pre> <p></p> <ul> <li>you can click Stop to end the SELECT query.</li> </ul> <p>Tip</p> <p>Stopping the SELECT query doesn't stop data movement through the stream.</p> create_users_table.sql<pre><code>CREATE TABLE users_table (id VARCHAR PRIMARY KEY)\n  WITH (KAFKA_TOPIC='users', VALUE_FORMAT='AVRO');\n</code></pre> <p>Output</p> <pre><code>{\n  \"@type\": \"currentStatus\",\n  \"statementText\": \"CREATE TABLE USERS_TABLE (ID STRING PRIMARY KEY, REGISTERTIME BIGINT, USERID STRING, REGIONID STRING, GENDER STRING) WITH (CLEANUP_POLICY='compact', KAFKA_TOPIC='users', KEY_FORMAT='KAFKA', VALUE_FORMAT='AVRO');\",\n  \"commandId\": \"table/`USERS_TABLE`/create\",\n  \"commandStatus\": {\n    \"status\": \"SUCCESS\",\n    \"message\": \"Table created\",\n    \"queryId\": null\n  },\n  \"commandSequenceNumber\": 4,\n  \"warnings\": [\n\n  ]\n}\n</code></pre> <p>Warning</p> <p>A table requires you to specify a PRIMARY KEY when you register it.</p>"},{"location":"projects/kafka-ksql/#inspect-the-schemas-of-your-stream-and-table","title":"Inspect the schemas of your stream and table","text":"<ul> <li>Click Streams to see the currently registered streams. In the list, click PAGEVIEWS_STREAM to see details.</li> </ul> <ul> <li>Click Tables to see the currently registered tables. In the list, click USERS_TABLE to see details.</li> </ul>"},{"location":"projects/kafka-ksql/#create-queries-to-process-data","title":"Create queries to process data","text":"<p>Different kinds of queries:</p> <ul> <li>Transient query</li> </ul> <p>a non-persistent, client-side query that you terminate manually or with a LIMIT clause. A transient query doesn\u2019t create a new topic.</p> <ul> <li>Persistent query</li> </ul> <p>a server-side query that outputs a new stream or table that\u2019s backed by a new topic. It runs until you issue the <code>TERMINATE</code> statement.</p> <ul> <li>Push query</li> </ul> <p>A query that produces results continuously to a subscription. The syntax for a push query uses the <code>EMIT CHANGES</code> keyword. Push queries can be transient or persistent.</p> <ul> <li>Pull query</li> </ul> <p>A query that gets a result as of \u201cnow\u201d, like a query against a traditional relational database. A pull query runs once and returns the current state of a table. Pull queries are always transient.</p>"},{"location":"projects/kafka-ksql/#query-for-pageviews","title":"Query for pageviews","text":"<p>Run this query to view the pageID.</p> query-pageviews.sql<pre><code>SELECT pageid FROM pageviews_stream EMIT CHANGES LIMIT 3;\n</code></pre> <p></p>"},{"location":"projects/kafka-ksql/#joining-data-from-stream-and-table","title":"Joining data from Stream and Table","text":"<p>This join enriches pageview data with information about the user who viewed the page. The joined rows are written to a new sink topic, which has the same name as the new stream, by default.</p> <p>Tip</p> <p>You can specify the name of the sink topic by using the <code>KAFKA_TOPIC</code> keyword in a <code>WITH</code> clause.</p> <p>Run below query to create the joined stream.</p> <pre><code>CREATE STREAM user_pageviews\n  AS SELECT users_table.id AS userid, pageid, regionid, gender\n    FROM pageviews_stream\n    LEFT JOIN users_table ON pageviews_stream.userid = users_table.id\nEMIT CHANGES;\n</code></pre> <p>Output</p> <pre><code>{\n  \"@type\": \"currentStatus\",\n  \"statementText\": \"CREATE STREAM USER_PAGEVIEWS WITH (CLEANUP_POLICY='delete', KAFKA_TOPIC='USER_PAGEVIEWS', PARTITIONS=1, REPLICAS=1, RETENTION_MS=604800000) AS SELECT\\n  USERS_TABLE.ID USERID,\\n  PAGEVIEWS_STREAM.PAGEID PAGEID,\\n  USERS_TABLE.REGIONID REGIONID,\\n  USERS_TABLE.GENDER GENDER\\nFROM PAGEVIEWS_STREAM PAGEVIEWS_STREAM\\nLEFT OUTER JOIN USERS_TABLE USERS_TABLE ON ((PAGEVIEWS_STREAM.USERID = USERS_TABLE.ID))\\nEMIT CHANGES;\",\n  \"commandId\": \"stream/`USER_PAGEVIEWS`/create\",\n  \"commandStatus\": {\n    \"status\": \"SUCCESS\",\n    \"message\": \"Created query with ID CSAS_USER_PAGEVIEWS_5\",\n    \"queryId\": \"CSAS_USER_PAGEVIEWS_5\"\n  },\n  \"commandSequenceNumber\": 6,\n  \"warnings\": [\n\n  ]\n}\n</code></pre> <p>Note</p> <p>The query uses the <code>EMIT CHANGES</code> syntax, which indicates that this is a push query.</p> <p>A push query enables you to query a stream or table with a subscription to the results. It continues until you stop it.</p> <p>Click Stop to end the transient push query.</p>"},{"location":"projects/kafka-ksql/#filter-a-stream","title":"Filter a stream","text":"<p>You will create a stream, named <code>pageviews_region_like_89</code>, which is made of <code>user_pageviews</code> rows that have a <code>regionid</code> value that ends with <code>8</code> or <code>9</code>.</p> <pre><code>CREATE STREAM pageviews_region_like_89\n  WITH (KAFKA_TOPIC='pageviews_filtered_r8_r9', VALUE_FORMAT='AVRO')\n    AS SELECT * FROM user_pageviews\n    WHERE regionid LIKE '%_8' OR regionid LIKE '%_9'\nEMIT CHANGES;\n</code></pre> <p>Output</p> <pre><code>{\n  \"@type\": \"currentStatus\",\n  \"statementText\": \"CREATE STREAM PAGEVIEWS_REGION_LIKE_89 WITH (CLEANUP_POLICY='delete', KAFKA_TOPIC='pageviews_filtered_r8_r9', PARTITIONS=1, REPLICAS=1, RETENTION_MS=604800000, VALUE_FORMAT='AVRO') AS SELECT *\\nFROM USER_PAGEVIEWS USER_PAGEVIEWS\\nWHERE ((USER_PAGEVIEWS.REGIONID LIKE '%_8') OR (USER_PAGEVIEWS.REGIONID LIKE '%_9'))\\nEMIT CHANGES;\",\n  \"commandId\": \"stream/`PAGEVIEWS_REGION_LIKE_89`/create\",\n  \"commandStatus\": {\n    \"status\": \"SUCCESS\",\n    \"message\": \"Created query with ID CSAS_PAGEVIEWS_REGION_LIKE_89_7\",\n    \"queryId\": \"CSAS_PAGEVIEWS_REGION_LIKE_89_7\"\n  },\n  \"commandSequenceNumber\": 8,\n  \"warnings\": [\n\n  ]\n}\n</code></pre> <p>to view the data, run below query</p> <pre><code>SELECT * FROM pageviews_region_like_89 EMIT CHANGES;\n</code></pre> <p></p>"},{"location":"projects/kafka-ksql/#create-a-windowed-view","title":"Create a windowed view","text":"<p>You will create a table named <code>pageviews_per_region_89</code> that counts the number of pageviews from regions <code>8</code> and <code>9</code> in a tumbling window with a SIZE of 30 seconds.</p> <p>Info</p> <p>A tumbling window is a hopping window whose window size is equal to its advance interval. Since tumbling windows never overlap, a data record will belong to one and only one window.</p> <p>Reference: Windowing tumbling</p> <pre><code>CREATE TABLE pageviews_per_region_89 WITH (KEY_FORMAT='JSON')\n  AS SELECT userid, gender, regionid, COUNT(*) AS numviews\n    FROM pageviews_region_like_89\n    WINDOW TUMBLING (SIZE 30 SECOND)\n    GROUP BY gender, regionid, userid\n    HAVING COUNT(*) &gt; 1\nEMIT CHANGES;\n</code></pre> <p>Output</p> <pre><code>{\n  \"@type\": \"currentStatus\",\n  \"statementText\": \"CREATE TABLE PAGEVIEWS_PER_REGION_89 WITH (CLEANUP_POLICY='compact,delete', KAFKA_TOPIC='PAGEVIEWS_PER_REGION_89', KEY_FORMAT='JSON', PARTITIONS=1, REPLICAS=1, RETENTION_MS=604800000) AS SELECT\\n  PAGEVIEWS_REGION_LIKE_89.USERID USERID,\\n  PAGEVIEWS_REGION_LIKE_89.GENDER GENDER,\\n  PAGEVIEWS_REGION_LIKE_89.REGIONID REGIONID,\\n  COUNT(*) NUMVIEWS\\nFROM PAGEVIEWS_REGION_LIKE_89 PAGEVIEWS_REGION_LIKE_89\\nWINDOW TUMBLING ( SIZE 30 SECONDS ) \\nGROUP BY PAGEVIEWS_REGION_LIKE_89.GENDER, PAGEVIEWS_REGION_LIKE_89.REGIONID, PAGEVIEWS_REGION_LIKE_89.USERID\\nHAVING (COUNT(*) &gt; 1)\\nEMIT CHANGES;\",\n  \"commandId\": \"table/`PAGEVIEWS_PER_REGION_89`/create\",\n  \"commandStatus\": {\n    \"status\": \"SUCCESS\",\n    \"message\": \"Created query with ID CTAS_PAGEVIEWS_PER_REGION_89_9\",\n    \"queryId\": \"CTAS_PAGEVIEWS_PER_REGION_89_9\"\n  },\n  \"commandSequenceNumber\": 10,\n  \"warnings\": [\n\n  ]\n}\n</code></pre> <p>to view the data, run below query</p> <pre><code>SELECT * FROM pageviews_per_region_89 EMIT CHANGES;\n</code></pre> <p></p> <p>Tip</p> <p>the NUMVIEWS column shows the count of views in a 30-second window.</p>"},{"location":"projects/kafka-ksql/#snapshot-a-table-by-using-a-pull-query","title":"Snapshot a table by using a pull query","text":"<p>You can get the current state of a table by using a pull query, which returns rows for a specific key at the time you issue the query. A pull query runs once and terminates.</p> <p>In this step, you query the <code>pageviews_per_region_89</code> table for all rows that have <code>User_1</code> in <code>Region_9</code>.</p> <pre><code>SELECT * FROM pageviews_per_region_89\n  WHERE userid = 'User_1' AND gender='FEMALE' AND regionid='Region_9';\n</code></pre>"},{"location":"projects/kafka-ksql/#inspect-your-streams-and-tables","title":"Inspect your streams and tables","text":"<ul> <li>In the upper-right corner of the editor, the All available streams and tables panes shows all of the streams and tables that you can access.</li> </ul> <ul> <li>Click Persistent queries to inspect the streams and tables that you've created.</li> </ul> <p>Tip</p> <p>this page to check whether your queries are running, to explain a query, and to terminate running queries.</p> <p></p>"},{"location":"projects/kafka-ksql/#4-visualize-stream-topology","title":"4. Visualize stream topology","text":"<p>In the streaming application you\u2019ve built, events flow from the Datagen connectors into the <code>pageviews</code> and <code>users</code> topics.</p> <p>Rows are processed with a join and filtered, and in the final step, rows are aggregated in a table view of the streaming data.</p> <ul> <li>Click Flow to open Flow view.</li> </ul> <p></p>"},{"location":"projects/kafka-ksql/#5-clean-up","title":"5. Clean up","text":"<p>Run below command</p> <pre><code>make down\n</code></pre> <p>Output</p> <pre><code>(base) \u279c  kafka-docker git:(main) make down \ndocker compose down --rmi all\n[+] Running 19/19\n \u2714 Container ksql-datagen                                 Removed                                                                                                                                                               10.2s \n \u2714 Container rest-proxy                                   Removed                                                                                                                                                                0.7s \n \u2714 Container ksqldb-cli                                   Removed                                                                                                                                                                0.2s \n \u2714 Container control-center                               Removed                                                                                                                                                                1.7s \n \u2714 Container kafka-ui                                     Removed                                                                                                                                                                2.3s \n \u2714 Container ksqldb-server                                Removed                                                                                                                                                                2.1s \n \u2714 Container connect                                      Removed                                                                                                                                                                2.6s \n \u2714 Container schema-registry                              Removed                                                                                                                                                                0.7s \n \u2714 Container broker                                       Removed                                                                                                                                                                1.0s \n \u2714 Image cnfldemos/cp-server-connect-datagen:0.6.4-7.6.0  Removed                                                                                                                                                                1.1s \n \u2714 Image confluentinc/cp-ksqldb-server:7.6.1              Removed                                                                                                                                                                1.1s \n \u2714 Image confluentinc/cp-ksqldb-cli:7.6.1                 Removed                                                                                                                                                                1.2s \n \u2714 Image confluentinc/cp-schema-registry:7.6.1            Removed                                                                                                                                                                1.3s \n \u2714 Image confluentinc/ksqldb-examples:7.6.1               Removed                                                                                                                                                                0.1s \n \u2714 Image confluentinc/cp-enterprise-control-center:7.6.1  Removed                                                                                                                                                                1.9s \n \u2714 Image provectuslabs/kafka-ui:latest                    Removed                                                                                                                                                                0.4s \n \u2714 Image confluentinc/cp-kafka-rest:7.6.1                 Removed                                                                                                                                                                0.3s \n \u2714 Image confluentinc/cp-kafka:7.6.1                      Removed                                                                                                                                                                0.1s \n \u2714 Network kafka-docker_default                           Removed    \n</code></pre>"},{"location":"projects/kafka-ksql/#references","title":"References","text":"<ul> <li>Kafka Basics</li> <li>Apache Kafka Architecture</li> <li>Official Apache Kafka documentation</li> </ul>"},{"location":"projects/spark-docker/","title":"Running Spark using Docker Compose","text":"<p>For better following this exercise, you should clone the repo <code>spark-docker</code></p> <pre><code>git clone git@github.com:karlchris/spark-docker.git\n</code></pre>"},{"location":"projects/spark-docker/#spark-containers-and-components","title":"Spark Containers and Components","text":"<p>There are several Spark containers which we need to execute at once and connect them altogether.</p> docker-compose.yml<pre><code>services:\n  spark-master:\n    container_name: spark-master\n    build: .\n    image: spark-image\n    entrypoint: ['./entrypoint.sh', 'master']\n    healthcheck:\n      test: [ \"CMD\", \"curl\", \"-f\", \"http://localhost:8080\" ]\n      interval: 5s\n      timeout: 3s\n      retries: 3\n    volumes:\n      - ./scripts:/opt/spark/scripts\n      - ./data:/opt/spark/data\n      - spark-logs:/opt/spark/spark-events\n    env_file:\n      - .env.spark\n    ports:\n      - '8080:8080'\n      - '7077:7077'\n\n  spark-history-server:\n    container_name: spark-history\n    image: spark-image\n    entrypoint: ['./entrypoint.sh', 'history']\n    depends_on:\n      - spark-master\n    env_file:\n      - .env.spark\n    volumes:\n      - ./scripts:/opt/spark/scripts\n      - ./data:/opt/spark/data\n      - spark-logs:/opt/spark/spark-events\n    ports:\n      - '18080:18080'\n\n  spark-worker:\n    container_name: spark-worker\n    image: spark-image\n    entrypoint: ['./entrypoint.sh', 'worker']\n    depends_on:\n      - spark-master\n    env_file:\n      - .env.spark\n    volumes:\n      - ./scripts:/opt/spark/scripts\n      - ./data:/opt/spark/data\n      - spark-logs:/opt/spark/spark-events\n\nvolumes:\n  spark-logs:\n</code></pre>"},{"location":"projects/spark-docker/#spark-master","title":"Spark Master","text":"<p>It is the master control plane where it can define the workers, execute the scripts and sharing the resources among workers. We can access it through <code>localhost:8080</code> as the UI and it's the main controller of everything about this cluster. Whatever jobs you're submitting and it's still running, you can see the spark jobs logs from here. And, it's also having the main spark docker image, which we will build.</p> <p>Info</p> <p>The three different containers are coming fom the same docker image, but we call them differently.</p> <p>As to specify <code>master</code>, we need to call <code>./entrypoint.sh master</code></p>"},{"location":"projects/spark-docker/#spark-workers","title":"Spark Worker(s)","text":"<p>You can have 1 or more spark worker(s), it is where your spark code are actually being executed. After it's executed, the result are brought back to <code>spark master</code></p> <p>As the info above, this is coming from same docker image, but to specify it, you just need to call <code>./entrypoint.sh worker</code></p>"},{"location":"projects/spark-docker/#spark-history-server","title":"Spark History Server","text":"<p>As its name, it is where your spark jobs have been done executed, you can see the detail of each jobs in this view.</p> <p>Info</p> <p>You can access the UI through <code>localhost:18080</code></p> <p>Example of Spark job view</p> <p></p>"},{"location":"projects/spark-docker/#why-docker-compose","title":"Why Docker Compose","text":"<p>Docker compose is a tool for defining and running multi-container applications. It is the key to unlocking a streamlined and efficient development and deployment experience.</p> <p>Compose simplifies the control of your entire application stack, making it easy to manage services, networks, and volumes in a single, comprehensible YAML configuration file.</p> <p>Then, with a single command, you create and start all the services from your configuration file.</p>"},{"location":"projects/spark-docker/#common-use-case-of-docker-compose","title":"Common use case of Docker compose","text":"<ul> <li>Development environments</li> </ul> <p>When you're developing software, the ability to run an application in an isolated environment and interact with it is crucial. The Compose command line tool can be used to create the environment and interact with it.</p> <p>with just single command <code>docker compose up</code> you can bring up all the containers and orchestrate the application.</p> <p>and single command <code>docker compose down</code> to bring down all the alive containers so it's not exhausting the resources.</p> <ul> <li>Automated testing environments</li> </ul> <p>An important part of any Continuous Deployment or Continuous Integration process is the automated test suite. Automated end-to-end testing requires an environment in which to run tests. Compose provides a convenient way to create and destroy isolated testing environments for your test suite.</p> <p>Code example:</p> <pre><code>docker compose up -d\n./run_tests\ndocker compose down\n</code></pre> <ul> <li>Single host deployments</li> </ul> <p>Compose has traditionally been focused on development and testing workflows, but with each release we're making progress on more production-oriented features.</p>"},{"location":"projects/spark-docker/#spark-docker-image","title":"Spark Docker Image","text":"<p>The base of Docker Image is <code>python:3.10-bullseye</code></p> <pre><code>FROM python:3.10-bullseye as spark-base\n</code></pre> <p>Installing tools that OS will need, such as sudo, openjdk, etc</p> <pre><code>RUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends \\\n      sudo \\\n      curl \\\n      vim \\\n      unzip \\\n      rsync \\\n      openjdk-11-jdk \\\n      build-essential \\\n      software-properties-common \\\n      ssh &amp;&amp; \\\n    apt-get clean &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n</code></pre> <p>We setup some directories and environment variables</p> <pre><code># Optional env variables\nENV SPARK_HOME=${SPARK_HOME:-\"/opt/spark\"}\nENV HADOOP_HOME=${HADOOP_HOME:-\"/opt/hadoop\"}\n\nRUN mkdir -p ${HADOOP_HOME} &amp;&amp; mkdir -p ${SPARK_HOME}\nWORKDIR ${SPARK_HOME}\n\nENV SPARK_VERSION=3.5.1\n</code></pre> <p>Downloading <code>spark</code></p> <pre><code># Download spark\nRUN mkdir -p ${SPARK_HOME} \\\n    &amp;&amp; curl https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz -o spark-${SPARK_VERSION}-bin-hadoop3.tgz \\\n    &amp;&amp; tar xvzf spark-${SPARK_VERSION}-bin-hadoop3.tgz --directory /opt/spark --strip-components 1 \\\n    &amp;&amp; rm -rf spark-${SPARK_VERSION}-bin-hadoop3.tgz\n</code></pre> <p>Installing python dependencies in <code>requirements.txt</code></p> requirements.txt<pre><code>pyarrow==16.1.0\npyspark==3.5.1\n</code></pre> <pre><code># Install python deps\nCOPY requirements.txt .\nRUN pip3 install -r requirements.txt\n</code></pre> <p>Set some environment variables for Spark</p> <pre><code>ENV PATH=\"/opt/spark/sbin:/opt/spark/bin:${PATH}\"\nENV SPARK_HOME=\"/opt/spark\"\nENV SPARK_MASTER=\"spark://spark-master:7077\"\nENV SPARK_MASTER_HOST spark-master\nENV SPARK_MASTER_PORT 7077\nENV PYSPARK_PYTHON python3\n</code></pre> <p>Copy the spark defaults configuration</p> conf/spark-defaults.conf<pre><code>spark.master                           spark://spark-master:7077\nspark.eventLog.enabled                 true\nspark.eventLog.dir                     /opt/spark/spark-events\nspark.history.fs.logDirectory          /opt/spark/spark-events\n</code></pre> <pre><code>COPY conf/spark-defaults.conf \"$SPARK_HOME/conf\"\n</code></pre> <p>Here, we\u2019re setting that the spark master will be a standalone cluster with the master on port <code>7077</code>. We\u2019re also enabling the <code>eventLog</code>, <code>eventLog</code> directory and history filesystem logDirectory.</p> <p>These three settings are required to use the <code>Spark history server</code>. The setting <code>spark.eventLog.dir</code> is the base directory where the events are logged. The <code>spark.history.fs.logDirectory</code> is the directory from which the filesystem history provider will load the logs.</p> <p>Make the binaries and scripts executable and set <code>PYTHONPATH</code> environment variable to use the python version that comes with spark</p> <pre><code>RUN chmod u+x /opt/spark/sbin/* &amp;&amp; \\\n    chmod u+x /opt/spark/bin/*\n\nENV PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH\n</code></pre> <p>Finally, copy the entrypoint script and set it as image entrypoint</p> entrypoint.sh<pre><code>#!/bin/bash\n\nSPARK_WORKLOAD=$1\n\necho \"SPARK_WORKLOAD: $SPARK_WORKLOAD\"\n\nif [ \"$SPARK_WORKLOAD\" == \"master\" ];\nthen\n  start-master.sh -p 7077\nelif [ \"$SPARK_WORKLOAD\" == \"worker\" ];\nthen\n  start-worker.sh spark://spark-master:7077\nelif [ \"$SPARK_WORKLOAD\" == \"history\" ]\nthen\n  start-history-server.sh\nfi\n</code></pre> <pre><code>COPY entrypoint.sh .\nRUN chmod u+x /opt/spark/entrypoint.sh\n\nENTRYPOINT [\"./entrypoint.sh\"]\nCMD [ \"bash\" ]\n</code></pre> <p>Tip</p> <p>this <code>entrypoint.sh</code> is where we define to execute the spark code either <code>master</code>, <code>worker</code> or <code>history server</code></p>"},{"location":"projects/spark-docker/#build-and-up-cluster","title":"Build and Up Cluster","text":"<p>To build up the image and bring up all the containers, you can just execute this command</p> <pre><code>make up\n</code></pre> <p>Output</p> <pre><code>(base) \u279c  spark-docker git:(main) make up  \ndocker compose up -d --build\n[+] Running 2/2\n \u2718 spark-worker Error         pull access denied for spark-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied                                                                                                                                      2.8s \n \u2718 spark-history-server Error pull access denied for spark-image, repository does not exist or may require 'docker login': denied: requested access to the resource is denied                                                                                                                                      2.7s \n[+] Building 148.9s (17/17) FINISHED                                                                                                                                                                                                                                                               docker:desktop-linux\n =&gt; [spark-master internal] load build definition from Dockerfile                                                                                                                                                                                                                                                  0.0s\n =&gt; =&gt; transferring dockerfile: 1.63kB                                                                                                                                                                                                                                                                             0.0s\n =&gt; [spark-master internal] load metadata for docker.io/library/python:3.10-bullseye                                                                                                                                                                                                                               2.8s\n =&gt; [spark-master auth] library/python:pull token for registry-1.docker.io                                                                                                                                                                                                                                         0.0s\n =&gt; [spark-master internal] load .dockerignore                                                                                                                                                                                                                                                                     0.0s\n =&gt; =&gt; transferring context: 2B                                                                                                                                                                                                                                                                                    0.0s\n =&gt; [spark-master spark-base 1/5] FROM docker.io/library/python:3.10-bullseye@sha256:a4d9982b391b91d6ffaae6156f2b55ed769d854682d6611c3805c436b1920b72                                                                                                                                                             33.7s\n =&gt; =&gt; resolve docker.io/library/python:3.10-bullseye@sha256:a4d9982b391b91d6ffaae6156f2b55ed769d854682d6611c3805c436b1920b72                                                                                                                                                                                      0.0s\n =&gt; =&gt; sha256:a4d9982b391b91d6ffaae6156f2b55ed769d854682d6611c3805c436b1920b72 1.65kB / 1.65kB                                                                                                                                                                                                                     0.0s\n =&gt; =&gt; sha256:d344660ab716c56ed8760cd05fd6f09d201a7398129ac252241dcae6a8edce6c 7.95kB / 7.95kB                                                                                                                                                                                                                     0.0s\n =&gt; =&gt; sha256:bbf3ed169027034dd58cdf87bf07f4506ecf971f22a8c5b8167832d2f69c3a6e 2.01kB / 2.01kB                                                                                                                                                                                                                     0.0s\n =&gt; =&gt; sha256:f975e52008de385eb513258b4912477b214cddf1c8e87877f85028d940bfcdae 53.74MB / 53.74MB                                                                                                                                                                                                                  23.9s\n =&gt; =&gt; sha256:859b5bb8f5d471015f3add7e778bc507fc4a6f1fce8561c2b0a336734a55a365 15.75MB / 15.75MB                                                                                                                                                                                                                   4.0s\n =&gt; =&gt; sha256:28a7ca076c1ea04622ddf9f43ff2f138f6c50a40118747a45d2618cc64591d6b 54.70MB / 54.70MB                                                                                                                                                                                                                  20.5s\n =&gt; =&gt; sha256:315e6e292ef6ca0113f94dbf806ecbea714420b97b385d4138e3cb84616995e3 189.94MB / 189.94MB                                                                                                                                                                                                                30.2s\n =&gt; =&gt; sha256:fc2e3a8abd9b020e02a6952b9f866f08fd333baf06330ccbb22a1f28ebd1e972 6.41MB / 6.41MB                                                                                                                                                                                                                    22.4s\n =&gt; =&gt; sha256:3e954bdeb399238cc34f825d4f940e4fe2d4fb1ba1ceecc503d5a0cd03932696 17.22MB / 17.22MB                                                                                                                                                                                                                  25.1s\n =&gt; =&gt; extracting sha256:f975e52008de385eb513258b4912477b214cddf1c8e87877f85028d940bfcdae                                                                                                                                                                                                                          1.2s\n =&gt; =&gt; sha256:813820ddecbd5e83f239c09103deb809036a470e0976d4d044f3db994fc70741 245B / 245B                                                                                                                                                                                                                        24.4s\n =&gt; =&gt; sha256:e6b6c385c0e4d7394694a178278f2fcf3ce8af2e986c85d3e369882020ad96b6 3.08MB / 3.08MB                                                                                                                                                                                                                    25.9s\n =&gt; =&gt; extracting sha256:859b5bb8f5d471015f3add7e778bc507fc4a6f1fce8561c2b0a336734a55a365                                                                                                                                                                                                                          0.2s\n =&gt; =&gt; extracting sha256:28a7ca076c1ea04622ddf9f43ff2f138f6c50a40118747a45d2618cc64591d6b                                                                                                                                                                                                                          1.2s\n =&gt; =&gt; extracting sha256:315e6e292ef6ca0113f94dbf806ecbea714420b97b385d4138e3cb84616995e3                                                                                                                                                                                                                          2.9s\n =&gt; =&gt; extracting sha256:fc2e3a8abd9b020e02a6952b9f866f08fd333baf06330ccbb22a1f28ebd1e972                                                                                                                                                                                                                          0.1s\n =&gt; =&gt; extracting sha256:3e954bdeb399238cc34f825d4f940e4fe2d4fb1ba1ceecc503d5a0cd03932696                                                                                                                                                                                                                          0.3s\n =&gt; =&gt; extracting sha256:813820ddecbd5e83f239c09103deb809036a470e0976d4d044f3db994fc70741                                                                                                                                                                                                                          0.0s\n =&gt; =&gt; extracting sha256:e6b6c385c0e4d7394694a178278f2fcf3ce8af2e986c85d3e369882020ad96b6                                                                                                                                                                                                                          0.1s\n =&gt; [spark-master internal] load build context                                                                                                                                                                                                                                                                     0.0s\n =&gt; =&gt; transferring context: 984B                                                                                                                                                                                                                                                                                  0.0s\n =&gt; [spark-master spark-base 2/5] RUN apt-get update &amp;&amp;     apt-get install -y --no-install-recommends       sudo       curl       vim       unzip       rsync       openjdk-11-jdk       build-essential       software-properties-common       ssh &amp;&amp;     apt-get clean &amp;&amp;     rm -rf /var/lib/apt/lists/*      25.0s\n =&gt; [spark-master spark-base 3/5] RUN mkdir -p /opt/hadoop &amp;&amp; mkdir -p /opt/spark                                                                                                                                                                                                                                  0.1s \n =&gt; [spark-master spark-base 4/5] WORKDIR /opt/spark                                                                                                                                                                                                                                                               0.0s \n =&gt; [spark-master spark-base 5/5] RUN mkdir -p /opt/spark     &amp;&amp; curl https://dlcdn.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz -o spark-3.5.1-bin-hadoop3.tgz     &amp;&amp; tar xvzf spark-3.5.1-bin-hadoop3.tgz --directory /opt/spark --strip-components 1     &amp;&amp; rm -rf spark-3.5.1-bin-hadoop3.tgz     37.8s \n =&gt; [spark-master pyspark 1/6] COPY requirements.txt .                                                                                                                                                                                                                                                             0.0s \n =&gt; [spark-master pyspark 2/6] RUN pip3 install -r requirements.txt                                                                                                                                                                                                                                               47.5s \n =&gt; [spark-master pyspark 3/6] COPY conf/spark-defaults.conf /opt/spark/conf                                                                                                                                                                                                                                       0.0s \n =&gt; [spark-master pyspark 4/6] RUN chmod u+x /opt/spark/sbin/* &amp;&amp;     chmod u+x /opt/spark/bin/*                                                                                                                                                                                                                   0.1s \n =&gt; [spark-master pyspark 5/6] COPY entrypoint.sh .                                                                                                                                                                                                                                                                0.0s \n =&gt; [spark-master pyspark 6/6] RUN chmod u+x /opt/spark/entrypoint.sh                                                                                                                                                                                                                                              0.1s \n =&gt; [spark-master] exporting to image                                                                                                                                                                                                                                                                              1.5s \n =&gt; =&gt; exporting layers                                                                                                                                                                                                                                                                                            1.5s \n =&gt; =&gt; writing image sha256:ca5932db9f24372402aa4da122aa71c98abd27140c0738bc68418e2c12bbd4f7                                                                                                                                                                                                                       0.0s\n =&gt; =&gt; naming to docker.io/library/spark-image                                                                                                                                                                                                                                                                     0.0s\n[+] Running 5/5\n \u2714 Network spark-docker_default      Created                                                                                                                                                                                                                                                                       0.0s \n \u2714 Volume \"spark-docker_spark-logs\"  Created                                                                                                                                                                                                                                                                       0.0s \n \u2714 Container spark-master            Started                                                                                                                                                                                                                                                                       0.1s \n \u2714 Container spark-worker            Started                                                                                                                                                                                                                                                                       0.2s \n \u2714 Container spark-history           Started    \n</code></pre> <p>You can check the running containers by running this command</p> <pre><code>docker ps\n</code></pre> <p>Output</p> <pre><code>(base) \u279c  spark-docker git:(main) docker ps   \nFound existing alias for \"docker ps\". You should use: \"dps\"\nCONTAINER ID   IMAGE          COMMAND                  CREATED          STATUS                    PORTS                                            NAMES\n93e611f3161b   spark-image    \"./entrypoint.sh his\u2026\"   18 seconds ago   Up 17 seconds             0.0.0.0:18080-&gt;18080/tcp                         spark-history\n31ca78964798   spark-image    \"./entrypoint.sh wor\u2026\"   18 seconds ago   Up 17 seconds                                                              spark-worker\n73ffa359696b   spark-image    \"./entrypoint.sh mas\u2026\"   18 seconds ago   Up 17 seconds (healthy)   0.0.0.0:7077-&gt;7077/tcp, 0.0.0.0:8080-&gt;8080/tcp   spark-master\n</code></pre>"},{"location":"projects/spark-docker/#submit-jobs","title":"Submit Jobs","text":"<p>To submit the jobs, you must enter the spark master runtime container.</p> <pre><code>make dev\n</code></pre> <p>Output</p> <pre><code>(base) \u279c  spark-docker git:(main) make dev\ndocker exec -it spark-master bash\nroot@73ffa359696b:/opt/spark# \n</code></pre>"},{"location":"projects/spark-docker/#running-ml-models","title":"Running ML models","text":"<p>I have sample script for ML prediction here.</p> <p></p> scripts/ml.py<pre><code>from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import date_format, col\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.evaluation import ClusteringEvaluator\n\n# Initialize spark session\nspark = (\n    SparkSession.\n    builder.\n    appName(\"ml_demo\").\n    config(\"spark.cores.max\", \"4\").\n    config(\"spark.sql.shuffle.partitions\", \"5\").\n    getOrCreate()\n)\n\n# Loading the data\ndf = spark.read.options(header=True, inferSchema=True).csv('./data/retail-data/by-day')\ndf.createOrReplaceTempView(\"retail_data\")\ndf.show(5)\n\n# Preprocessing\npdf = df.na.fill(0).withColumn(\"day_of_week\", date_format(col(\"InvoiceDate\"), \"EEEE\")).coalesce(5)\n\n# Split the data\ntrain_df = pdf.where(\"InvoiceDate &lt; '2011-07-01'\")\ntest_df = pdf.where(\"InvoiceDate &gt;= '2011-07-01'\")\n\n# ML Pipeline\nindexer = StringIndexer().setInputCol(\"day_of_week\").setOutputCol(\"day_of_week_index\")\nencoder = OneHotEncoder().setInputCol(\"day_of_week_index\").setOutputCol(\"day_of_week_encoded\")\nvectorAssembler = VectorAssembler().setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"]).setOutputCol(\"features\")\n\ntf_pipeline = Pipeline().setStages([indexer, encoder, vectorAssembler])\n\nfitted_pipeline = tf_pipeline.fit(train_df)\n\n# Transform Data\ntransformed_train = fitted_pipeline.transform(train_df)\ntransformed_test = fitted_pipeline.transform(test_df)\n\n# building Model\nkmeans = KMeans().setK(20).setSeed(1)\nkmModel = kmeans.fit(transformed_train)\n\nprint(kmModel.summary.trainingCost)\n\n# Predictions\npredictions = kmModel.transform(transformed_test)\nevaluator = ClusteringEvaluator()\n\nsilhouette = evaluator.evaluate(predictions)\nprint(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n\ncenters = kmModel.clusterCenters()\nprint(\"Cluster Centers: \")\nfor center in centers:\n    print(center)\n</code></pre> <p>to execute, once enter the spark-master, run this command below</p> <pre><code>spark-submit ./scripts/ml.py\n</code></pre> <p>Output</p> <pre><code>Silhouette with squared euclidean distance = 0.5427938390491535\n\nCluster Centers: \n[4.09293606 2.73959977 0.18896861 0.19629835 0.18589279 0.16698473\n 0.14731972]\n[1.0400e+00 7.4215e+04 0.0000e+00 1.0000e+00 0.0000e+00 0.0000e+00\n 0.0000e+00]\n[ 1.0400e+00 -7.4215e+04  0.0000e+00  1.0000e+00  0.0000e+00  0.0000e+00\n  0.0000e+00]\n[ 3.897e+04 -1.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n  1.000e+00]\n[ 1.6670865e+04 -1.0000000e+00  0.0000000e+00  0.0000000e+00\n  0.0000000e+00  1.0000000e+00  0.0000000e+00]\n[ 7.5000e-03 -9.4045e+03  2.5000e-01  7.5000e-01  0.0000e+00  0.0000e+00\n  0.0000e+00]\n[ 7.385808e+03 -6.000000e-01  0.000000e+00  8.000000e-01  2.000000e-01\n  0.000000e+00  0.000000e+00]\n[ 1.94092118e+03 -1.76470588e-01  5.88235294e-02  1.76470588e-01\n  4.11764706e-01  0.00000000e+00  3.52941176e-01]\n[8.407500e-01 1.213475e+03 2.125000e-01 2.500000e-01 1.125000e-01\n 2.000000e-01 1.625000e-01]\n[ 2.91481481e-01 -1.34285185e+03  1.85185185e-01  2.96296296e-01\n  1.85185185e-01  2.59259259e-01  7.40740741e-02]\n[  1.41599732 114.6946616    0.20817512   0.24078624   0.12754076\n   0.21710967   0.15434443]\n[ 1.3524695e+04 -5.0000000e-01  0.0000000e+00  1.0000000e+00\n  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n[ 5.43415e+03 -1.00000e+00  0.00000e+00  1.25000e-01  3.75000e-01\n  0.00000e+00  5.00000e-01]\n[ 1.51681407 19.67695062  0.22239997  0.18547297  0.15614466  0.19282409\n  0.15604944]\n[6.06428571e-01 3.20957143e+03 2.85714286e-01 1.42857143e-01\n 1.42857143e-01 1.42857143e-01 2.85714286e-01]\n[1.90351052e+02 8.47036329e-01 1.64435946e-01 3.02103250e-01\n 2.31357553e-01 1.51051625e-01 1.30019120e-01]\n[1.21189516e+00 4.17725806e+02 2.98387097e-01 2.01612903e-01\n 1.45161290e-01 1.81451613e-01 1.41129032e-01]\n[7.75176053e+02 1.03947368e+00 1.97368421e-01 1.97368421e-01\n 3.02631579e-01 1.05263158e-01 1.97368421e-01]\n[ 3.50000000e-01 -3.15500000e+03  0.00000000e+00  1.66666667e-01\n  3.33333333e-01  5.00000000e-01  0.00000000e+00]\n[ 4.08333333e-01 -4.22783333e+02  2.00000000e-01  2.00000000e-01\n  2.16666667e-01  1.50000000e-01  2.33333333e-01]\n</code></pre>"},{"location":"projects/spark-docker/#running-etl","title":"Running ETL","text":"<p>I have sample ETL code</p> <p></p> etl.py<pre><code>from pyspark.sql import SparkSession\n\n# Initialize spark session\nspark = (\n    SparkSession.\n    builder.\n    appName(\"etl_demo\").\n    config(\"spark.cores.max\", \"4\").\n    config(\"spark.sql.shuffle.partitions\", \"5\").\n    getOrCreate()\n)\n\nSOURCE_PATH = \"./data/yellow_trip_data\"\nDEST_PATH = \"./data/output/count_by_vendor.parquet\"\n\n# Loading the data\ndf = spark.read.options(inferSchema=True).parquet(SOURCE_PATH)\ndf.createOrReplaceTempView(\"yellow_trip_data\")\ndf.show(5)\n\n# Transformation: Count\ndf2 = df.groupBy(\"VendorID\").count()\ndf2.write.mode(\"overwrite\").parquet(DEST_PATH)\n\n# Viewing Output\ndf3 = spark.read.options(inferSchema=True).parquet(DEST_PATH)\ndf3.createOrReplaceTempView(\"count_by_vendor\")\nspark.sql(\"select * from count_by_vendor\").show()\n</code></pre> <p>Source files</p> <pre><code>+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n|       1| 2022-03-01 00:13:08|  2022-03-01 00:24:35|            1.0|          2.4|       1.0|                 N|          90|         209|           2|       10.0|  3.0|    0.5|       0.0|         0.0|                  0.3|        13.8|                 2.5|        0.0|\n|       1| 2022-03-01 00:47:52|  2022-03-01 01:00:08|            1.0|          2.2|       1.0|                 N|         148|         234|           2|       10.5|  3.0|    0.5|       0.0|         0.0|                  0.3|        14.3|                 2.5|        0.0|\n|       2| 2022-03-01 00:02:46|  2022-03-01 00:46:43|            1.0|        19.78|       2.0|                 N|         132|         249|           1|       52.0|  0.0|    0.5|     11.06|         0.0|                  0.3|       67.61|                 2.5|       1.25|\n|       2| 2022-03-01 00:52:43|  2022-03-01 01:03:40|            2.0|         2.94|       1.0|                 N|         211|          66|           1|       11.0|  0.5|    0.5|      4.44|         0.0|                  0.3|       19.24|                 2.5|        0.0|\n|       2| 2022-03-01 00:15:35|  2022-03-01 00:34:13|            1.0|         8.57|       1.0|                 N|         138|         197|           1|       25.0|  0.5|    0.5|      5.51|         0.0|                  0.3|       33.06|                 0.0|       1.25|\n+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n</code></pre> <p>Output</p> <pre><code>+--------+-------+\n|VendorID|  count|\n+--------+-------+\n|       6|  27817|\n|       2|8897508|\n|       5|     97|\n|       1|3745742|\n+--------+-------+\n</code></pre> <p>Info</p> <p>If you go to your local files, you will find that the output parquets are written to your local, in path <code>data/output/</code></p> <p>In next interations, you can try to write into cloud storage instead, such as: Google Cloud Storage or Amazon S3, so you're not dumping everything to your local.</p> <p></p>"},{"location":"projects/spark-docker/#bring-down-the-resources","title":"Bring down the Resources","text":"<p>In order to not exhausting the resources of your local, you can bring down all the alive containers by just 1 command</p> <pre><code>make down\n</code></pre> <p>Output</p> <pre><code>(base) \u279c  spark-docker git:(main) make down\ndocker compose down --rmi all\n[+] Running 5/3\n \u2714 Container spark-history       Removed                                                                                                                                                                                                                                                                          10.2s \n \u2714 Container spark-worker        Removed                                                                                                                                                                                                                                                                          10.2s \n \u2714 Container spark-master        Removed                                                                                                                                                                                                                                                                          10.1s \n \u2714 Image spark-image:latest      Removed                                                                                                                                                                                                                                                                           0.0s \n \u2714 Network spark-docker_default  Removed     \n</code></pre>"},{"location":"projects/spark-docker/#references","title":"References","text":"<ul> <li>Setting up a Spark standalone cluster on Docker in Layman terms</li> <li>Docker Spark Iceberg</li> <li>How Compose works</li> </ul>"},{"location":"projects/spark-hudi/","title":"Apache Hudi and Spark","text":""},{"location":"projects/spark-hudi/#what-is-hudi","title":"What is Hudi","text":"<p>Apache Hudi is a transactional data lake platform that brings database and data warehouse capabilities to the data lake. Hudi reimagines slow old-school batch data processing with a powerful new incremental processing framework for low latency minute-level analytics.</p> <p></p>"},{"location":"projects/spark-hudi/#hudi-stack","title":"Hudi Stack","text":"<ul> <li>Apache Hudi is a Transactional Data Lakehouse Platform built around a database kernel.</li> <li>It brings core warehouse and database functionality directly to a data lake thereby providing a table-level abstraction over open file formats like Apache Parquet/ORC (more recently known as the lakehouse architecture) and enabling transactional capabilities such as updates/deletes.</li> <li>Hudi also incorporates essential table services that are tightly integrated with the database kernel.</li> <li>These services can be executed automatically across both ingested and derived data to manage various aspects such as table bookkeeping, metadata, and storage layout.</li> <li>This integration along with various platform-specific services extends Hudi's role from being just a 'table format' to a comprehensive and robust data lakehouse platform.</li> </ul>"},{"location":"projects/spark-hudi/#lake-storage","title":"Lake Storage","text":"<p>The storage layer is where the data files (such as Parquet) are stored. Hudi interacts with the storage layer through the Hadoop FileSystem API, enabling compatibility with various systems including HDFS for fast appends, and various cloud stores such as Amazon S3, Google Cloud Storage (GCS), and Azure Blob Storage.</p> <p>Additionally, Hudi offers its own storage APIs that can rely on Hadoop-independent file system implementation to simplify the integration of various file systems.</p>"},{"location":"projects/spark-hudi/#file-formats","title":"File Formats","text":"<ul> <li>File formats hold the raw data and are physically stored on the lake storage.</li> <li>Hudi operates on logical structures of File Groups and File Slices, which consist of Base File and Log Files.</li> <li>Base Files are compacted and optimized for reads and are augmented with Log Files for efficient append.</li> </ul>"},{"location":"projects/spark-hudi/#transactional-database-layer","title":"Transactional Database Layer","text":"<p>The transactional database layer of Hudi comprises the core components that are responsible for the fundamental operations and services that enable Hudi to store, retrieve, and manage data efficiently on data lakehouse storages.</p>"},{"location":"projects/spark-hudi/#table-format","title":"Table Format","text":"<p>There are 3 major components related to Hudi\u2019s table format:</p> <ul> <li>Timeline</li> </ul> <p>Hudi's timeline, stored in the /.hoodie folder, is a crucial event log recording all table actions in an ordered manner, with events kept for a specified period. Hudi uniquely designs each File Group as a self-contained log, enabling record state reconstruction through delta logs, even after archival of related actions.</p> <ul> <li>File Group and File Slice</li> </ul> <p>Within each partition the data is physically stored as base and Log Files and organized into logical concepts as File groups and File Slices. File groups contain multiple versions of File Slices and are split into multiple File Slices. A File Slice comprises the Base and Log File. Each File Slice within the file-group is uniquely identified by the commit's timestamp that created it.</p> <ul> <li>Metadata Table</li> </ul> <p>Implemented as a merge-on-read table, Hudi's metadata table efficiently handles quick updates with low write amplification. It leverages the HFile format for quick, indexed key lookups, storing vital information like file paths, column statistics, bloom filters, and record indexes.</p> <p>Tips</p> <p>Hudi\u2019s approach of recording updates into Log Files is more efficient and involves low merge overhead than systems like Hive ACID, where merging all delta records against all Base Files is required.</p>"},{"location":"projects/spark-hudi/#indexes","title":"Indexes","text":"<ul> <li>Indexes in Hudi enhance query planning, minimizing I/O, speeding up response times and providing faster writes with low merge costs.</li> <li>Hudi\u2019s metadata table brings the benefits of indexes generally to both the readers and writers.</li> <li>Compute engines can leverage various indexes in the metadata table, like file listings, column statistics, bloom filters, record-level indexes, and functional indexes to quickly generate optimized query plans and improve read performance.</li> </ul>"},{"location":"projects/spark-hudi/#table-services","title":"Table Services","text":"<ul> <li>Apache Hudi offers various table services to help keep the table storage layout and metadata management performant.</li> <li>Hudi was designed with built-in table services that enables running them in inline, semi-asynchronous or full-asynchronous modes.</li> <li>Furthermore, Spark and Flink streaming writers can run in continuous mode, and invoke table services asynchronously sharing the underlying executors intelligently with writers.</li> </ul>"},{"location":"projects/spark-hudi/#implementation","title":"Implementation","text":"<p>Let's take a look into executing Hudi spark code.</p> <p>First, you need to clone this repo.</p> <pre><code>git clone git@github.com:karlchris/spark-docker.git\n</code></pre>"},{"location":"projects/spark-hudi/#quickstart","title":"Quickstart","text":"<ul> <li>run below command to execute docker compose</li> </ul> <pre><code>make up\n</code></pre> <ul> <li>enter into the container</li> </ul> <pre><code>make dev\n</code></pre> <ul> <li>execute the code</li> </ul> <pre><code>./bin/spark-submit scripts/hudi-demo.py\n</code></pre> <ul> <li>remove the running containers</li> </ul> <pre><code>make down\n</code></pre>"},{"location":"projects/spark-hudi/#installing-hudi-package","title":"Installing Hudi Package","text":"<ul> <li>add this to the Dockerfile</li> </ul> <pre><code># Download hudi jars\nRUN curl https://repo1.maven.org/maven2/org/apache/hudi/hudi-spark3-bundle_2.12/0.15.0/hudi-spark3-bundle_2.12-0.15.0.jar -Lo /opt/spark/jars/hudi-spark3-bundle_2.12-0.15.0.jar\n</code></pre> <ul> <li>Setup Hudi in spark session</li> </ul> <pre><code>from pyspark.sql import SparkSession\nfrom pyspark import SparkConf\nfrom pyspark.sql.functions import col\n\njars = 'org.apache.hudi:hudi-spark3-bundle_2.12:0.15.0'\n\n# Setup config\nconf = SparkConf().setAppName(\"hudiDemo\") \\\n    .set(\"spark.sql.extensions\", \"org.apache.spark.sql.hudi.HoodieSparkSessionExtension\") \\\n    .set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.hudi.catalog.HoodieCatalog\") \\\n    .set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n    .set(\"spark.kryo.registrator\", \"org.apache.spark.HoodieSparkKryoRegistrar\") \\\n    .set('spark.jars.packages', jars)\n\n# Create spark session\nspark = SparkSession.builder.config(conf=conf).getOrCreate()\nspark.sparkContext.setLogLevel(\"ERROR\")\n</code></pre>"},{"location":"projects/spark-hudi/#create-hudi-table-and-insert-data","title":"Create Hudi Table and Insert Data","text":"<pre><code># Path\nHOME = \"/opt/spark/\"\n\n# Create table\ntableName = \"trips_table\"\nbasePath = HOME + \"warehouse/hudi/trips_table\"\nprint(f\"Creating table {tableName} at {basePath}\")\n\n# Insert data\nprint(\"Inserting data\")\ncolumns = [\"ts\",\"uuid\",\"rider\",\"driver\",\"fare\",\"city\"]\ndata =[(1695159649087,\"334e26e9-8355-45cc-97c6-c31daf0df330\",\"rider-A\",\"driver-K\",19.10,\"san_francisco\"),\n    (1695091554788,\"e96c4396-3fad-413a-a942-4cb36106d721\",\"rider-C\",\"driver-M\",27.70 ,\"san_francisco\"),\n    (1695046462179,\"9909a8b1-2d15-4d3d-8ec9-efc48c536a00\",\"rider-D\",\"driver-L\",33.90 ,\"san_francisco\"),\n    (1695516137016,\"e3cf430c-889d-4015-bc98-59bdce1e530c\",\"rider-F\",\"driver-P\",34.15,\"sao_paulo\"),\n    (1695115999911,\"c8abbe79-8d89-47ea-b4ce-4d224bae5bfa\",\"rider-J\",\"driver-T\",17.85,\"chennai\")]\ninserts = spark.createDataFrame(data).toDF(*columns)\n\nhudi_options = {\n    'hoodie.table.name': tableName,\n    'hoodie.datasource.write.partitionpath.field': 'city',\n    'hoodie.datasource.write.recordkey.field': 'uuid'\n}\n\ninserts.write.format(\"hudi\"). \\\n    options(**hudi_options). \\\n    mode(\"overwrite\"). \\\n    save(basePath)\n</code></pre> <p>It will create <code>trips_table</code> and write it within path <code>/opt/spark/warehouse/hudi/trips_table</code>. Then, it will insert data as mentioned above.</p> <p>Info</p> <p>Hudi also allows users to specify a record key, which will be used to uniquely identify a record within a Hudi table. This is useful and critical to support features like indexing and clustering, which speed up upserts and queries respectively, in a consistent manner.</p> <p>In above example, <code>'hoodie.datasource.write.recordkey.field': 'uuid'</code> was set up.</p>"},{"location":"projects/spark-hudi/#query-data","title":"Query data","text":"<pre><code>print(\"Querying data\")\ntripsDF = spark.read.format(\"hudi\").load(basePath)\ntripsDF.createOrReplaceTempView(\"trips_table\")\n\nspark.sql(\"SELECT uuid, fare, ts, rider, driver, city FROM  trips_table WHERE fare &gt; 20.0\").show()\nspark.sql(\"SELECT _hoodie_commit_time, _hoodie_record_key, _hoodie_partition_path, rider, driver, fare FROM trips_table\").show()\n</code></pre> <p>It will query the table and get records with <code>fare more than 20</code></p> <p>Output:</p> <pre><code>+--------------------+-----+-------------+-------+--------+-------------+\n|                uuid| fare|           ts|  rider|  driver|         city|\n+--------------------+-----+-------------+-------+--------+-------------+\n|e96c4396-3fad-413...| 27.7|1695091554788|rider-C|driver-M|san_francisco|\n|9909a8b1-2d15-4d3...| 33.9|1695046462179|rider-D|driver-L|san_francisco|\n|e3cf430c-889d-401...|34.15|1695516137016|rider-F|driver-P|    sao_paulo|\n+--------------------+-----+-------------+-------+--------+-------------+\n\n+-------------------+--------------------+----------------------+-------+--------+-----+\n|_hoodie_commit_time|  _hoodie_record_key|_hoodie_partition_path|  rider|  driver| fare|\n+-------------------+--------------------+----------------------+-------+--------+-----+\n|  20240705004838235|334e26e9-8355-45c...|         san_francisco|rider-A|driver-K| 19.1|\n|  20240705004838235|e96c4396-3fad-413...|         san_francisco|rider-C|driver-M| 27.7|\n|  20240705004838235|9909a8b1-2d15-4d3...|         san_francisco|rider-D|driver-L| 33.9|\n|  20240705004838235|e3cf430c-889d-401...|             sao_paulo|rider-F|driver-P|34.15|\n|  20240705004838235|c8abbe79-8d89-47e...|               chennai|rider-J|driver-T|17.85|\n+-------------------+--------------------+----------------------+-------+--------+-----+\n</code></pre>"},{"location":"projects/spark-hudi/#update-data","title":"Update data","text":"<pre><code>print(\"Updating data\")\n# Lets read data from target Hudi table, modify fare column for rider-D and update it.\nupdatesDf = spark.read.format(\"hudi\").load(basePath).filter(\"rider == 'rider-D'\").withColumn(\"fare\",col(\"fare\")*10)\n\nupdatesDf.write.format(\"hudi\"). \\\n    options(**hudi_options). \\\n    mode(\"append\"). \\\n    save(basePath)\n</code></pre> <p>It will update the <code>fare</code> column where rider is <code>rider-D</code>.</p> <p>Output:</p> <pre><code>+-------------------+-----+-------+--------+--------------------+-------------+\n|_hoodie_commit_time| fare|  rider|  driver|                uuid|           ts|\n+-------------------+-----+-------+--------+--------------------+-------------+\n|  20240705004845127|339.0|rider-D|driver-L|9909a8b1-2d15-4d3...|1695046462179|\n+-------------------+-----+-------+--------+--------------------+-------------+\n</code></pre>"},{"location":"projects/spark-hudi/#delete-data","title":"Delete data","text":"<pre><code>print(\"Deleting data\")\ndeletesDF = spark.read.format(\"hudi\").load(basePath).filter(\"rider == 'rider-F'\")\n\n# issue deletes\nhudi_hard_delete_options = {\n    'hoodie.table.name': tableName,\n    'hoodie.datasource.write.partitionpath.field': 'city',\n    'hoodie.datasource.write.operation': 'delete',\n}\n\ndeletesDF.write.format(\"hudi\"). \\\n    options(**hudi_hard_delete_options). \\\n    mode(\"append\"). \\\n    save(basePath)\n</code></pre> <p>It will delete record where rider is <code>rider-F</code></p>"},{"location":"projects/spark-hudi/#incremental-query","title":"Incremental Query","text":"<p>Hudi provides the unique capability to obtain a set of records that changed between a start and end commit time, providing you with the \"latest state\" for each such record as of the end commit time.</p> <p>Info</p> <p>By default, Hudi tables are configured to support incremental queries, using record level metadata tracking.</p> <pre><code># reload data\nspark.read.format(\"hudi\").load(basePath).createOrReplaceTempView(\"trips_table\")\n\ncommits = list(map(lambda row: row[0], spark.sql(\"SELECT DISTINCT(_hoodie_commit_time) AS commitTime FROM  trips_table ORDER BY commitTime\").limit(50).collect()))\nbeginTime = commits[len(commits) - 2] # commit time we are interested in\n\nprint(f\"Querying data incrementally from commit time: {beginTime}\")\n# incrementally query data\nincremental_read_options = {\n    'hoodie.datasource.query.type': 'incremental',\n    'hoodie.datasource.read.begin.instanttime': beginTime,\n}\n\ntripsIncrementalDF = spark.read.format(\"hudi\"). \\\n    options(**incremental_read_options). \\\n    load(basePath)\ntripsIncrementalDF.createOrReplaceTempView(\"trips_incremental\")\n\nspark.sql(\"SELECT `_hoodie_commit_time`, fare, rider, driver, uuid, ts FROM trips_incremental WHERE fare &gt; 20.0\").show()\n</code></pre>"},{"location":"projects/spark-hudi/#change-data-capture-query","title":"Change Data Capture Query","text":"<p>Hudi also exposes first-class support for Change Data Capture (CDC) queries. CDC queries are useful for applications that need to obtain all the changes, along with before/after images of records, given a commit time range.</p> <pre><code>print(\"Change Data Capture Query\")\n# Lets first insert data to a new table with cdc enabled.\ncolumns = [\"ts\",\"uuid\",\"rider\",\"driver\",\"fare\",\"city\"]\ndata =[(1695159649087,\"334e26e9-8355-45cc-97c6-c31daf0df330\",\"rider-A\",\"driver-K\",19.10,\"san_francisco\"),\n    (1695091554788,\"e96c4396-3fad-413a-a942-4cb36106d721\",\"rider-B\",\"driver-L\",27.70 ,\"san_francisco\"),\n    (1695046462179,\"9909a8b1-2d15-4d3d-8ec9-efc48c536a00\",\"rider-C\",\"driver-M\",33.90 ,\"san_francisco\"),\n    (1695516137016,\"e3cf430c-889d-4015-bc98-59bdce1e530c\",\"rider-C\",\"driver-N\",34.15,\"sao_paulo\")]\n\ninserts = spark.createDataFrame(data).toDF(*columns)\n\nhudi_options = {\n    'hoodie.table.name': tableName,\n    'hoodie.datasource.write.partitionpath.field': 'city',\n    'hoodie.table.cdc.enabled': 'true',\n    'hoodie.datasource.write.precombine.field': 'ts',\n    'hoodie.datasource.write.recordkey.field': 'uuid'\n}\n# Insert data\ninserts.write.format(\"hudi\"). \\\n    options(**hudi_options). \\\n    mode(\"overwrite\"). \\\n    save(basePath)\n\n\n#  Update fare for riders: rider-A and rider-B \nupdatesDf = spark.read.format(\"hudi\").load(basePath).filter(\"rider == 'rider-A' or rider == 'rider-B'\").withColumn(\"fare\",col(\"fare\")*10)\n\nupdatesDf.write.format(\"hudi\"). \\\n    mode(\"append\"). \\\n    save(basePath)\n\n# Query CDC data\ncdc_read_options = {\n    'hoodie.datasource.query.incremental.format': 'cdc',\n    'hoodie.datasource.query.type': 'incremental',\n    'hoodie.datasource.read.begin.instanttime': 0\n}\nspark.read.format(\"hudi\"). \\\n    options(**cdc_read_options). \\\n    load(basePath).show(10, False)\n</code></pre> <p>Output:</p> <pre><code>Change Data Capture Query\n+---+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|op |ts_ms            |before                                                                                                                                                                                                                                                                                                                                                                                                                                                         |after                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n+---+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|i  |20240705004850080|NULL                                                                                                                                                                                                                                                                                                                                                                                                                                                           |{\"driver\":\"driver-N\",\"_hoodie_record_key\":\"e3cf430c-889d-4015-bc98-59bdce1e530c\",\"city\":\"sao_paulo\",\"_hoodie_partition_path\":\"sao_paulo\",\"_hoodie_file_name\":\"1b3061be-e5df-44bb-b234-2da032806851-0_1-146-246_20240705004850080.parquet\",\"_hoodie_commit_seqno\":\"20240705004850080_1_0\",\"fare\":34.15,\"rider\":\"rider-C\",\"_hoodie_commit_time\":\"20240705004850080\",\"ts\":1695516137016,\"uuid\":\"e3cf430c-889d-4015-bc98-59bdce1e530c\"}       |\n|i  |20240705004850080|NULL                                                                                                                                                                                                                                                                                                                                                                                                                                                           |{\"driver\":\"driver-M\",\"_hoodie_record_key\":\"9909a8b1-2d15-4d3d-8ec9-efc48c536a00\",\"city\":\"san_francisco\",\"_hoodie_partition_path\":\"san_francisco\",\"_hoodie_file_name\":\"69ea351b-2ce4-4590-97f0-11b4d4d3146d-0_0-146-245_20240705004850080.parquet\",\"_hoodie_commit_seqno\":\"20240705004850080_0_0\",\"fare\":33.9,\"rider\":\"rider-C\",\"_hoodie_commit_time\":\"20240705004850080\",\"ts\":1695046462179,\"uuid\":\"9909a8b1-2d15-4d3d-8ec9-efc48c536a00\"}|\n|i  |20240705004850080|NULL                                                                                                                                                                                                                                                                                                                                                                                                                                                           |{\"driver\":\"driver-K\",\"_hoodie_record_key\":\"334e26e9-8355-45cc-97c6-c31daf0df330\",\"city\":\"san_francisco\",\"_hoodie_partition_path\":\"san_francisco\",\"_hoodie_file_name\":\"69ea351b-2ce4-4590-97f0-11b4d4d3146d-0_0-146-245_20240705004850080.parquet\",\"_hoodie_commit_seqno\":\"20240705004850080_0_1\",\"fare\":19.1,\"rider\":\"rider-A\",\"_hoodie_commit_time\":\"20240705004850080\",\"ts\":1695159649087,\"uuid\":\"334e26e9-8355-45cc-97c6-c31daf0df330\"}|\n|i  |20240705004850080|NULL                                                                                                                                                                                                                                                                                                                                                                                                                                                           |{\"driver\":\"driver-L\",\"_hoodie_record_key\":\"e96c4396-3fad-413a-a942-4cb36106d721\",\"city\":\"san_francisco\",\"_hoodie_partition_path\":\"san_francisco\",\"_hoodie_file_name\":\"69ea351b-2ce4-4590-97f0-11b4d4d3146d-0_0-146-245_20240705004850080.parquet\",\"_hoodie_commit_seqno\":\"20240705004850080_0_2\",\"fare\":27.7,\"rider\":\"rider-B\",\"_hoodie_commit_time\":\"20240705004850080\",\"ts\":1695091554788,\"uuid\":\"e96c4396-3fad-413a-a942-4cb36106d721\"}|\n|u  |20240705004851440|{\"_hoodie_commit_time\": \"20240705004850080\", \"_hoodie_commit_seqno\": \"20240705004850080_0_1\", \"_hoodie_record_key\": \"334e26e9-8355-45cc-97c6-c31daf0df330\", \"_hoodie_partition_path\": \"san_francisco\", \"_hoodie_file_name\": \"69ea351b-2ce4-4590-97f0-11b4d4d3146d-0_0-146-245_20240705004850080.parquet\", \"ts\": 1695159649087, \"uuid\": \"334e26e9-8355-45cc-97c6-c31daf0df330\", \"rider\": \"rider-A\", \"driver\": \"driver-K\", \"fare\": 19.1, \"city\": \"san_francisco\"}|{\"_hoodie_commit_time\": \"20240705004851440\", \"_hoodie_commit_seqno\": \"20240705004851440_0_1\", \"_hoodie_record_key\": \"334e26e9-8355-45cc-97c6-c31daf0df330\", \"_hoodie_partition_path\": \"san_francisco\", \"_hoodie_file_name\": \"69ea351b-2ce4-4590-97f0-11b4d4d3146d-0\", \"ts\": 1695159649087, \"uuid\": \"334e26e9-8355-45cc-97c6-c31daf0df330\", \"rider\": \"rider-A\", \"driver\": \"driver-K\", \"fare\": 191.0, \"city\": \"san_francisco\"}              |\n|u  |20240705004851440|{\"_hoodie_commit_time\": \"20240705004850080\", \"_hoodie_commit_seqno\": \"20240705004850080_0_2\", \"_hoodie_record_key\": \"e96c4396-3fad-413a-a942-4cb36106d721\", \"_hoodie_partition_path\": \"san_francisco\", \"_hoodie_file_name\": \"69ea351b-2ce4-4590-97f0-11b4d4d3146d-0_0-146-245_20240705004850080.parquet\", \"ts\": 1695091554788, \"uuid\": \"e96c4396-3fad-413a-a942-4cb36106d721\", \"rider\": \"rider-B\", \"driver\": \"driver-L\", \"fare\": 27.7, \"city\": \"san_francisco\"}|{\"_hoodie_commit_time\": \"20240705004851440\", \"_hoodie_commit_seqno\": \"20240705004851440_0_2\", \"_hoodie_record_key\": \"e96c4396-3fad-413a-a942-4cb36106d721\", \"_hoodie_partition_path\": \"san_francisco\", \"_hoodie_file_name\": \"69ea351b-2ce4-4590-97f0-11b4d4d3146d-0\", \"ts\": 1695091554788, \"uuid\": \"e96c4396-3fad-413a-a942-4cb36106d721\", \"rider\": \"rider-B\", \"driver\": \"driver-L\", \"fare\": 277.0, \"city\": \"san_francisco\"}              |\n+---+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n</code></pre> <p>Warning</p> <p>Note that CDC queries are currently only supported on <code>Copy-on-Write</code> tables.</p>"},{"location":"projects/spark-hudi/#references","title":"References","text":"<ul> <li>Apache Hudi Official Documentation</li> </ul>"},{"location":"projects/spark-iceberg/","title":"Apache Iceberg and PySpark","text":"<p>If you've had experiences with data lakes, you likely faced significant challenges related to executing updates and deletes.</p> <p>Managing the concurrency between multiple readers and writers, addressing schema evolution in your data, and managing the partitions evolution when data volume or query patterns change.</p> <p>This is how Apache Iceberg comes into the picture.</p>"},{"location":"projects/spark-iceberg/#about-apache-iceberg","title":"About Apache Iceberg","text":"<p>Apache Iceberg is an open table format designed for extensive analytics datasets. It is compatible with widely used big data processing engines such as Apache Spark, Trino, Flink, and Hive.</p> <p>Iceberg tackles several limitations we listed above by acting as a metadata layer on top of the file format like Apache Parquet and Apache ORC.</p>"},{"location":"projects/spark-iceberg/#features","title":"Features","text":"<ul> <li>Schema Evolution</li> </ul> <p>Allows for seamless schema evolution, overcoming the challenges associated with changes in data structure over time.</p> <ul> <li>Transactional Writes</li> </ul> <p>By supporting transactional writes, Iceberg ensures the atomicity, consistency, isolation, and durability (ACID) properties, enhancing data integrity during write operations.</p> <ul> <li>Query Isolation</li> </ul> <p>Iceberg provides query isolation, preventing interference between concurrent read and write operations, thus improving overall system reliability and performance.</p> <ul> <li>Time Travel</li> </ul> <p>The time travel feature in Iceberg allows users to access historical versions of the data, offering a valuable mechanism for auditing, analysis, and debugging.</p> <ul> <li>Partition Pruning</li> </ul> <p>Iceberg\u2019s partition pruning capability optimizes query performance by selectively scanning only relevant partitions, reducing the amount of data processed and improving query speed.</p>"},{"location":"projects/spark-iceberg/#architecture","title":"Architecture","text":"<p>Apache Iceberg table has three different layers.</p>"},{"location":"projects/spark-iceberg/#data-layer","title":"Data Layer","text":"<p>This is the layer where the actual data for the table is stored and is primarily made of data files. Apache Iceberg is file-format agnostic and it currently supports Apache Parquet, Apache ORC, and Apache Avro. It stores the data by default in Apache Parquet file format.</p> <p>This file-format agnostic provides the ability for a user to choose the underlying file format based on the use case, for example, Parquet might be used for a large-scale OLAP analytics table, whereas Avro might be used for a low-latency streaming analytics table.</p> <p>Tip</p> <p>The data layer is backed by a distributed file system like HDFS or a cloud object storage like AWS S3.</p>"},{"location":"projects/spark-iceberg/#metadata-layer","title":"Metadata Layer","text":"<p>This layer contains all of the metadata files for an Iceberg table. It has a tree structure that tracks the data files and metadata about them along with the details of the operation that made them.</p> <p>Info</p> <p>The files in this layer are immutable files so everytime an insert, merge, upsert or delete operation happens on the table, a new set of files are written.</p> <p>This layer contains 3 file types</p>"},{"location":"projects/spark-iceberg/#manifest-files","title":"Manifest Files","text":"<p>Manifest lists keep track of manifest files, including the location, the partitions it belong to, and the upper and lower bound for partition columns for the data it tracks. It stores all this information in avro file format.</p> <p>Note</p> <p>A Manifest list file is a snapshot of an Iceberg Table as it contains the details of the snapshot along with snapshot_id that has added it.</p>"},{"location":"projects/spark-iceberg/#metadata-files","title":"Metadata Files","text":"<p>Metadata files keep track of Manifest Lists. These files include the information about the metadata of the Iceberg Table at a certain point in time i.e. table\u2019s schema, partition information, snapshots, and which snapshot is the current one.</p> <p>All this information is stored in a <code>json</code> format file.</p> Example v1.metadata.json<pre><code>{\n  \"format-version\" : 2,\n  \"table-uuid\" : \"152c11c8-e993-487f-86d3-9603ff8c5b7f\",\n  \"location\" : \"./warehouse/db/persons\",\n  \"last-sequence-number\" : 1,\n  \"last-updated-ms\" : 1718930007854,\n  \"last-column-id\" : 3,\n  \"current-schema-id\" : 0,\n  \"schemas\" : [ {\n    \"type\" : \"struct\",\n    \"schema-id\" : 0,\n    \"fields\" : [ {\n      \"id\" : 1,\n      \"name\" : \"name\",\n      \"required\" : false,\n      \"type\" : \"string\"\n    }, {\n      \"id\" : 2,\n      \"name\" : \"age\",\n      \"required\" : false,\n      \"type\" : \"int\"\n    }, {\n      \"id\" : 3,\n      \"name\" : \"job_title\",\n      \"required\" : false,\n      \"type\" : \"string\"\n    } ]\n  } ],\n  \"default-spec-id\" : 0,\n  \"partition-specs\" : [ {\n    \"spec-id\" : 0,\n    \"fields\" : [ ]\n  } ],\n  \"last-partition-id\" : 999,\n  \"default-sort-order-id\" : 0,\n  \"sort-orders\" : [ {\n    \"order-id\" : 0,\n    \"fields\" : [ ]\n  } ],\n  \"properties\" : {\n    \"owner\" : \"root\",\n    \"write.parquet.compression-codec\" : \"zstd\"\n  },\n  \"current-snapshot-id\" : 5317106206696669755,\n  \"refs\" : {\n    \"main\" : {\n      \"snapshot-id\" : 5317106206696669755,\n      \"type\" : \"branch\"\n    }\n  },\n  \"snapshots\" : [ {\n    \"sequence-number\" : 1,\n    \"snapshot-id\" : 5317106206696669755,\n    \"timestamp-ms\" : 1718930007854,\n    \"summary\" : {\n      \"operation\" : \"append\",\n      \"spark.app.id\" : \"app-20240621003324-0000\",\n      \"added-data-files\" : \"2\",\n      \"added-records\" : \"3\",\n      \"added-files-size\" : \"1838\",\n      \"changed-partition-count\" : \"1\",\n      \"total-records\" : \"3\",\n      \"total-files-size\" : \"1838\",\n      \"total-data-files\" : \"2\",\n      \"total-delete-files\" : \"0\",\n      \"total-position-deletes\" : \"0\",\n      \"total-equality-deletes\" : \"0\"\n    },\n    \"manifest-list\" : \"warehouse/db/persons/metadata/snap-5317106206696669755-1-59b1ef61-4e45-4a54-bddf-c89989f2e7ef.avro\",\n    \"schema-id\" : 0\n  } ],\n  \"statistics\" : [ ],\n  \"snapshot-log\" : [ {\n    \"timestamp-ms\" : 1718930007854,\n    \"snapshot-id\" : 5317106206696669755\n  } ],\n  \"metadata-log\" : [ ]\n}\n</code></pre>"},{"location":"projects/spark-iceberg/#catalog-layer","title":"Catalog Layer","text":"<p>Within the Catalog layer, there is a reference or pointer, that points to the current metadata file for that table.</p>"},{"location":"projects/spark-iceberg/#installing-apache-iceberg","title":"Installing Apache Iceberg","text":"<p>First, you need to clone this repo</p> <pre><code>git clone git@github.com:karlchris/spark-docker.git\n</code></pre> <ul> <li>Add Iceberg installation in the Dockerfile</li> </ul> <pre><code>FROM pyspark\n\n# Download iceberg spark runtime\nRUN curl https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.4_2.12/1.4.3/iceberg-spark-runtime-3.4_2.12-1.4.3.jar -Lo /opt/spark/jars/iceberg-spark-runtime-3.4_2.12-1.4.3.jar\n\n# Add iceberg spark runtime jar to IJava classpath\nENV IJAVA_CLASSPATH=/opt/spark/jars/*\n</code></pre> <ul> <li>Add <code>warehouse</code> volume mount for each of containers</li> </ul> <pre><code>volumes:\n  - ./scripts:/opt/spark/scripts\n  - ./data:/opt/spark/data\n  - spark-logs:/opt/spark/spark-events\n  - ./warehouse:/opt/spark/warehouse\n</code></pre>"},{"location":"projects/spark-iceberg/#setup-iceberg-session","title":"Setup Iceberg Session","text":"<p>Note</p> <p>Starting from here, you better to start up the docker containers by running</p> <pre><code>make up\n</code></pre> <p>Danger</p> <p>Don't forget to clean up your docker resources by running <code>make down</code> after you finish this exercise.</p> <p>To begin working with Iceberg tables in PySpark, it's essential to configure the PySpark session.</p> <p>In the following steps, we will use a catalog named <code>demo</code> for tables located under the path <code>./warehouse</code> of the Hadoop type.</p> <p>Warning</p> <p>Crucially, ensure compatibility between the Iceberg-Spark-Runtime JAR and the PySpark version in use.</p> <p>You can find the necessary JARs in the Iceberg releases</p> <pre><code>warehouse_path = \"./warehouse\"\niceberg_spark_jar = 'org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.3'\niceberg_spark_ext = 'org.apache.iceberg:iceberg-spark-extensions-3.4_2.12:1.4.3'\ncatalog_name = \"demo\"\n\n# Setup iceberg config\nconf = SparkConf().setAppName(\"YourAppName\") \\\n    .set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n    .set(f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .set('spark.jars.packages', iceberg_spark_jar) \\\n    .set('spark.jars.packages', iceberg_spark_ext) \\\n    .set(f\"spark.sql.catalog.{catalog_name}.warehouse\", warehouse_path) \\\n    .set(f\"spark.sql.catalog.{catalog_name}.type\", \"hadoop\")\\\n    .set(\"spark.sql.defaultCatalog\", catalog_name)\n\n# Create spark session\nspark = SparkSession.builder.config(conf=conf).getOrCreate()\nspark.sparkContext.setLogLevel(\"ERROR\")\n</code></pre>"},{"location":"projects/spark-iceberg/#create-and-write-iceberg-table-with-pyspark","title":"Create and Write Iceberg Table with PySpark","text":"<pre><code># Create a dataframe\nschema = StructType([\n    StructField('name', StringType(), True),\n    StructField('age', IntegerType(), True),\n    StructField('job_title', StringType(), True)\n])\ndata = [(\"person1\", 28, \"Doctor\"), (\"person2\", 35, \"Singer\"), (\"person3\", 42, \"Teacher\")]\ndf = spark.createDataFrame(data, schema=schema)\n\n# Create database\nspark.sql(f\"CREATE DATABASE IF NOT EXISTS db\")\n\n# Write and read Iceberg table\ntable_name = \"db.persons\"\ndf.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(f\"{table_name}\")\niceberg_df = spark.read.format(\"iceberg\").load(f\"{table_name}\")\niceberg_df.printSchema()\niceberg_df.show()\n</code></pre> <p>Output</p> <pre><code>root\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- job_title: string (nullable = true)\n\n+-------+---+---------+\n|   name|age|job_title|\n+-------+---+---------+\n|person1| 28|   Doctor|\n|person2| 35|   Singer|\n|person3| 42|  Teacher|\n+-------+---+---------+\n</code></pre> <p>We create a PySpark DataFrame, write it into an Iceberg table, and subsequently display the data stored in the Iceberg table.</p>"},{"location":"projects/spark-iceberg/#schema-evolution","title":"Schema Evolution","text":"<p>The flexibility of Data Lakes, allowing storage of diverse data formats, can pose challenges in managing schema changes.</p> <p>Iceberg addresses this by enabling the addition, removal, or modification of table columns without requiring a complete data rewrite.</p> <p>This feature simplifies the process of evolving schemas over time.</p> <pre><code># Schema Evolution\nspark.sql(f\"ALTER TABLE {table_name} RENAME COLUMN job_title TO job\")\nspark.sql(f\"ALTER TABLE {table_name} ALTER COLUMN age TYPE bigint\")\nspark.sql(f\"ALTER TABLE {table_name} ADD COLUMN salary FLOAT AFTER job\")\niceberg_df = spark.read.format(\"iceberg\").load(f\"{table_name}\")\niceberg_df.printSchema()\niceberg_df.show()\n\nspark.sql(f\"SELECT * FROM {table_name}.snapshots\").show()\n</code></pre> <p>schema BEFORE altering the table</p> <pre><code>root\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- job_title: string (nullable = true)\n</code></pre> <p>schema AFTER altering the table</p> <pre><code>root\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n |-- job: string (nullable = true)\n |-- salary: float (nullable = true)\n</code></pre> <p>The above code shows schema evolution by <code>renaming</code>, <code>changing</code> column types, and <code>adding</code> a new column.</p> <p>As you can observe in the schema changes:</p> <ul> <li>The column <code>age</code> type has changed</li> <li>The column <code>job_title</code> is now renamed to <code>job</code></li> <li>The column <code>salary</code> has been added</li> </ul> <p>The first time you run the code, in the snapshot table you notice that Iceberg executed all alterations without rewriting the data. This is indicated by having only 1 snapshot ID and no parents (<code>parent_id = NULL</code>), telling us that no data rewriting was performed.</p> <pre><code>+--------------------+-------------------+---------+---------+--------------------+--------------------+\n|        committed_at|        snapshot_id|parent_id|operation|       manifest_list|             summary|\n+--------------------+-------------------+---------+---------+--------------------+--------------------+\n|2024-06-21 00:33:...|5317106206696669755|     NULL|   append|warehouse/db/pers...|{spark.app.id -&gt; ...|\n+--------------------+-------------------+---------+---------+--------------------+--------------------+\n</code></pre>"},{"location":"projects/spark-iceberg/#acid-transactions","title":"ACID Transactions","text":"<p>Data accuracy and consistency are crucial in data lakes, particularly for business-critical purposes.</p> <p>Iceberg supports ACID transactions for write operations, ensuring that data remains in a consistent state, and enhancing the reliability of the stored information.</p> <pre><code># ACID: add and delete records\nspark.sql(f\"DELETE FROM {table_name} WHERE age = 42\")\nspark.sql(f\"INSERT INTO {table_name} values ('person4', 50, 'Teacher', 2000)\")\nspark.sql(f\"SELECT * FROM {table_name}.snapshots\").show()\n</code></pre> <p>snapshot table</p> <pre><code>+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n|2024-06-21 00:33:...|5317106206696669755|               NULL|   append|warehouse/db/pers...|{spark.app.id -&gt; ...|\n|2024-06-21 00:33:...| 333991769332533017|5354326724936052873|overwrite|warehouse/db/pers...|{spark.app.id -&gt; ...|\n|2024-06-21 00:33:...|1227170189499516533| 333991769332533017|   append|warehouse/db/pers...|{spark.app.id -&gt; ...|\n+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n</code></pre> <p>In the snapshots table, we can now observe that Iceberg has added three snapshot IDs, each created from the preceding one.</p> <p>Note</p> <p>If, for any reason, one of the actions fails, the transactions will fail, and the snapshot won't be created.</p>"},{"location":"projects/spark-iceberg/#table-partitioning","title":"Table Partitioning","text":"<p>As you may be aware, querying large amounts of data in data lakes can be resource-intensive.</p> <p>Iceberg supports data partitioning by one or more columns. This significantly improves query performance by reducing the volume of data read during queries.</p> <pre><code># Alter Partitions\nspark.sql(f\"ALTER TABLE {table_name} ADD PARTITION FIELD age\")\nspark.read.format(\"iceberg\").load(f\"{table_name}\").where(\"age = 28\").show()\n</code></pre> <p>Partitioned DataFrame</p> <pre><code>+-------+---+------+------+\n|   name|age|   job|salary|\n+-------+---+------+------+\n|person1| 28|Doctor|  NULL|\n+-------+---+------+------+\n</code></pre> <p>The code creates a new partition using the <code>age</code> column.</p> <p>Note</p> <p>This partition will apply to the new rows that get inserted moving forward, and old data will not be impacted</p> <p>Info</p> <p>We can also add partitions when we create the Iceberg table</p> <pre><code>spark.sql(f\"\"\"\n    CREATE TABLE IF NOT EXISTS {table_name}\n    (name STRING, age INT, job STRING, salary INT)\n    USING iceberg\n    PARTITIONED BY (age)\n\"\"\")\n</code></pre>"},{"location":"projects/spark-iceberg/#time-travel","title":"Time Travel","text":"<p>Analyzing historical trends or tracking changes over time is often essential in a data lake.</p> <p>Iceberg provides a time-travel API that allows users to query data as it appeared at a specific <code>version</code> or <code>timestamp</code>, facilitating historical data analysis.</p> <p>Apache Iceberg gives you the flexibility to load any snapshot or data at a given point in time.</p> <p>This allows you to examine changes at a given time or roll back to a specific version.</p> <p>Warning</p> <p>You need to change the <code>snapshot-id</code> as per your code runtime.</p> <pre><code>spark.sql(f\"SELECT * FROM {table_name}.snapshots\").show(1, truncate=False)\n\n# Read snapshot by id run\nsnapshot_id = \"333991769332533017\"\nspark.read.option(\"snapshot-id\", snapshot_id).table(table_name).show()\n</code></pre> <pre><code>+-----------------------+-------------------+---------+---------+--------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|committed_at           |snapshot_id        |parent_id|operation|manifest_list                                                                                     |summary                                                                                                                                                                                                                                                                                             |\n+-----------------------+-------------------+---------+---------+--------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|2024-06-21 00:33:27.854|5317106206696669755|NULL     |append   |warehouse/db/persons/metadata/snap-5317106206696669755-1-59b1ef61-4e45-4a54-bddf-c89989f2e7ef.avro|{spark.app.id -&gt; app-20240621003324-0000, added-data-files -&gt; 2, added-records -&gt; 3, added-files-size -&gt; 1838, changed-partition-count -&gt; 1, total-records -&gt; 3, total-files-size -&gt; 1838, total-data-files -&gt; 2, total-delete-files -&gt; 0, total-position-deletes -&gt; 0, total-equality-deletes -&gt; 0}|\n+-----------------------+-------------------+---------+---------+--------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nonly showing top 1 row\n</code></pre>"},{"location":"projects/spark-iceberg/#load-parquet-files-into-iceberg","title":"Load Parquet Files into Iceberg","text":"scripts/iceberg-load.py<pre><code>from pyspark.sql import SparkSession\nfrom pyspark import SparkConf\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\n\nwarehouse_path = \"./warehouse\"\niceberg_spark_jar = 'org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.3'\niceberg_spark_ext = 'org.apache.iceberg:iceberg-spark-extensions-3.4_2.12:1.4.3'\ncatalog_name = \"demo\"\n\n# Setup iceberg config\nconf = SparkConf().setAppName(\"YourAppName\") \\\n    .set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n    .set(f\"spark.sql.catalog.{catalog_name}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .set('spark.jars.packages', iceberg_spark_jar) \\\n    .set('spark.jars.packages', iceberg_spark_ext) \\\n    .set(f\"spark.sql.catalog.{catalog_name}.warehouse\", warehouse_path) \\\n    .set(f\"spark.sql.catalog.{catalog_name}.type\", \"hadoop\")\\\n    .set(\"spark.sql.defaultCatalog\", catalog_name)\n\n# Create spark session\nspark = SparkSession.builder.config(conf=conf).getOrCreate()\nspark.sparkContext.setLogLevel(\"ERROR\")\n\nSOURCE_PATH = \"./data/yellow_trip_data\"\nDEST_PATH = \"./data/output/count_by_vendor.parquet\"\n\n# Loading the data\ndf = spark.read.options(inferSchema=True).parquet(SOURCE_PATH)\ndf.printSchema()\n\nspark.sql(f\"CREATE DATABASE IF NOT EXISTS db\")\ndf.writeTo(\"db.yellow_trip_data\") \\\n    .createOrReplace()\n\nspark.sql(\"DESCRIBE TABLE db.yellow_trip_data\").show(truncate=False)\nspark.sql(\"SHOW CREATE TABLE db.yellow_trip_data\").show(truncate=False)\nspark.sql(\"SELECT * FROM db.yellow_trip_data LIMIT 5\").show()\n</code></pre> <p>Run above code using</p> <pre><code>make dev\n\n./bin/spark-submit ./scripts/iceberg-load.py\n</code></pre> <p>By this exercise, we will learn</p> <ul> <li>load the parquet files into spark runtime</li> <li>write the data into Iceberg table</li> </ul> <p>Schema</p> <pre><code>root\n |-- VendorID: long (nullable = true)\n |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n |-- passenger_count: double (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- RatecodeID: double (nullable = true)\n |-- store_and_fwd_flag: string (nullable = true)\n |-- PULocationID: long (nullable = true)\n |-- DOLocationID: long (nullable = true)\n |-- payment_type: long (nullable = true)\n |-- fare_amount: double (nullable = true)\n |-- extra: double (nullable = true)\n |-- mta_tax: double (nullable = true)\n |-- tip_amount: double (nullable = true)\n |-- tolls_amount: double (nullable = true)\n |-- improvement_surcharge: double (nullable = true)\n |-- total_amount: double (nullable = true)\n |-- congestion_surcharge: double (nullable = true)\n |-- airport_fee: double (nullable = true)\n</code></pre> <p><code>CREATE TABLE</code> statement</p> <pre><code>CREATE TABLE demo.db.yellow_trip_data ( \n    VendorID BIGINT,\n    tpep_pickup_datetime TIMESTAMP_NTZ,\n    tpep_dropoff_datetime TIMESTAMP_NTZ,\n    passenger_count DOUBLE,\n    trip_distance DOUBLE,\n    RatecodeID DOUBLE,\n    store_and_fwd_flag STRING,\n    PULocationID BIGINT,\n    DOLocationID BIGINT,\n    payment_type BIGINT,\n    fare_amount DOUBLE,\n    extra DOUBLE,\n    mta_tax DOUBLE,\n    tip_amount DOUBLE,\n    tolls_amount DOUBLE,\n    improvement_surcharge DOUBLE,\n    total_amount DOUBLE,\n    congestion_surcharge DOUBLE,\n    airport_fee DOUBLE)\nUSING iceberg\nLOCATION './warehouse/db/yellow_trip_data'\nTBLPROPERTIES (\n    'current-snapshot-id' = '3768997006627294211',\n    'format' = 'iceberg/parquet',\n    'format-version' = '2',\n    'write.parquet.compression-codec' = 'zstd')\n</code></pre> <p>Data overview</p> <pre><code>+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n|       1| 2022-01-01 00:35:40|  2022-01-01 00:53:29|            2.0|          3.8|       1.0|                 N|         142|         236|           1|       14.5|  3.0|    0.5|      3.65|         0.0|                  0.3|       21.95|                 2.5|        0.0|\n|       1| 2022-01-01 00:33:43|  2022-01-01 00:42:07|            1.0|          2.1|       1.0|                 N|         236|          42|           1|        8.0|  0.5|    0.5|       4.0|         0.0|                  0.3|        13.3|                 0.0|        0.0|\n|       2| 2022-01-01 00:53:21|  2022-01-01 01:02:19|            1.0|         0.97|       1.0|                 N|         166|         166|           1|        7.5|  0.5|    0.5|      1.76|         0.0|                  0.3|       10.56|                 0.0|        0.0|\n|       2| 2022-01-01 00:25:21|  2022-01-01 00:35:23|            1.0|         1.09|       1.0|                 N|         114|          68|           2|        8.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        11.8|                 2.5|        0.0|\n|       2| 2022-01-01 00:36:48|  2022-01-01 01:14:20|            1.0|          4.3|       1.0|                 N|          68|         163|           1|       23.5|  0.5|    0.5|       3.0|         0.0|                  0.3|        30.3|                 2.5|        0.0|\n+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n</code></pre>"},{"location":"projects/spark-iceberg/#conclusion","title":"Conclusion","text":"<p>Apache Iceberg provides a robust solution for managing big data tables with features like atomic commits, schema evolution, and time travel.</p> <p>When combined with the power of <code>PySpark</code>, you can harness the capabilities of Iceberg while leveraging the flexibility and scalability of <code>PySpark</code> for your big data processing needs.</p>"},{"location":"projects/spark-iceberg/#references","title":"References","text":"<ul> <li>Why You Should use Apache Iceberg with PySpark</li> <li>Apache Iceberg -- Architecture Demystified</li> <li>Apache Iceberg Official Documentation</li> </ul>"}]}